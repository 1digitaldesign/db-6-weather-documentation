---
description: Comprehensive validation suite for SQL queries in db-{N}/queries/queries.md files
alwaysApply: true
---

# Query Validation Suite

## Overview

This validation suite provides comprehensive verification, testing, and evaluation of SQL queries in `db-{N}/queries/queries.md` files. The suite ensures queries meet all requirements including cross-database compatibility, complexity standards, and formatting consistency.

## Timestamp Requirements

**CRITICAL**: All JSON files generated by the validation suite must include timestamps in **EST (Eastern Standard Time)** timezone using the format **YYYYMMDD-HHMM**.

- **Format**: `YYYYMMDD-HHMM` (e.g., `20260203-1430` for February 3, 2026 at 2:30 PM EST)
- **Timezone**: EST (US/Eastern) - accounts for both EST and EDT automatically
- **Required Fields**: All JSON output files must include timestamp fields using this format
- **Utility Function**: Use `scripts/timestamp_utils.py` for consistent timestamp generation

### Timestamp Fields

All validation JSON files must include timestamp fields:
- `validation_date`: Overall validation timestamp
- `extraction_timestamp`: Query extraction timestamp (for queries.json)
- `start_time`: Phase start time
- `end_time`: Phase end time
- `test_date`: Test execution date
- Any other timestamp fields in validation outputs

### Implementation

```python

# Import timestamp utility

try:
    from timestamp_utils import get_est_timestamp
except ImportError:
    # If running from db-{N}/scripts/, import from root
    import sys
    from pathlib import Path
    root_scripts = Path(__file__).parent.parent.parent / 'scripts'
    sys.path.insert(0, str(root_scripts))
    from timestamp_utils import get_est_timestamp

# Use in JSON generation

result = {
    'validation_date': get_est_timestamp(),  # Returns: '20260203-1430' (EST)
    'start_time': get_est_timestamp(),
    # ... other fields
}
```

### Example Timestamps

- `20260203-1430` - February 3, 2026 at 2:30 PM EST
- `20260203-0915` - February 3, 2026 at 9:15 AM EST
- `20260203-2359` - February 3, 2026 at 11:59 PM EST

### Files Affected

All validation scripts that generate JSON files must use this format:
- `scripts/extract_queries_to_json.py` - `extraction_timestamp`
- `scripts/validate.py` - `validation_date`, `start_time`, `end_time`
- `db-{N}/scripts/verify_fixes.py` - `verification_date`
- `db-{N}/scripts/comprehensive_validator.py` - `validation_date`
- `db-{N}/scripts/generate_final_report.py` - `report_date`
- `db-{N}/scripts/execution_tester.py` - `test_date` (if applicable)

## Validation Phases

The validation suite consists of six phases:

0. **Phase 0: Query Abstraction** - **REQUIRED**: Extract queries.md to queries.json (must complete before all other phases)
1. **Phase 1: Fix Verification** - Verifies that critical fixes have been applied correctly
2. **Phase 2: Syntax Validation** - Validates SQL syntax on PostgreSQL
3. **Phase 3: Execution Testing** - Executes queries on available databases and collects metrics
4. **Phase 4: Comprehensive Evaluation** - Evaluates queries against all requirements
5. **Phase 5: Report Generation** - Generates comprehensive validation reports

## Directory Structure

Each database repository (db-1 through db-15) includes validation infrastructure:

```
db-{N}/
├── scripts/
│   ├── verify_fixes.py              # Phase 1: Fix verification
│   ├── comprehensive_validator.py   # Phase 2 & 4: Syntax validation and evaluation
│   ├── execution_tester.py          # Phase 3: Execution testing
│   ├── generate_final_report.py     # Phase 5: Final report generation
│   ├── extract_queries_to_json.py   # Extract queries.md to queries.json
│   └── test_queries_postgres.py  # Base testing infrastructure
├── results/
│   ├── fix_verification.json                    # Phase 1 results
│   ├── comprehensive_validation_report.json     # Phase 2 & 4 results
│   ├── query_test_results_postgres.json  # Phase 3 results
│   └── final_comprehensive_validation_report.json  # Phase 5 final report
└── queries/
    ├── queries.md                    # SQL queries (source)
    └── queries.json                  # Query metadata abstraction (extracted from queries.md)
```

## Phase 0: Query Abstraction (REQUIRED)

### Purpose

**MANDATORY STEP**: All queries from `queries.md` must be abstracted into `queries.json` before any validation can proceed. This is a hard requirement - validation scripts will check for `queries.json` and fail if it's missing or out of sync.

### Script

**File**: `db-{N}/scripts/extract_queries_to_json.py` or `/path/to/db/scripts/extract_queries_to_json.py`

### Usage

```bash

# Extract queries for specific database

cd db-{N}
python3 scripts/extract_queries_to_json.py

# Or extract for multiple databases from root

cd /path/to/db
python3 scripts/extract_queries_to_json.py [db-number]
```

### Requirements

- **queries.json must exist**: Validation scripts will fail if `queries.json` is missing
- **queries.json must be current**: Extraction timestamp should match recent edits to `queries.md`
- **queries.json must be valid**: Must contain all required fields and valid JSON structure

### Validation Check

Before running any validation phase, scripts should verify:
1. `queries.json` exists in `db-{N}/queries/`
2. `queries.json` contains valid JSON
3. `queries.json` has `total_queries` matching expected count (30)
4. `queries.json` extraction timestamp is recent (within last 24 hours or after last `queries.md` edit)

### Failure Behavior

If `queries.json` is missing or invalid:
- Validation scripts should **FAIL** with clear error message
- Error message should instruct user to run extraction script first
- No other validation phases should proceed

### queries.json Structure

```json
{
  "source_file": "path/to/queries.md",
  "extraction_timestamp": "ISO timestamp",
  "total_queries": 30,
  "queries": [
    {
      "number": 1,
      "title": "Query title",
      "description": "Query description",
      "complexity": "Complexity description",
      "expected_output": "Expected output description",
      "sql": "Full SQL query text",
      "line_number": 152
    }
  ]
}
```

### Required Fields

- **number**: Query number (1-30)
- **title**: Query title from header
- **description**: Query description text
- **complexity**: Complexity description
- **expected_output**: Expected output description
- **sql**: Complete SQL query text
- **line_number**: Line number where query starts in queries.md

### When to Extract

- **Before any validation**: Always extract before running validation suite
- **After editing queries.md**: Extract immediately after any edits to `queries.md`
- **In CI/CD pipelines**: Include extraction as first step in validation workflow
- **After fixing queries**: Extract after applying fixes to ensure JSON is current

### Integration with Validation

All validation phases should use `queries.json` as the source of truth:
- Faster query access (no markdown parsing)
- Programmatic query manipulation
- Easier integration with tools
- Consistent metadata structure
- Validation scripts should read from `queries.json`, not parse `queries.md`

## Phase 1: Fix Verification

### Purpose

Verifies that queries meet conceptual quality and compatibility standards:
- Query 2: PostgreSQL array syntax compatibility (conceptual check)
- Query 26: Query complexity and structure validation (conceptual check)
- Query titles: Uniqueness and descriptiveness verification (conceptual check)
- Header formatting: Consistency check

### Script

**File**: `db-{N}/scripts/verify_fixes.py`

### Usage

```bash
cd db-{N}
python3 scripts/verify_fixes.py
```

### Checks Performed (Conceptual Approach)

**Note**: Fix verification uses a **conceptual and relaxed approach** - it checks for general patterns and quality indicators rather than specific table names, CTE names, or exact query structures. This allows different databases (db-1 through db-15) to have their own domain-specific queries while still ensuring quality standards.

1. **Query 2 Array Syntax Compatibility** (Conceptual):
   - Verifies query uses CTEs (any CTE structure)
   - Checks for PostgreSQL-compatible array operations (array_length, ANY(), array concatenation)
   - Validates query structure is complete (SELECT, FROM clauses)
   - Notes array slicing usage if present (PostgreSQL array slicing is valid)

2. **Query 26 Complexity Validation** (Conceptual):
   - Verifies query uses CTEs (standard or recursive)
   - Checks for query complexity indicators (multiple CTEs, joins, aggregations)
   - Validates SQL structure is complete
   - Confirms query code block is properly formatted

3. **Query Title Uniqueness** (Conceptual):
   - Verifies all queries 1-30 have titles
   - Checks titles are descriptive (sufficient length)
   - Ensures no duplicate titles across all queries
   - Confirms queries 26-30 exist (without expecting specific titles)

4. **Header Formatting**:
   - Verifies all queries use `## Query N:` format
   - Checks for absence of `---` prefix

### Output

**File**: `db-{N}/results/fix_verification.json`

**Structure**:
```json
{
  "verification_date": "ISO timestamp",
  "file": "path/to/queries.md",
  "fixes": {
    "query_2_array_syntax": {
      "query_number": 2,
      "fix": "Array slicing syntax replacement",
      "Pass": 1|0,
      "checks": [...],
      "notes": ["warning messages if any"]
    },
    "query_26_recursive_cte": {...},
    "query_title_uniqueness": {...},
    "header_formatting": {...}
  },
  "Pass": 1|0,
  "notes": ["warning messages if any"]
}
```

### Success Criteria

- All fixes show `"Pass": 1`
- Overall `"Pass": 1` indicates success
- No critical issues found

## Phase 2: Syntax Validation

### Purpose

Validates SQL syntax on PostgreSQL using `EXPLAIN` statements to check parsing without executing queries.

### Script

**File**: `db-{N}/scripts/comprehensive_validator.py`

### Usage

```bash
cd db-{N}
python3 scripts/comprehensive_validator.py
```

### Database Configuration

Set environment variables for database connections:

**PostgreSQL**:
```bash
export PG_HOST=localhost
export PG_PORT=5432
export PG_USER=postgres
export PG_PASSWORD=your_password
export PG_DATABASE=db_1_validation
```


### Validation Method

For each database:
1. Attempts connection using provided credentials
2. For each query, executes `EXPLAIN {query_sql}` to validate syntax
3. Records success/failure and error messages
4. Does not execute actual queries (no data required)

### Output

**File**: `db-{N}/results/comprehensive_validation_report.json`

**Structure**:
```json
{
  "validation_date": "ISO timestamp",
  "database": "db-{N}",
  "total_queries": 30,
  "syntax_validation": {
    "postgresql": {
      "available": true|false,
      "queries": [
        {
          "query_number": 1,
          "success": true|false,
          "error": "error message if failed",
          "error_type": "ErrorType"
        }
      ]
    },
  }
}
```

### Success Criteria

- **REQUIRED**: At least one database (PostgreSQL) must be available for syntax validation
- All queries parse successfully on all available databases
- No syntax errors reported
- Cross-database compatibility confirmed
- **Validation fails if no databases are available**

## Phase 3: Execution Testing

### Purpose

Executes queries on available databases and collects execution metrics including success/failure, execution time, row counts, and column schemas.

### Script

**File**: `db-{N}/scripts/execution_tester.py`

### Usage

```bash
cd db-{N}
python3 scripts/execution_tester.py
```

### Database Configuration

Same as Phase 2. Requires databases to be set up with schema and sample data.

### Testing Method

For each database:
1. Connects using provided credentials
2. Executes each query with `LIMIT 100` or `FETCH FIRST 100 ROWS`
3. Collects metrics:
   - Execution success/failure
   - Execution time (milliseconds)
   - Row count returned
   - Column schema
   - Error messages (if any)

### Output

**File**: `db-{N}/results/query_test_results_postgres.json`

**Structure**:
```json
{
  "database": "db-{N}",
  "test_date": "ISO timestamp",
  "postgresql": {
    "available": true|false,
    "queries": [
      {
        "query_number": 1,
        "success": true|false,
        "execution_time_ms": 123.45,
        "row_count": 100,
        "columns": [...],
        "error": "error message if failed"
      }
    ]
  },
  "summary": {
    "total_queries": 30,
    "postgresql": {
      "tested": 30,
      "successful": 28,
      "failed": 2,
      "success_rate": 93.33
    }
  }
}
```

### Success Criteria

- **REQUIRED**: At least one database (PostgreSQL) must be available for execution testing
- High success rate (>90%) on available databases
- Execution times within acceptable limits (<5 seconds per query)
- Consistent results across databases (when multiple available)
- **Validation fails if no databases are available**

## Phase 4: Comprehensive Evaluation

### Purpose

Evaluates queries against all requirements:
- Query count (exactly 30)
- Recursive CTE usage (claims vs. actual)
- CTE usage (all queries must use CTEs)
- Query complexity metrics
- Cross-database compatibility

### Script

**File**: `db-{N}/scripts/comprehensive_validator.py` (includes evaluation)

### Evaluation Checks

1. **Query Count Validation**:
   - Verifies exactly 30 queries
   - Checks sequential numbering (1-30)

2. **Recursive CTE Validation**:
   - Checks if queries claiming recursive CTE have `WITH RECURSIVE`
   - Identifies mismatches (claims recursive but doesn't have it)
   - Reports queries with recursive CTE but not claiming it

3. **CTE Usage Validation**:
   - Verifies all queries use at least one CTE (`WITH` clause)
   - Reports queries without CTEs

4. **Complexity Evaluation**:
   - Counts CTEs per query
   - Counts joins per query
   - Counts window functions per query
   - Counts aggregations per query
   - Counts subqueries per query
   - Calculates average complexity metrics

5. **Cross-Database Compatibility**:
   - Combines syntax validation results
   - Identifies queries that fail on any database
   - Reports overall compatibility score

### Output

**File**: `db-{N}/results/comprehensive_validation_report.json`

**Structure**:
```json
{
  "evaluation": {
    "query_count": {
      "requirement": "Exactly 30 queries",
      "found": 30,
      "Pass": 1|0,
      "notes": ["warning messages if any"]
    },
    "recursive_cte_usage": {
      "requirement": "Queries claiming recursive CTE must have WITH RECURSIVE",
      "total_queries": 30,
      "queries_with_recursive": 14,
      "mismatched": 0,
      "Pass": 1|0,
      "mismatched_queries": [...],
      "notes": ["warning messages if any"]
    },
    "cte_usage": {
      "requirement": "All queries must use CTEs",
      "queries_with_cte": 30,
      "queries_without_cte": 0,
      "Pass": 1|0,
      "notes": ["warning messages if any"]
    },
    "complexity": {
      "average_cte_count": 1.0,
      "average_join_count": 3.6,
      "average_window_function_count": 6.8,
      "complexity_scores": [...]
    }
  }
}
```

### Success Criteria

- Query count: Exactly 30 queries (`"Pass": 1`)
- Recursive CTE: No mismatches (all claims match reality) (`"Pass": 1`)
- CTE usage: All queries use CTEs (`"Pass": 1`)
- Complexity: Meets minimum complexity requirements (`"Pass": 1`)
- Cross-database: All queries work on all databases (`"Pass": 1`)

## Phase 5: Report Generation

### Purpose

Generates a final comprehensive validation report combining all phases into a single document.

### Script

**File**: `db-{N}/scripts/generate_final_report.py`

### Usage

```bash
cd db-{N}
python3 scripts/generate_final_report.py
```

### Report Contents

The final report combines:
- Phase 1: Fix verification results
- Phase 2: Syntax validation results
- Phase 3: Execution testing results
- Phase 4: Comprehensive evaluation results
- Findings: Critical issues, warnings, recommendations

### Output

**File**: `db-{N}/results/final_comprehensive_validation_report.json`

**Structure**:
```json
{
  "report_date": "ISO timestamp",
  "database": "db-{N}",
  "Pass": 1|0,
  "summary": {
    "total_queries": 30,
    "fix_verification_Pass": 1|0,
    "syntax_validation_Pass": 1|0,
    "evaluation_Pass": 1|0,
    "execution_testing_Pass": 1|0
  },
  "phase_1_fix_verification": {...},
  "phase_2_syntax_validation": {...},
  "phase_3_execution_testing": {...},
  "phase_4_comprehensive_evaluation": {...},
  "findings": {
    "critical_issues": [...],
    "warnings": [...],
    "recommendations": [...]
  },
  "notes": ["warning messages if any"]
}
```

### Success Criteria

- **CRITICAL**: All phases must complete successfully for validation to pass
- **REQUIRED**: Phase 0 (Query Abstraction) must complete
- **REQUIRED**: Phase 1 (Fix Verification) must pass
- **REQUIRED**: Phase 2 (Syntax Validation) must have at least one database available and pass
- **REQUIRED**: Phase 3 (Execution Testing) must have at least one database available and pass
- **REQUIRED**: Phase 4 (Comprehensive Evaluation) must pass
- **REQUIRED**: Phase 5 (Report Generation) must complete
- Overall `"Pass": 1` indicates success (0 indicates failure)
- If warnings exist, include `"notes"` key with warning messages (Pass remains 1)
- No critical issues found (critical issues result in Pass: 0)
- **Validation fails if any required phase cannot complete**

## Running the Complete Validation Suite

### Using the /validate Command (Recommended)

The easiest way to run validation is using the `/validate` command:

```bash

# Validate single database

/validate @db/db-1/
/validate db-1

# Validate range of databases

/validate @db/db-1/ @db/db-5/
/validate db-1 db-5

# Validate all databases

/validate -a
```

The `/validate` command automatically:
1. Runs Phase 0 (query extraction) if needed
2. Executes all validation phases in sequence
3. Generates comprehensive reports
4. Provides summary statistics

See `scripts/VALIDATE_COMMAND_USAGE.md` for complete usage documentation.

### Quick Validation (No Database Required)

```bash
cd db-{N}

# Phase 0: REQUIRED - Extract queries.md to queries.json

python3 scripts/extract_queries_to_json.py

# Phase 1: Fix verification

python3 scripts/verify_fixes.py

# Phase 4: Evaluation (no DB required)

python3 scripts/comprehensive_validator.py

# Phase 5: Generate report

python3 scripts/generate_final_report.py
```

### Full Validation (With Database Connections)

```bash
cd db-{N}

# Phase 0: REQUIRED - Extract queries.md to queries.json

python3 scripts/extract_queries_to_json.py

# Set up database environment variables (see Phase 2)

# Phase 1: Fix verification

python3 scripts/verify_fixes.py

# Phase 2 & 4: Syntax validation and evaluation

python3 scripts/comprehensive_validator.py

# Phase 3: Execution testing

python3 scripts/execution_tester.py

# Phase 5: Generate final report

python3 scripts/generate_final_report.py
```

### Validation Workflow Requirements

**CRITICAL**: Phase 0 (Query Abstraction) is **MANDATORY** and must be completed before all other phases:
1. Validation scripts should check for `queries.json` existence
2. If missing, scripts should fail with clear error message
3. Extraction must be run manually or as part of automated workflow
4. `queries.json` must be kept in sync with `queries.md` at all times
5. The `/validate` command automatically handles Phase 0 extraction

## Requirements and Standards

### Query Requirements

All queries in `queries.md` must meet these requirements:

1. **Count**: Exactly 30 queries, numbered sequentially (Query 1 through Query 30)

2. **Cross-Database Compatibility**: All queries must work on PostgreSQL

3. **Unique SQL Expressions**: Each query must be a unique SQL expression - no duplicates or minor variations

4. **CTE Usage**: All queries must use at least one CTE (Common Table Expression)

5. **Recursive CTE Claims**: Queries claiming recursive CTE in title/description must contain `WITH RECURSIVE`

6. **Complexity**: Queries must demonstrate high complexity with:
   - Multiple CTEs (preferably 3+)
   - Joins, aggregations, window functions
   - Subqueries and correlated subqueries
   - Recursive CTEs where claimed

7. **Formatting**: Consistent formatting:
   - Headers: `## Query N: Title`
   - SQL code blocks: Marked with ` ```sql `
   - Descriptions: Clear and accurate

### Validation Standards

- **Fix Verification**: Must pass all checks
- **Syntax Validation**: All queries must parse on all available databases
- **Execution Testing**: >90% success rate on available databases
- **Evaluation**: All requirements must be met

## Interpreting Results

### Overall Pass Value

- **Pass: 1**: All checks passed, no issues found (may include `"notes"` for warnings)
- **Pass: 0**: Critical issues found that must be addressed
- **Notes**: When warnings are present, include `"notes"` key with warning messages (Pass remains 1)

### Common Issues and Solutions

1. **Array Slicing Syntax** (Query 2):
   - **Issue**: PostgreSQL-specific `array[1:5]` syntax
   - **Solution**: Use `ARRAY_LENGTH` checks and array element access

2. **Missing Recursive CTE**:
   - **Issue**: Query claims recursive CTE but doesn't have `WITH RECURSIVE`
   - **Solution**: Add `WITH RECURSIVE` or update description

3. **Duplicate Titles**:
   - **Issue**: Multiple queries with same title
   - **Solution**: Rename queries to be unique and descriptive

4. **Formatting Inconsistency**:
   - **Issue**: Headers use `---## Query N:` instead of `## Query N:`
   - **Solution**: Standardize header format

5. **Syntax Errors**:
   - **Issue**: Query fails to parse on a database
   - **Solution**: Review database-specific syntax requirements

## Integration with CI/CD

### Automated Validation

The validation suite can be integrated into CI/CD pipelines:

```yaml

# Example GitHub Actions workflow

name: Validate Queries
on: [push, pull_request]
jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: |
          pip install psycopg2-binary
      - name: Phase 0 - Extract queries to JSON (REQUIRED)
        run: |
          cd db-1
          python3 scripts/extract_queries_to_json.py
      - name: Phase 1 - Verify fixes
        run: |
          cd db-1
          python3 scripts/verify_fixes.py
      - name: Phase 2 & 4 - Syntax validation and evaluation
        run: |
          cd db-1
          python3 scripts/comprehensive_validator.py
      - name: Phase 5 - Generate report
        run: |
          cd db-1
          python3 scripts/generate_final_report.py
      - name: Upload results
        uses: actions/upload-artifact@v2
        with:
          name: validation-results
          path: db-1/results/*.json
      - name: Check queries.json exists
        run: |
          test -f db-1/queries/queries.json || (echo "ERROR: queries.json missing - Phase 0 must complete successfully" && exit 1)
```

**CRITICAL**: Phase 0 (query extraction) must be the first step in any CI/CD validation workflow.

## Dependencies

### Python Packages

Required packages (install via `pip install -r requirements.txt`):

```
psycopg2-binary>=2.9.0  # PostgreSQL
```

### Database Access

- **PostgreSQL**: Requires database server with schema loaded


## Best Practices

1. **Phase 0 is MANDATORY**: Always extract queries.md to queries.json before any validation (Phase 0)
2. **Validate JSON Exists**: All validation scripts must check for queries.json and fail if missing
3. **Keep JSON in Sync**: Extract to queries.json immediately after any edits to queries.md
4. **Run Phase 1 Next**: Always verify fixes before running other phases
5. **Check Results**: Review JSON output files for detailed information
6. **Fix Issues**: Address critical issues before proceeding to next phase
7. **Document Changes**: Update queries.md, extract to queries.json, and commit changes together
8. **Regular Validation**: Run validation suite after any query changes
9. **Use JSON as Source**: Validation scripts should read from queries.json, not parse queries.md
10. **CI/CD Integration**: Include Phase 0 extraction as first step in all automated workflows

## Troubleshooting

### Common Errors

1. **"queries.json not found"** or **"queries.json missing"**:
   - **CRITICAL ERROR**: Phase 0 must be completed first
   - Run `python3 scripts/extract_queries_to_json.py` to create queries.json
   - Ensure extraction completes successfully before running other phases
   - Check that queries.json exists in `db-{N}/queries/` directory

2. **"queries.json is out of sync"**:
   - Extract queries.json again: `python3 scripts/extract_queries_to_json.py`
   - Ensure queries.json extraction timestamp is after last queries.md edit
   - Re-run extraction after any edits to queries.md

3. **"PostgreSQL connection failed"**:
   - Check environment variables are set
   - Verify database server is running
   - Confirm credentials are correct


5. **"No queries found"**:
   - Verify `queries.md` exists in correct location
   - Check query header format matches `## Query N:`
   - Ensure queries.json was extracted successfully (check total_queries field)

6. **"Syntax error in query"**:
   - Review query SQL for database-specific syntax
   - Check for incompatible functions or operators
   - Verify CTE structure is correct

## Maintenance

### Updating Validation Rules

When requirements change:
1. Update this ruleset document
2. Update validation scripts accordingly
3. Update success criteria
4. Re-run validation on all databases

### Adding New Checks

To add new validation checks:
1. Add check to appropriate phase script
2. Update output JSON structure
3. Update this ruleset documentation
4. Test on sample queries

## References

- Database Compatibility Rules: `.cursor/rules/database-compatibility.mdc`
- Query Requirements: `.cursor/rules/database-compatibility.mdc`
- Research Metadata: `.cursor/rules/research-metadata-directories.mdc`

---
**Last Updated**: 2026-02-03
**Version**: 1.0
