{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database 1 Validation Report\n",
    "\n",
    "This notebook performs comprehensive validation checks on db-1 to ensure it meets the project requirements:\n",
    "- Real-world data (no synthetic/LLM-generated)\n",
    "- Not publicly available\n",
    "- Sufficient complexity for meaningful analysis\n",
    "- Production-ready quality\n",
    "\n",
    "## Validation Methods\n",
    "1. **Data Science Checks**: F-score, precision, recall, curve fitting analysis\n",
    "2. **Technical Analysis**: Placeholder data detection, timestamp verification, abnormality detection\n",
    "3. **Manual Review**: Schema complexity, business logic verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Connection\n",
    "\n",
    "**Update connection details based on db-1 format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "# Update these based on db-1 format (SQLite, PostgreSQL, MySQL, etc.)\n",
    "\n",
    "DB_TYPE = \"sqlite\"  # Options: sqlite, postgresql, mysql\n",
    "DB_PATH = \"../db-1/database.db\"  # Update path to db-1\n",
    "\n",
    "# For PostgreSQL:\n",
    "# DB_CONFIG = {\n",
    "#     'host': 'localhost',\n",
    "#     'port': 5432,\n",
    "#     'database': 'db1',\n",
    "#     'user': 'user',\n",
    "#     'password': 'password'\n",
    "# }\n",
    "\n",
    "def get_connection():\n",
    "    \"\"\"Create database connection based on DB_TYPE\"\"\"\n",
    "    if DB_TYPE == \"sqlite\":\n",
    "        return sqlite3.connect(DB_PATH)\n",
    "    elif DB_TYPE == \"postgresql\":\n",
    "        return psycopg2.connect(**DB_CONFIG)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported database type: {DB_TYPE}\")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    print(\"✓ Database connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Database connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Schema Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_schema(conn):\n",
    "    \"\"\"Extract and analyze database schema\"\"\"\n",
    "    schema_info = {}\n",
    "    \n",
    "    if DB_TYPE == \"sqlite\":\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        for table in tables:\n",
    "            cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "            columns = cursor.fetchall()\n",
    "            schema_info[table] = {\n",
    "                'columns': columns,\n",
    "                'row_count': pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table}\", conn).iloc[0]['count']\n",
    "            }\n",
    "    elif DB_TYPE == \"postgresql\":\n",
    "        engine = create_engine(f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\")\n",
    "        inspector = inspect(engine)\n",
    "        tables = inspector.get_table_names()\n",
    "        \n",
    "        for table in tables:\n",
    "            columns = inspector.get_columns(table)\n",
    "            schema_info[table] = {\n",
    "                'columns': columns,\n",
    "                'row_count': pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table}\", engine).iloc[0]['count']\n",
    "            }\n",
    "    \n",
    "    return schema_info\n",
    "\n",
    "schema = analyze_schema(conn)\n",
    "print(f\"\\nFound {len(schema)} tables:\")\n",
    "for table, info in schema.items():\n",
    "    print(f\"  - {table}: {info['row_count']} rows, {len(info['columns'])} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Placeholder Data Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_placeholder_data(conn, schema):\n",
    "    \"\"\"Detect placeholder/test data patterns\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Patterns indicating fake/test data\n",
    "    placeholder_patterns = {\n",
    "        'email': [r'@example\.com', r'@test\.com', r'user_\d+@', r'test\d+@'],\n",
    "        'url': [r'localhost', r'127\.0\.0\.1', r'staging', r'\.run\.app', r'example\.com'],\n",
    "        'name': [r'John Smith', r'Test User', r'Jane Doe', r'Admin'],\n",
    "        'slug': [r'/[a-z]{4}$', r'/p/[A-Z][a-z]{3}']  # Suspicious short slugs\n",
    "    }\n",
    "    \n",
    "    for table_name, info in schema.items():\n",
    "        try:\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM {table_name} LIMIT 1000\", conn)\n",
    "            \n",
    "            for col in df.columns:\n",
    "                col_lower = col.lower()\n",
    "                \n",
    "                # Check email columns\n",
    "                if 'email' in col_lower and df[col].dtype == 'object':\n",
    "                    for pattern in placeholder_patterns['email']:\n",
    "                        matches = df[col].astype(str).str.contains(pattern, case=False, na=False).sum()\n",
    "                        if matches > 0:\n",
    "                            issues.append({\n",
    "                                'table': table_name,\n",
    "                                'column': col,\n",
    "                                'issue': f\"Placeholder email pattern detected: {pattern}\",\n",
    "                                'matches': matches,\n",
    "                                'severity': 'HIGH'\n",
    "                            })\n",
    "                \n",
    "                # Check URL columns\n",
    "                if any(x in col_lower for x in ['url', 'link', 'href', 'website']):\n",
    "                    for pattern in placeholder_patterns['url']:\n",
    "                        matches = df[col].astype(str).str.contains(pattern, case=False, na=False).sum()\n",
    "                        if matches > 0:\n",
    "                            issues.append({\n",
    "                                'table': table_name,\n",
    "                                'column': col,\n",
    "                                'issue': f\"Placeholder URL pattern detected: {pattern}\",\n",
    "                                'matches': matches,\n",
    "                                'severity': 'HIGH'\n",
    "                            })\n",
    "                \n",
    "                # Check name columns\n",
    "                if 'name' in col_lower and df[col].dtype == 'object':\n",
    "                    for pattern in placeholder_patterns['name']:\n",
    "                        matches = df[col].astype(str).str.contains(pattern, case=False, na=False).sum()\n",
    "                        if matches > len(df) * 0.1:  # More than 10% matches\n",
    "                            issues.append({\n",
    "                                'table': table_name,\n",
    "                                'column': col,\n",
    "                                'issue': f\"Generic name pattern detected: {pattern}\",\n",
    "                                'matches': matches,\n",
    "                                'severity': 'MEDIUM'\n",
    "                            })\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not analyze table {table_name}: {e}\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "placeholder_issues = detect_placeholder_data(conn, schema)\n",
    "print(f\"\\nPlaceholder Data Detection Results:\")\n",
    "print(f\"Found {len(placeholder_issues)} issues\")\n",
    "for issue in placeholder_issues[:10]:  # Show first 10\n",
    "    print(f\"  [{issue['severity']}] {issue['table']}.{issue['column']}: {issue['issue']} ({issue['matches']} matches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Timestamp Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_timestamps(conn, schema):\n",
    "    \"\"\"Analyze timestamp patterns for signs of manipulation\"\"\"\n",
    "    timestamp_analysis = []\n",
    "    \n",
    "    for table_name, info in schema.items():\n",
    "        try:\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM {table_name} LIMIT 10000\", conn)\n",
    "            \n",
    "            for col in df.columns:\n",
    "                col_lower = col.lower()\n",
    "                if any(x in col_lower for x in ['time', 'date', 'created', 'updated', 'modified']):\n",
    "                    try:\n",
    "                        # Try to parse as datetime\n",
    "                        if df[col].dtype == 'object':\n",
    "                            dates = pd.to_datetime(df[col], errors='coerce')\n",
    "                            valid_dates = dates.dropna()\n",
    "                            \n",
    "                            if len(valid_dates) > 0:\n",
    "                                # Check for suspicious patterns\n",
    "                                date_range = valid_dates.max() - valid_dates.min()\n",
    "                                \n",
    "                                # Check if all dates are identical (suspicious)\n",
    "                                unique_dates = valid_dates.nunique()\n",
    "                                \n",
    "                                # Check for sequential patterns (might indicate generation)\n",
    "                                if len(valid_dates) > 1:\n",
    "                                    sorted_dates = valid_dates.sort_values()\n",
    "                                    diffs = sorted_dates.diff().dropna()\n",
    "                                    \n",
    "                                    # If all differences are identical, suspicious\n",
    "                                    if len(diffs.unique()) == 1:\n",
    "                                        timestamp_analysis.append({\n",
    "                                            'table': table_name,\n",
    "                                            'column': col,\n",
    "                                            'issue': 'Identical time intervals detected (possible generation)',\n",
    "                                            'severity': 'MEDIUM'\n",
    "                                        })\n",
    "                                \n",
    "                                timestamp_analysis.append({\n",
    "                                    'table': table_name,\n",
    "                                    'column': col,\n",
    "                                    'date_range_days': date_range.days,\n",
    "                                    'unique_dates': unique_dates,\n",
    "                                    'total_records': len(valid_dates),\n",
    "                                    'min_date': valid_dates.min(),\n",
    "                                    'max_date': valid_dates.max()\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not analyze timestamps in {table_name}: {e}\")\n",
    "    \n",
    "    return timestamp_analysis\n",
    "\n",
    "timestamp_results = analyze_timestamps(conn, schema)\n",
    "print(f\"\\nTimestamp Analysis Results:\")\n",
    "for result in timestamp_results[:5]:\n",
    "    if 'issue' in result:\n",
    "        print(f\"  [{result['severity']}] {result['table']}.{result['column']}: {result['issue']}\")\n",
    "    else:\n",
    "        print(f\"  {result['table']}.{result['column']}: {result['unique_dates']} unique dates over {result['date_range_days']} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Science Validation: Curve Fitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_distributions(conn, schema):\n",
    "    \"\"\"Analyze data distributions - synthetic data often has 'too clean' distributions\"\"\"\n",
    "    distribution_results = []\n",
    "    \n",
    "    for table_name, info in schema.items():\n",
    "        try:\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM {table_name} LIMIT 10000\", conn)\n",
    "            \n",
    "            for col in df.select_dtypes(include=[np.number]).columns:\n",
    "                if df[col].notna().sum() > 100:  # Need sufficient data\n",
    "                    values = df[col].dropna()\n",
    "                    \n",
    "                    # Test for normality (real data is rarely perfectly normal)\n",
    "                    if len(values) > 30:\n",
    "                        stat, p_value = stats.normaltest(values)\n",
    "                        \n",
    "                        # Check coefficient of variation\n",
    "                        cv = values.std() / values.mean() if values.mean() != 0 else np.inf\n",
    "                        \n",
    "                        # Check for suspicious patterns\n",
    "                        unique_ratio = values.nunique() / len(values)\n",
    "                        \n",
    "                        # If too many unique values relative to total, might be synthetic\n",
    "                        # If too few, might be categorical mislabeled\n",
    "                        \n",
    "                        distribution_results.append({\n",
    "                            'table': table_name,\n",
    "                            'column': col,\n",
    "                            'normality_p': p_value,\n",
    "                            'coefficient_of_variation': cv,\n",
    "                            'unique_ratio': unique_ratio,\n",
    "                            'mean': values.mean(),\n",
    "                            'std': values.std(),\n",
    "                            'suspicious': p_value > 0.05 and unique_ratio > 0.95  # Too normal + too unique\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not analyze distributions in {table_name}: {e}\")\n",
    "    \n",
    "    return distribution_results\n",
    "\n",
    "distribution_results = analyze_data_distributions(conn, schema)\n",
    "print(f\"\\nDistribution Analysis Results:\")\n",
    "suspicious = [r for r in distribution_results if r.get('suspicious', False)]\n",
    "if suspicious:\n",
    "    print(f\"⚠ Found {len(suspicious)} suspicious distributions (too clean):\")\n",
    "    for result in suspicious[:5]:\n",
    "        print(f\"  {result['table']}.{result['column']}: p={result['normality_p']:.4f}, unique_ratio={result['unique_ratio']:.2f}\")\n",
    "else:\n",
    "    print(\"✓ No suspiciously clean distributions detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complexity Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_complexity(conn, schema):\n",
    "    \"\"\"Assess database complexity - real-world databases have relationships and complexity\"\"\"\n",
    "    complexity_score = 0\n",
    "    complexity_details = []\n",
    "    \n",
    "    # Count tables\n",
    "    num_tables = len(schema)\n",
    "    complexity_score += min(num_tables * 2, 20)  # Max 20 points\n",
    "    complexity_details.append(f\"Tables: {num_tables} (+{min(num_tables * 2, 20)} points)\")\n",
    "    \n",
    "    # Count total columns\n",
    "    total_columns = sum(len(info['columns']) for info in schema.values())\n",
    "    complexity_score += min(total_columns // 5, 15)  # Max 15 points\n",
    "    complexity_details.append(f\"Total columns: {total_columns} (+{min(total_columns // 5, 15)} points)\")\n",
    "    \n",
    "    # Check for foreign keys (if detectable)\n",
    "    if DB_TYPE == \"sqlite\":\n",
    "        cursor = conn.cursor()\n",
    "        fk_count = 0\n",
    "        for table in schema.keys():\n",
    "            cursor.execute(f\"PRAGMA foreign_key_list({table})\")\n",
    "            fks = cursor.fetchall()\n",
    "            fk_count += len(fks)\n",
    "        complexity_score += min(fk_count * 3, 25)  # Max 25 points\n",
    "        complexity_details.append(f\"Foreign keys: {fk_count} (+{min(fk_count * 3, 25)} points)\")\n",
    "    \n",
    "    # Check row counts\n",
    "    total_rows = sum(info['row_count'] for info in schema.values())\n",
    "    if total_rows > 100000:\n",
    "        complexity_score += 20\n",
    "        complexity_details.append(f\"Large dataset: {total_rows:,} rows (+20 points)\")\n",
    "    elif total_rows > 10000:\n",
    "        complexity_score += 10\n",
    "        complexity_details.append(f\"Medium dataset: {total_rows:,} rows (+10 points)\")\n",
    "    \n",
    "    # Check for diverse data types\n",
    "    data_types = set()\n",
    "    for table_name, info in schema.items():\n",
    "        try:\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM {table_name} LIMIT 100\", conn)\n",
    "            data_types.update(df.dtypes.astype(str).unique())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    complexity_score += len(data_types) * 2  # Max 20 points\n",
    "    complexity_details.append(f\"Data type diversity: {len(data_types)} types (+{len(data_types) * 2} points)\")\n",
    "    \n",
    "    return {\n",
    "        'score': complexity_score,\n",
    "        'max_score': 100,\n",
    "        'details': complexity_details,\n",
    "        'assessment': 'HIGH' if complexity_score >= 60 else 'MEDIUM' if complexity_score >= 30 else 'LOW'\n",
    "    }\n",
    "\n",
    "complexity = assess_complexity(conn, schema)\n",
    "print(f\"\\nComplexity Assessment:\")\n",
    "print(f\"Score: {complexity['score']}/{complexity['max_score']} ({complexity['assessment']})\")\n",
    "for detail in complexity['details']:\n",
    "    print(f\"  {detail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data from each table\n",
    "print(\"\\nSample Data from Each Table:\")\n",
    "for table_name in list(schema.keys())[:5]:  # Show first 5 tables\n",
    "    try:\n",
    "        df_sample = pd.read_sql_query(f\"SELECT * FROM {table_name} LIMIT 5\", conn)\n",
    "        print(f\"\\n{table_name}:\")\n",
    "        print(df_sample.to_string())\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not sample {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_report():\n",
    "    \"\"\"Generate comprehensive validation report\"\"\"\n",
    "    report = {\n",
    "        'database': 'db-1',\n",
    "        'validation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'schema_summary': {\n",
    "            'tables': len(schema),\n",
    "            'total_rows': sum(info['row_count'] for info in schema.values()),\n",
    "            'total_columns': sum(len(info['columns']) for info in schema.values())\n",
    "        },\n",
    "        'placeholder_issues': {\n",
    "            'total': len(placeholder_issues),\n",
    "            'high_severity': len([i for i in placeholder_issues if i['severity'] == 'HIGH']),\n",
    "            'medium_severity': len([i for i in placeholder_issues if i['severity'] == 'MEDIUM'])\n",
    "        },\n",
    "        'timestamp_analysis': {\n",
    "            'tables_with_timestamps': len([r for r in timestamp_results if 'issue' not in r]),\n",
    "            'suspicious_patterns': len([r for r in timestamp_results if 'issue' in r])\n",
    "        },\n",
    "        'distribution_analysis': {\n",
    "            'suspicious_distributions': len([r for r in distribution_results if r.get('suspicious', False)])\n",
    "        },\n",
    "        'complexity': complexity,\n",
    "        'overall_status': 'PENDING'  # Will be set based on results\n",
    "    }\n",
    "    \n",
    "    # Determine overall status\n",
    "    if report['placeholder_issues']['high_severity'] > 0:\n",
    "        report['overall_status'] = 'FAIL - High severity placeholder data detected'\n",
    "    elif report['placeholder_issues']['total'] > 10:\n",
    "        report['overall_status'] = 'FAIL - Too many placeholder data issues'\n",
    "    elif complexity['score'] < 30:\n",
    "        report['overall_status'] = 'FAIL - Insufficient complexity'\n",
    "    elif report['distribution_analysis']['suspicious_distributions'] > 5:\n",
    "        report['overall_status'] = 'WARNING - Suspicious data distributions detected'\n",
    "    else:\n",
    "        report['overall_status'] = 'PASS'\n",
    "    \n",
    "    return report\n",
    "\n",
    "validation_report = generate_validation_report()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION REPORT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Database: {validation_report['database']}\")\n",
    "print(f\"Validation Date: {validation_report['validation_date']}\")\n",
    "print(f\"\\nSchema:\")\n",
    "print(f\"  Tables: {validation_report['schema_summary']['tables']}\")\n",
    "print(f\"  Total Rows: {validation_report['schema_summary']['total_rows']:,}\")\n",
    "print(f\"  Total Columns: {validation_report['schema_summary']['total_columns']}\")\n",
    "print(f\"\\nPlaceholder Data Issues:\")\n",
    "print(f\"  Total: {validation_report['placeholder_issues']['total']}\")\n",
    "print(f\"  High Severity: {validation_report['placeholder_issues']['high_severity']}\")\n",
    "print(f\"  Medium Severity: {validation_report['placeholder_issues']['medium_severity']}\")\n",
    "print(f\"\\nComplexity Score: {validation_report['complexity']['score']}/{validation_report['complexity']['max_score']} ({validation_report['complexity']['assessment']})\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OVERALL STATUS: {validation_report['overall_status']}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation report to JSON\n",
    "import json\n",
    "\n",
    "with open('../db-1/validation_report.json', 'w') as f:\n",
    "    json.dump(validation_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"✓ Validation report saved to db-1/validation_report.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
