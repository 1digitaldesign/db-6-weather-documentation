{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL/ELT Pipeline - DB-1\n",
        "\n",
        "This notebook provides a comprehensive ETL/ELT pipeline for database db-1.\n",
        "\n",
        "## Pipeline Overview\n",
        "1. **Extract**: Load data from source systems\n",
        "2. **Transform**: Clean, validate, and transform data\n",
        "3. **Load**: Load transformed data into target database\n",
        "4. **Validate**: Verify data quality and completeness\n",
        "5. **Monitor**: Track pipeline performance and errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, List, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# API and HTTP requests\n",
        "try:\n",
        "    import requests\n",
        "    REQUESTS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    REQUESTS_AVAILABLE = False\n",
        "    print(\"Warning: requests library not available. Install with: pip install requests\")\n",
        "\n",
        "# Database connections\n",
        "try:\n",
        "    from sqlalchemy import create_engine, text\n",
        "    SQLALCHEMY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SQLALCHEMY_AVAILABLE = False\n",
        "    print(\"Warning: sqlalchemy not available\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"✓ Imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DB_NAME = \"db-1\"\n",
        "DB_PATH = Path.cwd().parent\n",
        "\n",
        "# Database connection strings (configure as needed)\n",
        "# PostgreSQL\n",
        "POSTGRES_CONNECTION_STRING = None  # \"postgresql://user:password@localhost:5432/dbname\"\n",
        "\n",
        "# Databricks\n",
        "DATABRICKS_CONNECTION_STRING = None  # Configure Databricks connection\n",
        "\n",
        "# Databricks\n",
        "SNOWFLAKE_CONNECTION_STRING = None  # Configure Databricks connection\n",
        "\n",
        "# Source data paths\n",
        "DATA_DIR = DB_PATH / \"data\"\n",
        "SCHEMA_FILE = DATA_DIR / \"schema.sql\"\n",
        "DATA_FILE = DATA_DIR / \"data.sql\"\n",
        "\n",
        "# API Configuration\n",
        "# Data.gov CKAN API (no key required for metadata access)\n",
        "DATA_GOV_CKAN_API_BASE = \"https://catalog.data.gov/api/3/action\"\n",
        "\n",
        "# api.data.gov (requires API key - get from https://api.data.gov/signup/)\n",
        "# Set via environment variable: export API_DATA_GOV_KEY=\"your_key_here\"\n",
        "API_DATA_GOV_KEY = os.getenv(\"API_DATA_GOV_KEY\", None)\n",
        "\n",
        "# National Weather Service API (no key required)\n",
        "NWS_API_BASE = \"https://api.weather.gov\"\n",
        "\n",
        "# GeoPlatform.gov API (no key required)\n",
        "GEOPLATFORM_API_BASE = \"https://geoapi.geoplatform.gov\"\n",
        "GEOPLATFORM_STAC_BASE = \"https://stac.geoplatform.gov\"\n",
        "GEOPLATFORM_WEB_BASE = \"https://www.geoplatform.gov\"\n",
        "\n",
        "# Research directory for resource documentation\n",
        "RESEARCH_DIR = DB_PATH / \"research\"\n",
        "RESOURCES_FILE = RESEARCH_DIR / \"data_resources.json\"\n",
        "SOURCE_METADATA_FILE = RESEARCH_DIR / \"source_metadata.json\"\n",
        "\n",
        "print(f\"Database: {DB_NAME}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Schema file exists: {SCHEMA_FILE.exists()}\")\n",
        "print(f\"Data file exists: {DATA_FILE.exists()}\")\n",
        "print(f\"Research directory: {RESEARCH_DIR}\")\n",
        "print(f\"✓ API endpoints configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Extract - Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_schema_file(schema_path: Path) -> Optional[str]:\n",
        "    \"\"\"Load database schema from SQL file.\"\"\"\n",
        "    try:\n",
        "        if schema_path.exists():\n",
        "            with open(schema_path, 'r') as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            logger.warning(f\"Schema file not found: {schema_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading schema: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_data_file(data_path: Path) -> Optional[str]:\n",
        "    \"\"\"Load data from SQL file.\"\"\"\n",
        "    try:\n",
        "        if data_path.exists():\n",
        "            with open(data_path, 'r') as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            logger.warning(f\"Data file not found: {data_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load schema and data\n",
        "schema_sql = load_schema_file(SCHEMA_FILE)\n",
        "data_sql = load_data_file(DATA_FILE)\n",
        "\n",
        "if schema_sql:\n",
        "    print(f\"✓ Schema loaded ({len(schema_sql)} characters)\")\n",
        "if data_sql:\n",
        "    print(f\"✓ Data loaded ({len(data_sql)} characters)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_from_csv(csv_path: Path) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Extract data from CSV file.\"\"\"\n",
        "    try:\n",
        "        if csv_path.exists():\n",
        "            df = pd.read_csv(csv_path)\n",
        "            logger.info(f\"Loaded {len(df)} rows from {csv_path.name}\")\n",
        "            return df\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading CSV {csv_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_from_json(json_path: Path) -> Optional[Dict]:\n",
        "    \"\"\"Extract data from JSON file.\"\"\"\n",
        "    try:\n",
        "        if json_path.exists():\n",
        "            with open(json_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            logger.info(f\"Loaded JSON from {json_path.name}\")\n",
        "            return data\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading JSON {json_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Find and load data files\n",
        "csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
        "json_files = list(DATA_DIR.glob(\"*.json\"))\n",
        "\n",
        "extracted_data = {}\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    df = extract_from_csv(csv_file)\n",
        "    if df is not None:\n",
        "        extracted_data[csv_file.stem] = df\n",
        "\n",
        "for json_file in json_files:\n",
        "    data = extract_from_json(json_file)\n",
        "    if data is not None:\n",
        "        extracted_data[json_file.stem] = data\n",
        "\n",
        "print(f\"✓ Extracted {len(extracted_data)} data sources\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2.1: Extract - Data.gov API Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_datagov_datasets(query: str = \"\", limit: int = 10, organization: str = None) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Search Data.gov datasets using CKAN API.\n",
        "\n",
        "    Args:\n",
        "        query: Search query string\n",
        "        limit: Maximum number of results to return\n",
        "        organization: Filter by organization (e.g., 'usgs-gov', 'noaa-gov')\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing search results\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        logger.warning(\"requests library not available for API calls\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        params = {\n",
        "            'q': query,\n",
        "            'rows': limit,\n",
        "            'sort': 'metadata_modified desc'\n",
        "        }\n",
        "\n",
        "        if organization:\n",
        "            params['fq'] = f'organization:{organization}'\n",
        "\n",
        "        url = f\"{DATA_GOV_CKAN_API_BASE}/package_search\"\n",
        "        response = requests.get(url, params=params, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        if data.get('success'):\n",
        "            results = data.get('result', {})\n",
        "            logger.info(f\"Found {results.get('count', 0)} datasets matching query: {query}\")\n",
        "            return results\n",
        "        else:\n",
        "            logger.error(f\"API returned success=False: {data.get('error', {})}\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error querying Data.gov API: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_datagov_dataset_details(package_id: str) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Get detailed information about a specific Data.gov dataset.\n",
        "\n",
        "    Args:\n",
        "        package_id: Dataset package ID from Data.gov\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing dataset details\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        url = f\"{DATA_GOV_CKAN_API_BASE}/package_show\"\n",
        "        params = {'id': package_id}\n",
        "        response = requests.get(url, params=params, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        if data.get('success'):\n",
        "            return data.get('result')\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching dataset details for {package_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_datagov_resource(resource_url: str, output_path: Path = None) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Download and load a Data.gov resource (CSV, JSON, etc.).\n",
        "\n",
        "    Args:\n",
        "        resource_url: URL to the resource file\n",
        "        output_path: Optional path to save the file locally\n",
        "\n",
        "    Returns:\n",
        "        DataFrame if CSV, Dict if JSON, None otherwise\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        response = requests.get(resource_url, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Determine file type from URL or content\n",
        "        if resource_url.endswith('.csv') or 'csv' in response.headers.get('content-type', ''):\n",
        "            df = pd.read_csv(pd.io.common.StringIO(response.text))\n",
        "            if output_path:\n",
        "                df.to_csv(output_path, index=False)\n",
        "            logger.info(f\"Downloaded CSV resource: {len(df)} rows\")\n",
        "            return df\n",
        "        elif resource_url.endswith('.json') or 'json' in response.headers.get('content-type', ''):\n",
        "            data = response.json()\n",
        "            if output_path:\n",
        "                with open(output_path, 'w') as f:\n",
        "                    json.dump(data, f, indent=2)\n",
        "            logger.info(f\"Downloaded JSON resource\")\n",
        "            return data\n",
        "        else:\n",
        "            logger.warning(f\"Unsupported resource type: {resource_url}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error downloading resource {resource_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example: Search for weather-related datasets\n",
        "print(\"Searching Data.gov for weather datasets...\")\n",
        "weather_datasets = search_datagov_datasets(query=\"weather\", limit=5, organization=\"noaa-gov\")\n",
        "\n",
        "if weather_datasets:\n",
        "    print(f\"✓ Found {weather_datasets.get('count', 0)} weather datasets\")\n",
        "    if weather_datasets.get('results'):\n",
        "        print(\"\\nSample datasets:\")\n",
        "        for dataset in weather_datasets['results'][:3]:\n",
        "            print(f\"  - {dataset.get('title', 'N/A')} (ID: {dataset.get('id', 'N/A')})\")\n",
        "else:\n",
        "    print(\"⚠ Could not search Data.gov (requests library may not be available)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2.2: Extract - National Weather Service API Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_nws_points(latitude: float, longitude: float) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Get NWS forecast office and gridpoint information for a location.\n",
        "\n",
        "    Args:\n",
        "        latitude: Latitude in decimal degrees (WGS84)\n",
        "        longitude: Longitude in decimal degrees (WGS84)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing forecast URLs and gridpoint information\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Round to 4 decimal places as required by NWS API\n",
        "        lat = round(latitude, 4)\n",
        "        lon = round(longitude, 4)\n",
        "\n",
        "        url = f\"{NWS_API_BASE}/points/{lat},{lon}\"\n",
        "        headers = {'User-Agent': 'ETL-Pipeline/1.0 (Research Project)'}\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        logger.info(f\"Retrieved NWS points data for ({lat}, {lon})\")\n",
        "        return data\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error querying NWS API: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_nws_forecast(forecast_url: str) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Get forecast data from NWS forecast URL.\n",
        "\n",
        "    Args:\n",
        "        forecast_url: Forecast URL from points endpoint\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing forecast data\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        headers = {'User-Agent': 'ETL-Pipeline/1.0 (Research Project)'}\n",
        "        response = requests.get(forecast_url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        logger.info(\"Retrieved NWS forecast data\")\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching forecast: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_nws_gridpoint_forecast(grid_id: str, grid_x: int, grid_y: int) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Get raw numerical forecast data from NWS gridpoint endpoint.\n",
        "\n",
        "    Args:\n",
        "        grid_id: Grid ID (e.g., 'OKX')\n",
        "        grid_x: Grid X coordinate\n",
        "        grid_y: Grid Y coordinate\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing time-series forecast data\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        url = f\"{NWS_API_BASE}/gridpoints/{grid_id}/{grid_x},{grid_y}/forecast\"\n",
        "        headers = {'User-Agent': 'ETL-Pipeline/1.0 (Research Project)'}\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        logger.info(f\"Retrieved NWS gridpoint forecast for {grid_id}/{grid_x},{grid_y}\")\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching gridpoint forecast: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_nws_alerts(area: str = None, severity: str = None, urgency: str = None) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Get active weather alerts from NWS.\n",
        "\n",
        "    Args:\n",
        "        area: State code (e.g., 'NY', 'CA') or area code\n",
        "        severity: Filter by severity (extreme, severe, moderate, minor, unknown)\n",
        "        urgency: Filter by urgency (immediate, expected, future, past, unknown)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing alert data\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        url = f\"{NWS_API_BASE}/alerts/active\"\n",
        "        params = {}\n",
        "        if area:\n",
        "            params['area'] = area\n",
        "        if severity:\n",
        "            params['severity'] = severity\n",
        "        if urgency:\n",
        "            params['urgency'] = urgency\n",
        "\n",
        "        headers = {'User-Agent': 'ETL-Pipeline/1.0 (Research Project)'}\n",
        "        response = requests.get(url, params=params, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        logger.info(f\"Retrieved NWS alerts (area: {area or 'all'})\")\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching alerts: {e}\")\n",
        "        return None\n",
        "\n",
        "def nws_forecast_to_dataframe(forecast_data: Dict) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Convert NWS forecast data to pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        forecast_data: Forecast data dictionary from NWS API\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with forecast periods\n",
        "    \"\"\"\n",
        "    if not forecast_data or 'properties' not in forecast_data:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        periods = forecast_data['properties'].get('periods', [])\n",
        "        if not periods:\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(periods)\n",
        "        logger.info(f\"Converted {len(df)} forecast periods to DataFrame\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error converting forecast to DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example: Get forecast for New York City (latitude: 40.7128, longitude: -74.0060)\n",
        "print(\"Fetching NWS data for New York City...\")\n",
        "nyc_points = get_nws_points(40.7128, -74.0060)\n",
        "\n",
        "if nyc_points:\n",
        "    print(\"✓ Retrieved NWS points data\")\n",
        "    properties = nyc_points.get('properties', {})\n",
        "    print(f\"  Forecast Office: {properties.get('forecastOffice', 'N/A')}\")\n",
        "    print(f\"  Grid ID: {properties.get('gridId', 'N/A')}\")\n",
        "    print(f\"  Grid X: {properties.get('gridX', 'N/A')}\")\n",
        "    print(f\"  Grid Y: {properties.get('gridY', 'N/A')}\")\n",
        "\n",
        "    # Get forecast\n",
        "    forecast_url = properties.get('forecast')\n",
        "    if forecast_url:\n",
        "        forecast_data = get_nws_forecast(forecast_url)\n",
        "        if forecast_data:\n",
        "            forecast_df = nws_forecast_to_dataframe(forecast_data)\n",
        "            if forecast_df is not None:\n",
        "                extracted_data['nws_nyc_forecast'] = forecast_df\n",
        "                print(f\"✓ Added forecast data: {len(forecast_df)} periods\")\n",
        "else:\n",
        "    print(\"⚠ Could not fetch NWS data (requests library may not be available)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2.3: Resource Collation and Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2.3: Extract - GeoPlatform.gov API Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_geoplatform_datasets(query: str = \"\", limit: int = 10, ngda_only: bool = False) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Search GeoPlatform.gov datasets using the GeoAPI.\n",
        "\n",
        "    Args:\n",
        "        query: Search query string\n",
        "        limit: Maximum number of results to return\n",
        "        ngda_only: Filter to National Geospatial Data Assets (NGDA) only\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing search results\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        logger.warning(\"requests library not available for API calls\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # GeoPlatform API endpoint for searching\n",
        "        url = f\"{GEOPLATFORM_API_BASE}/items/search\"\n",
        "        params = {\n",
        "            'q': query,\n",
        "            'limit': limit\n",
        "        }\n",
        "\n",
        "        if ngda_only:\n",
        "            params['collections'] = 'ngda'\n",
        "\n",
        "        headers = {'Accept': 'application/json'}\n",
        "        response = requests.get(url, params=params, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        logger.info(f\"Found {len(data.get('features', []))} GeoPlatform datasets matching query: {query}\")\n",
        "        return data\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error querying GeoPlatform API: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_geoplatform_stac_collections() -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Get STAC (SpatioTemporal Asset Catalog) collections from GeoPlatform.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing STAC collections\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        url = f\"{GEOPLATFORM_STAC_BASE}/collections\"\n",
        "        headers = {'Accept': 'application/json'}\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        logger.info(f\"Retrieved {len(data.get('collections', []))} STAC collections\")\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching STAC collections: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_geoplatform_stac_collection(collection_id: str) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Get details of a specific STAC collection.\n",
        "\n",
        "    Args:\n",
        "        collection_id: Collection identifier\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing collection details\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        url = f\"{GEOPLATFORM_STAC_BASE}/collections/{collection_id}\"\n",
        "        headers = {'Accept': 'application/json'}\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        logger.info(f\"Retrieved STAC collection: {collection_id}\")\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching STAC collection {collection_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "def search_geoplatform_stac(query: Dict = None, collections: List[str] = None, limit: int = 10) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Search GeoPlatform STAC catalog for items.\n",
        "\n",
        "    Args:\n",
        "        query: STAC query parameters (bbox, datetime, etc.)\n",
        "        collections: List of collection IDs to search\n",
        "        limit: Maximum number of results\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing STAC search results\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        url = f\"{GEOPLATFORM_STAC_BASE}/search\"\n",
        "        params = {'limit': limit}\n",
        "\n",
        "        if collections:\n",
        "            params['collections'] = ','.join(collections)\n",
        "\n",
        "        payload = {}\n",
        "        if query:\n",
        "            payload.update(query)\n",
        "\n",
        "        headers = {\n",
        "            'Accept': 'application/json',\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, json=payload, params=params, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        logger.info(f\"Found {len(data.get('features', []))} STAC items\")\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error searching STAC catalog: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_geoplatform_resource(resource_url: str, output_path: Path = None) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Download a GeoPlatform resource (GeoJSON, GeoPackage, Shapefile, etc.).\n",
        "\n",
        "    Args:\n",
        "        resource_url: URL to the resource file\n",
        "        output_path: Optional path to save the file locally\n",
        "\n",
        "    Returns:\n",
        "        DataFrame if GeoJSON, file path if other formats, None otherwise\n",
        "    \"\"\"\n",
        "    if not REQUESTS_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        response = requests.get(resource_url, timeout=120)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Determine file type from URL or content\n",
        "        if resource_url.endswith('.geojson') or resource_url.endswith('.json'):\n",
        "            data = response.json()\n",
        "            if isinstance(data, dict) and 'features' in data:\n",
        "                # GeoJSON FeatureCollection\n",
        "                features = data.get('features', [])\n",
        "                if features:\n",
        "                    df = pd.json_normalize([f.get('properties', {}) for f in features])\n",
        "                    # Add geometry if needed\n",
        "                    if output_path:\n",
        "                        df.to_csv(output_path.with_suffix('.csv'), index=False)\n",
        "                    logger.info(f\"Downloaded GeoJSON resource: {len(df)} features\")\n",
        "                    return df\n",
        "            else:\n",
        "                # Regular JSON\n",
        "                if output_path:\n",
        "                    with open(output_path, 'w') as f:\n",
        "                        json.dump(data, f, indent=2)\n",
        "                return data\n",
        "        else:\n",
        "            # Save other formats (GeoPackage, Shapefile, etc.) as files\n",
        "            if output_path:\n",
        "                with open(output_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                logger.info(f\"Downloaded resource to {output_path}\")\n",
        "                return str(output_path)\n",
        "            else:\n",
        "                logger.warning(\"Output path required for non-JSON resources\")\n",
        "                return None\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error downloading resource {resource_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example: Search for NGDA datasets\n",
        "print(\"Searching GeoPlatform.gov for NGDA datasets...\")\n",
        "ngda_datasets = search_geoplatform_datasets(query=\"boundaries\", limit=5, ngda_only=True)\n",
        "\n",
        "if ngda_datasets:\n",
        "    print(f\"✓ Found {len(ngda_datasets.get('features', []))} NGDA datasets\")\n",
        "    if ngda_datasets.get('features'):\n",
        "        print(\"\\nSample datasets:\")\n",
        "        for feature in ngda_datasets['features'][:3]:\n",
        "            props = feature.get('properties', {})\n",
        "            print(f\"  - {props.get('title', 'N/A')} (ID: {feature.get('id', 'N/A')})\")\n",
        "else:\n",
        "    print(\"⚠ Could not search GeoPlatform (requests library may not be available)\")\n",
        "\n",
        "# Example: Get STAC collections\n",
        "print(\"\\nFetching GeoPlatform STAC collections...\")\n",
        "stac_collections = get_geoplatform_stac_collections()\n",
        "\n",
        "if stac_collections:\n",
        "    print(f\"✓ Retrieved {len(stac_collections.get('collections', []))} STAC collections\")\n",
        "    if stac_collections.get('collections'):\n",
        "        print(\"\\nSample collections:\")\n",
        "        for collection in stac_collections['collections'][:3]:\n",
        "            print(f\"  - {collection.get('title', 'N/A')} (ID: {collection.get('id', 'N/A')})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2.4: Resource Collation and Documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_data_resources() -> Dict:\n",
        "    \"\"\"\n",
        "    Collate and document all data resources used in this ETL pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing resource metadata\n",
        "    \"\"\"\n",
        "    resources = {\n",
        "        'database': DB_NAME,\n",
        "        'collation_timestamp': datetime.now().isoformat(),\n",
        "        'data_sources': {\n",
        "            'local_files': {\n",
        "                'description': 'Local CSV, JSON, and SQL files',\n",
        "                'location': str(DATA_DIR),\n",
        "                'files': []\n",
        "            },\n",
        "            'data_gov': {\n",
        "                'description': 'Data.gov CKAN API - Federal open data portal',\n",
        "                'api_base': DATA_GOV_CKAN_API_BASE,\n",
        "                'documentation': 'https://data.gov/developers/apis/',\n",
        "                'datasets': []\n",
        "            },\n",
        "            'national_weather_service': {\n",
        "                'description': 'National Weather Service API - Weather forecasts and alerts',\n",
        "                'api_base': NWS_API_BASE,\n",
        "                'documentation': 'https://weather-gov.github.io/api/',\n",
        "                'endpoints': {\n",
        "                    'points': '/points/{lat},{lon}',\n",
        "                    'forecast': '/gridpoints/{gridId}/{gridX},{gridY}/forecast',\n",
        "                    'alerts': '/alerts/active'\n",
        "                },\n",
        "                'data_collected': []\n",
        "            },\n",
        "            'geoplatform_gov': {\n",
        "                'description': 'GeoPlatform.gov - Federal geospatial data platform (FAIR principles)',\n",
        "                'api_base': GEOPLATFORM_API_BASE,\n",
        "                'stac_base': GEOPLATFORM_STAC_BASE,\n",
        "                'web_base': GEOPLATFORM_WEB_BASE,\n",
        "                'documentation': 'https://www.geoplatform.gov/',\n",
        "                'stac_docs': 'https://stac.geoplatform.gov',\n",
        "                'api_docs': 'https://geoapi.geoplatform.gov',\n",
        "                'endpoints': {\n",
        "                    'search': '/items/search',\n",
        "                    'stac_collections': '/collections',\n",
        "                    'stac_search': '/search'\n",
        "                },\n",
        "                'data_formats': ['GeoJSON', 'GeoPackage', 'Shapefile', 'WMS', 'WFS'],\n",
        "                'ngda_themes': 18,\n",
        "                'datasets': []\n",
        "            }\n",
        "        },\n",
        "        'extraction_summary': {\n",
        "            'total_sources': len(extracted_data),\n",
        "            'source_names': list(extracted_data.keys())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Document local files\n",
        "    csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
        "    json_files = list(DATA_DIR.glob(\"*.json\"))\n",
        "    sql_files = list(DATA_DIR.glob(\"*.sql\"))\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        resources['data_sources']['local_files']['files'].append({\n",
        "            'name': csv_file.name,\n",
        "            'type': 'CSV',\n",
        "            'path': str(csv_file)\n",
        "        })\n",
        "\n",
        "    for json_file in json_files:\n",
        "        resources['data_sources']['local_files']['files'].append({\n",
        "            'name': json_file.name,\n",
        "            'type': 'JSON',\n",
        "            'path': str(json_file)\n",
        "        })\n",
        "\n",
        "    for sql_file in sql_files:\n",
        "        resources['data_sources']['local_files']['files'].append({\n",
        "            'name': sql_file.name,\n",
        "            'type': 'SQL',\n",
        "            'path': str(sql_file)\n",
        "        })\n",
        "\n",
        "    # Document Data.gov datasets if searched\n",
        "    if 'weather_datasets' in locals() and weather_datasets:\n",
        "        for dataset in weather_datasets.get('results', [])[:10]:  # Limit to 10\n",
        "            resources['data_sources']['data_gov']['datasets'].append({\n",
        "                'id': dataset.get('id'),\n",
        "                'title': dataset.get('title'),\n",
        "                'organization': dataset.get('organization', {}).get('name', 'N/A'),\n",
        "                'modified': dataset.get('metadata_modified'),\n",
        "                'resources': len(dataset.get('resources', []))\n",
        "            })\n",
        "\n",
        "    # Document NWS data collected\n",
        "    if 'nyc_points' in locals() and nyc_points:\n",
        "        resources['data_sources']['national_weather_service']['data_collected'].append({\n",
        "            'type': 'points',\n",
        "            'location': 'New York City (40.7128, -74.0060)',\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'forecast_office': nyc_points.get('properties', {}).get('forecastOffice')\n",
        "        })\n",
        "\n",
        "    if 'nws_nyc_forecast' in extracted_data:\n",
        "        resources['data_sources']['national_weather_service']['data_collected'].append({\n",
        "            'type': 'forecast',\n",
        "            'location': 'New York City',\n",
        "            'periods': len(extracted_data['nws_nyc_forecast']),\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "    # Document GeoPlatform datasets if searched\n",
        "    if 'ngda_datasets' in locals() and ngda_datasets:\n",
        "        for feature in ngda_datasets.get('features', [])[:10]:  # Limit to 10\n",
        "            props = feature.get('properties', {})\n",
        "            resources['data_sources']['geoplatform_gov']['datasets'].append({\n",
        "                'id': feature.get('id'),\n",
        "                'title': props.get('title'),\n",
        "                'description': props.get('description', '')[:200],  # Truncate long descriptions\n",
        "                'ngda_theme': props.get('ngdaTheme'),\n",
        "                'modified': props.get('updated'),\n",
        "                'bbox': feature.get('bbox'),\n",
        "                'links': [link.get('href') for link in feature.get('links', []) if link.get('rel') == 'self']\n",
        "            })\n",
        "\n",
        "    if 'stac_collections' in locals() and stac_collections:\n",
        "        for collection in stac_collections.get('collections', [])[:10]:  # Limit to 10\n",
        "            resources['data_sources']['geoplatform_gov']['datasets'].append({\n",
        "                'id': collection.get('id'),\n",
        "                'title': collection.get('title'),\n",
        "                'description': collection.get('description', '')[:200],\n",
        "                'type': 'stac_collection',\n",
        "                'extent': collection.get('extent'),\n",
        "                'links': [link.get('href') for link in collection.get('links', [])]\n",
        "            })\n",
        "\n",
        "    return resources\n",
        "\n",
        "# Collate resources\n",
        "data_resources = collate_data_resources()\n",
        "\n",
        "# Save resources documentation\n",
        "RESOURCES_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(RESOURCES_FILE, 'w') as f:\n",
        "    json.dump(data_resources, f, indent=2, default=str)\n",
        "\n",
        "print(f\"✓ Resources collated and saved to {RESOURCES_FILE}\")\n",
        "print(f\"\\nResource Summary:\")\n",
        "print(f\"  Local files: {len(data_resources['data_sources']['local_files']['files'])}\")\n",
        "print(f\"  Data.gov datasets: {len(data_resources['data_sources']['data_gov']['datasets'])}\")\n",
        "print(f\"  NWS data collections: {len(data_resources['data_sources']['national_weather_service']['data_collected'])}\")\n",
        "print(f\"  GeoPlatform datasets: {len(data_resources['data_sources']['geoplatform_gov']['datasets'])}\")\n",
        "print(f\"  Total extracted sources: {data_resources['extraction_summary']['total_sources']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2.5: Source Metadata Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_source_metadata() -> Dict:\n",
        "    \"\"\"\n",
        "    Create comprehensive source metadata tracking for all data sources.\n",
        "    This provides a detailed reference list of where data is sourced from.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing detailed source metadata\n",
        "    \"\"\"\n",
        "    source_metadata = {\n",
        "        'database': DB_NAME,\n",
        "        'metadata_version': '1.0',\n",
        "        'created_timestamp': datetime.now().isoformat(),\n",
        "        'last_updated': datetime.now().isoformat(),\n",
        "        'sources': {\n",
        "            'data_gov': {\n",
        "                'name': 'Data.gov',\n",
        "                'type': 'federal_open_data_portal',\n",
        "                'url': 'https://data.gov',\n",
        "                'api_endpoints': {\n",
        "                    'ckan_api': {\n",
        "                        'base_url': DATA_GOV_CKAN_API_BASE,\n",
        "                        'description': 'CKAN API for dataset metadata search',\n",
        "                        'authentication': 'none_required',\n",
        "                        'rate_limit': 'none_specified',\n",
        "                        'documentation': 'https://data.gov/developers/apis/',\n",
        "                        'endpoints': {\n",
        "                            'package_search': '/package_search',\n",
        "                            'package_show': '/package_show'\n",
        "                        }\n",
        "                    },\n",
        "                    'api_data_gov': {\n",
        "                        'base_url': 'https://api.data.gov',\n",
        "                        'description': 'API management service for federal agency datasets',\n",
        "                        'authentication': 'api_key_required',\n",
        "                        'api_key_signup': 'https://api.data.gov/signup/',\n",
        "                        'rate_limit': '1000_requests_per_hour',\n",
        "                        'documentation': 'https://api.data.gov/docs/developer-manual/',\n",
        "                        'demo_key': 'DEMO_KEY (limited: 30 req/hour, 50/day)'\n",
        "                    }\n",
        "                },\n",
        "                'data_formats': ['CSV', 'JSON', 'XML', 'RDF', 'API'],\n",
        "                'datasets_accessed': []\n",
        "            },\n",
        "            'national_weather_service': {\n",
        "                'name': 'National Weather Service',\n",
        "                'type': 'weather_api',\n",
        "                'url': 'https://www.weather.gov',\n",
        "                'api_endpoints': {\n",
        "                    'base_url': NWS_API_BASE,\n",
        "                    'description': 'Public REST API for NWS weather data',\n",
        "                    'authentication': 'none_required',\n",
        "                    'user_agent_required': True,\n",
        "                    'rate_limit': 'none_official_be_respectful',\n",
        "                    'documentation': 'https://weather-gov.github.io/api/',\n",
        "                    'openapi_spec': 'https://api.weather.gov/openapi.json',\n",
        "                    'endpoints': {\n",
        "                        'points': {\n",
        "                            'path': '/points/{lat},{lon}',\n",
        "                            'description': 'Get forecast office and gridpoint info',\n",
        "                            'coordinate_system': 'WGS84 (EPSG:4326)',\n",
        "                            'precision': '4_decimal_places_maximum'\n",
        "                        },\n",
        "                        'forecast': {\n",
        "                            'path': '/gridpoints/{gridId}/{gridX},{gridY}/forecast',\n",
        "                            'description': 'Raw numerical forecast data on 2.5km grid'\n",
        "                        },\n",
        "                        'alerts': {\n",
        "                            'path': '/alerts/active',\n",
        "                            'description': 'Active weather alerts',\n",
        "                            'parameters': ['area', 'severity', 'urgency']\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                'data_formats': ['GeoJSON (RFC 7946)', 'CAP XML'],\n",
        "                'datasets_accessed': []\n",
        "            },\n",
        "            'geoplatform_gov': {\n",
        "                'name': 'GeoPlatform.gov',\n",
        "                'type': 'geospatial_data_platform',\n",
        "                'url': GEOPLATFORM_WEB_BASE,\n",
        "                'description': 'Federal geospatial data platform following FAIR principles',\n",
        "                'authority': 'Geospatial Data Act of 2018',\n",
        "                'api_endpoints': {\n",
        "                    'geoapi': {\n",
        "                        'base_url': GEOPLATFORM_API_BASE,\n",
        "                        'description': 'GeoPlatform REST API',\n",
        "                        'authentication': 'none_required',\n",
        "                        'documentation': 'https://geoapi.geoplatform.gov',\n",
        "                        'endpoints': {\n",
        "                            'items_search': '/items/search',\n",
        "                            'description': 'Search for geospatial datasets'\n",
        "                        }\n",
        "                    },\n",
        "                    'stac': {\n",
        "                        'base_url': GEOPLATFORM_STAC_BASE,\n",
        "                        'description': 'SpatioTemporal Asset Catalog (STAC) interface',\n",
        "                        'authentication': 'none_required',\n",
        "                        'documentation': 'https://stac.geoplatform.gov',\n",
        "                        'endpoints': {\n",
        "                            'collections': '/collections',\n",
        "                            'collection': '/collections/{collectionId}',\n",
        "                            'search': '/search'\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                'data_formats': ['GeoJSON', 'GeoPackage', 'Shapefile', 'WMS', 'WFS', 'Map Vector', 'XYZ Raster'],\n",
        "                'catalogs': {\n",
        "                    'artifact_catalog': 'Automatically converted data files (GeoPackage, Shapefiles, GeoJSON)',\n",
        "                    'tile_service_catalog': 'Map tiles in Map Vector and XYZ Raster formats',\n",
        "                    'map_services_catalog': 'WMS/WFS services with download options'\n",
        "                },\n",
        "                'ngda_themes': 18,\n",
        "                'ngda_description': 'National Geospatial Data Assets across 18 data themes',\n",
        "                'datasets_accessed': []\n",
        "            }\n",
        "        },\n",
        "        'extraction_history': [],\n",
        "        'data_lineage': {}\n",
        "    }\n",
        "\n",
        "    # Add extraction history from current session\n",
        "    if extracted_data:\n",
        "        for source_name, data in extracted_data.items():\n",
        "            source_info = {\n",
        "                'source_name': source_name,\n",
        "                'extraction_timestamp': datetime.now().isoformat(),\n",
        "                'data_type': type(data).__name__,\n",
        "                'row_count': len(data) if isinstance(data, pd.DataFrame) else 'N/A',\n",
        "                'source_system': 'unknown'\n",
        "            }\n",
        "\n",
        "            # Determine source system\n",
        "            if 'nws' in source_name.lower():\n",
        "                source_info['source_system'] = 'national_weather_service'\n",
        "            elif 'geoplatform' in source_name.lower() or 'ngda' in source_name.lower():\n",
        "                source_info['source_system'] = 'geoplatform_gov'\n",
        "            elif any(keyword in source_name.lower() for keyword in ['data', 'gov', 'federal']):\n",
        "                source_info['source_system'] = 'data_gov'\n",
        "\n",
        "            source_metadata['extraction_history'].append(source_info)\n",
        "\n",
        "    return source_metadata\n",
        "\n",
        "# Create comprehensive source metadata\n",
        "source_metadata = create_source_metadata()\n",
        "\n",
        "# Save source metadata\n",
        "SOURCE_METADATA_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(SOURCE_METADATA_FILE, 'w') as f:\n",
        "    json.dump(source_metadata, f, indent=2, default=str)\n",
        "\n",
        "print(f\"✓ Source metadata saved to {SOURCE_METADATA_FILE}\")\n",
        "print(f\"\\nSource Metadata Summary:\")\n",
        "print(f\"  Total sources documented: {len(source_metadata['sources'])}\")\n",
        "print(f\"  Extraction history entries: {len(source_metadata['extraction_history'])}\")\n",
        "print(f\"\\nDocumented Sources:\")\n",
        "for source_key, source_info in source_metadata['sources'].items():\n",
        "    print(f\"  - {source_info['name']}: {source_info['type']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Transform - Data Cleaning and Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean DataFrame: handle missing values, remove duplicates, etc.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    # Remove duplicates\n",
        "    initial_rows = len(df)\n",
        "    df = df.drop_duplicates()\n",
        "    duplicates_removed = initial_rows - len(df)\n",
        "\n",
        "    # Handle missing values\n",
        "    missing_before = df.isnull().sum().sum()\n",
        "    # Fill numeric columns with median\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "    # Fill text columns with mode\n",
        "    text_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in text_cols:\n",
        "        df[col] = df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else '')\n",
        "    missing_after = df.isnull().sum().sum()\n",
        "\n",
        "    logger.info(f\"Cleaned data: removed {duplicates_removed} duplicates, filled {missing_before - missing_after} missing values\")\n",
        "    return df\n",
        "\n",
        "# Clean extracted data\n",
        "cleaned_data = {}\n",
        "for name, data in extracted_data.items():\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        cleaned_data[name] = clean_dataframe(data)\n",
        "    else:\n",
        "        cleaned_data[name] = data\n",
        "\n",
        "print(f\"✓ Cleaned {len(cleaned_data)} data sources\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_dataframe(df: pd.DataFrame, required_columns: List[str] = None) -> Dict:\n",
        "    \"\"\"Validate DataFrame structure and data quality.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return {'valid': False, 'errors': ['DataFrame is empty or None']}\n",
        "\n",
        "    validation_results = {\n",
        "        'valid': True,\n",
        "        'row_count': len(df),\n",
        "        'column_count': len(df.columns),\n",
        "        'missing_values': df.isnull().sum().to_dict(),\n",
        "        'duplicate_rows': df.duplicated().sum(),\n",
        "        'errors': []\n",
        "    }\n",
        "\n",
        "    # Check required columns\n",
        "    if required_columns:\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            validation_results['valid'] = False\n",
        "            validation_results['errors'].append(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "    return validation_results\n",
        "\n",
        "# Validate cleaned data\n",
        "validation_results = {}\n",
        "for name, data in cleaned_data.items():\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        validation_results[name] = validate_dataframe(data)\n",
        "\n",
        "# Display validation results\n",
        "for name, results in validation_results.items():\n",
        "    status = \"✓\" if results['valid'] else \"✗\"\n",
        "    print(f\"{status} {name}: {results['row_count']} rows, {results['column_count']} columns\")\n",
        "    if results['errors']:\n",
        "        for error in results['errors']:\n",
        "            print(f\"  Error: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Load - Data Loading to Target Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_to_postgresql(df: pd.DataFrame, table_name: str, connection_string: str, if_exists: str = 'replace') -> bool:\n",
        "    \"\"\"Load DataFrame to PostgreSQL table.\"\"\"\n",
        "    if not SQLALCHEMY_AVAILABLE or connection_string is None:\n",
        "        logger.warning(\"PostgreSQL connection not available\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        engine = create_engine(connection_string)\n",
        "        df.to_sql(table_name, engine, if_exists=if_exists, index=False)\n",
        "        logger.info(f\"Loaded {len(df)} rows to PostgreSQL table {table_name}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading to PostgreSQL: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_to_databricks(df: pd.DataFrame, table_name: str, connection_string: str) -> bool:\n",
        "    \"\"\"Load DataFrame to Databricks table.\"\"\"\n",
        "    if not SQLALCHEMY_AVAILABLE or connection_string is None:\n",
        "        logger.warning(\"Databricks connection not available\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        engine = create_engine(connection_string)\n",
        "        df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
        "        logger.info(f\"Loaded {len(df)} rows to Databricks table {table_name}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading to Databricks: {e}\")\n",
        "        return False\n",
        "\n",
        "# Load data to target databases\n",
        "load_results = {}\n",
        "\n",
        "for name, data in cleaned_data.items():\n",
        "    if isinstance(data, pd.DataFrame) and not data.empty:\n",
        "        table_name = name.lower().replace(' ', '_')\n",
        "\n",
        "        # PostgreSQL\n",
        "        if POSTGRES_CONNECTION_STRING:\n",
        "            load_results[f\"{name}_postgres\"] = load_to_postgresql(\n",
        "                data, table_name, POSTGRES_CONNECTION_STRING\n",
        "            )\n",
        "\n",
        "        # Databricks\n",
        "        if SNOWFLAKE_CONNECTION_STRING:\n",
        "            load_results[f\"{name}_databricks\"] = load_to_databricks(\n",
        "                data, table_name, SNOWFLAKE_CONNECTION_STRING\n",
        "            )\n",
        "\n",
        "print(f\"✓ Loaded {sum(load_results.values())} datasets to target databases\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Validate - Data Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data_quality_report(df: pd.DataFrame, table_name: str) -> Dict:\n",
        "    \"\"\"Generate comprehensive data quality report.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return {'table': table_name, 'status': 'empty'}\n",
        "\n",
        "    report = {\n",
        "        'table': table_name,\n",
        "        'row_count': len(df),\n",
        "        'column_count': len(df.columns),\n",
        "        'missing_values': int(df.isnull().sum().sum()),\n",
        "        'missing_percentage': float((df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100),\n",
        "        'duplicate_rows': int(df.duplicated().sum()),\n",
        "        'data_types': df.dtypes.astype(str).to_dict(),\n",
        "        'numeric_stats': {},\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Add statistics for numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        report['numeric_stats'] = df[numeric_cols].describe().to_dict()\n",
        "\n",
        "    return report\n",
        "\n",
        "# Generate quality reports\n",
        "quality_reports = {}\n",
        "for name, data in cleaned_data.items():\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        quality_reports[name] = generate_data_quality_report(data, name)\n",
        "\n",
        "# Display quality reports\n",
        "for name, report in quality_reports.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"Rows: {report['row_count']}\")\n",
        "    print(f\"Columns: {report['column_count']}\")\n",
        "    print(f\"Missing values: {report['missing_values']} ({report['missing_percentage']:.2f}%)\")\n",
        "    print(f\"Duplicate rows: {report['duplicate_rows']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Monitor - Pipeline Monitoring and Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save pipeline execution metadata\n",
        "pipeline_metadata = {\n",
        "    'database': DB_NAME,\n",
        "    'execution_timestamp': datetime.now().isoformat(),\n",
        "    'data_sources': list(extracted_data.keys()),\n",
        "    'extracted_count': len(extracted_data),\n",
        "    'cleaned_count': len(cleaned_data),\n",
        "    'validation_results': validation_results,\n",
        "    'load_results': load_results,\n",
        "    'quality_reports': quality_reports,\n",
        "    'status': 'completed'\n",
        "}\n",
        "\n",
        "# Save metadata to JSON\n",
        "metadata_file = DB_PATH / \"metadata\" / \"pipeline_metadata.json\"\n",
        "metadata_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(metadata_file, 'w') as f:\n",
        "    json.dump(pipeline_metadata, f, indent=2, default=str)\n",
        "\n",
        "print(f\"✓ Pipeline metadata saved to {metadata_file}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PIPELINE EXECUTION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Database: {pipeline_metadata['database']}\")\n",
        "print(f\"Execution time: {pipeline_metadata['execution_timestamp']}\")\n",
        "print(f\"Data sources extracted: {pipeline_metadata['extracted_count']}\")\n",
        "print(f\"Datasets cleaned: {pipeline_metadata['cleaned_count']}\")\n",
        "print(f\"Successful loads: {sum(pipeline_metadata['load_results'].values())}\")\n",
        "print(f\"Status: {pipeline_metadata['status']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
