{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB-2 Query Testing Report\n",
    "\n",
    "This notebook performs comprehensive testing of all 30 SQL queries for db-2 (LinkWay E-commerce/Affiliate System).\n",
    "\n",
    "## Testing Scope\n",
    "1. **Syntax Validation**: Parse and validate SQL syntax\n",
    "2. **Cross-Database Compatibility**: Check compatibility with PostgreSQL\n",
    "3. **Schema Validation**: Verify table/column references against LinkWay schema\n",
    "4. **Execution Testing**: Execute queries with real or mock data\n",
    "5. **Performance Analysis**: Profile execution times and query plans\n",
    "6. **Correctness Validation**: Validate result schemas and data types\n",
    "7. **Edge Case Testing**: Test with empty tables, NULL values, etc.\n",
    "8. **Summary Report**: Generate comprehensive test results and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path to import test_queries\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from test_queries import (\n",
    "    QueryTester, QueryExtractor, SyntaxValidator, CrossDBCompatibilityChecker,\n",
    "    QueryExecutor, PerformanceProfiler, ResultValidator, MockDataGenerator\n",
    ")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "DB_NAME = \"db-2\"\n",
    "DB_PATH = Path.cwd()\n",
    "\n",
    "# Database connection (optional - set to None if database not available)\n",
    "# For PostgreSQL (LinkWay uses PostgreSQL):\n",
    "# from sqlalchemy import create_engine\n",
    "# DB_CONNECTION_STRING = \"postgresql://user:password@localhost:5432/linkway\"\n",
    "# engine = create_engine(DB_CONNECTION_STRING)\n",
    "# connection = engine.connect()\n",
    "\n",
    "# Set to None if database not available (will use mock data)\n",
    "connection = None\n",
    "\n",
    "print(f\"Database: {DB_NAME}\")\n",
    "print(f\"Path: {DB_PATH}\")\n",
    "print(f\"Connection: {'Available' if connection else 'Not available (will use mock data)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tester\n",
    "tester = QueryTester(DB_NAME, DB_PATH, connection)\n",
    "\n",
    "# Load queries\n",
    "queries = tester.load_queries()\n",
    "print(f\"✓ Loaded {len(queries)} queries\")\n",
    "print(f\"Query IDs: {[q.id for q in queries]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Syntax Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate syntax for all queries\n",
    "syntax_results = []\n",
    "\n",
    "for query in queries:\n",
    "    result = tester.syntax_validator.validate(query)\n",
    "    syntax_results.append({\n",
    "        'query_id': query.id,\n",
    "        'is_valid': result.is_valid,\n",
    "        'errors': result.errors,\n",
    "        'warnings': result.warnings\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "syntax_df = pd.DataFrame(syntax_results)\n",
    "print(f\"\\nSyntax Validation Results:\")\n",
    "print(f\"Valid: {syntax_df['is_valid'].sum()}/{len(syntax_df)}\")\n",
    "print(f\"Invalid: {(~syntax_df['is_valid']).sum()}/{len(syntax_df)}\")\n",
    "\n",
    "# Show queries with errors\n",
    "if not syntax_df[syntax_df['is_valid'] == False].empty:\n",
    "    print(\"\\nQueries with syntax errors:\")\n",
    "    display(syntax_df[syntax_df['is_valid'] == False][['query_id', 'errors']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Cross-Database Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check compatibility for all queries\n",
    "compatibility_results = []\n",
    "\n",
    "for query in queries:\n",
    "    result = tester.compatibility_checker.check(query)\n",
    "    compatibility_results.append({\n",
    "        'query_id': query.id,\n",
    "        'postgresql': result.postgresql,\n",
    "        'databricks': result.databricks,\n",
    "        'databricks': result.databricks,\n",
    "        'issues': result.issues,\n",
    "        'suggestions': result.suggestions\n",
    "    })\n",
    "\n",
    "# Create compatibility matrix\n",
    "compat_df = pd.DataFrame(compatibility_results)\n",
    "\n",
    "print(\"\\nCross-Database Compatibility Summary:\")\n",
    "print(f\"PostgreSQL compatible: {compat_df['postgresql'].sum()}/{len(compat_df)}\")\n",
    "print(f\"Databricks compatible: {compat_df['databricks'].sum()}/{len(compat_df)}\")\n",
    "print(f\"Databricks compatible: {compat_df['databricks'].sum()}/{len(compat_df)}\")\n",
    "\n",
    "# Create compatibility matrix visualization\n",
    "compat_matrix = compat_df[['query_id', 'postgresql', 'databricks', 'databricks']].set_index('query_id')\n",
    "compat_matrix = compat_matrix.astype(int)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(compat_matrix.T, annot=True, fmt='d', cmap='RdYlGn', cbar_kws={'label': 'Compatible'})\n",
    "plt.title('Cross-Database Compatibility Matrix (DB-2)')\n",
    "plt.xlabel('Query ID')\n",
    "plt.ylabel('Database')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show queries with compatibility issues\n",
    "issues_found = []\n",
    "for result in compatibility_results:\n",
    "    if result['issues']['postgresql'] or result['issues']['databricks'] or result['issues']['databricks']:\n",
    "        issues_found.append({\n",
    "            'query_id': result['query_id'],\n",
    "            'postgresql_issues': len(result['issues']['postgresql']),\n",
    "            'databricks_issues': len(result['issues']['databricks']),\n",
    "            'databricks_issues': len(result['issues']['databricks']),\n",
    "            'suggestions': result['suggestions']\n",
    "        })\n",
    "\n",
    "if issues_found:\n",
    "    issues_df = pd.DataFrame(issues_found)\n",
    "    print(\"\\nQueries with compatibility issues:\")\n",
    "    display(issues_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db-2 Schema: LinkWay E-commerce/Affiliate system\n",
    "# Expected tables based on SCHEMA.md:\n",
    "expected_tables = {\n",
    "    'authentication_user': ['id', 'username', 'email', 'role'],\n",
    "    'products_productcategory': ['id', 'name', 'parent_id'],\n",
    "    'products_product': ['id', 'name', 'category_id', 'seller_id', 'price'],\n",
    "    'affiliates_affiliatelink': ['id', 'marketer_id', 'product_id', 'unique_slug'],\n",
    "    'affiliates_clicktracking': ['id', 'affiliate_link_id', 'created_at', 'landing_page_url'],\n",
    "    'orders_order': ['id', 'customer_order_id', 'marketer_id', 'product_id', 'quantity', 'total_price', 'created_at'],\n",
    "    'commissions_commission': ['id', 'marketer_id', 'order_id', 'amount', 'status', 'created_at'],\n",
    "    'orders_customerorder': ['id', 'buyer_id', 'created_at'],\n",
    "    'affiliates_catalogue': ['id', 'marketer_id', 'name'],\n",
    "    'affiliates_catalogue_links': ['catalogue_id', 'affiliate_link_id']\n",
    "}\n",
    "\n",
    "# Extract table references from queries\n",
    "import re\n",
    "table_references = {}\n",
    "\n",
    "for query in queries:\n",
    "    # Find FROM and JOIN clauses\n",
    "    from_tables = re.findall(r'\\bFROM\\s+(\\w+)', query.text, re.IGNORECASE)\n",
    "    join_tables = re.findall(r'\\bJOIN\\s+(\\w+)', query.text, re.IGNORECASE)\n",
    "    all_tables = set(from_tables + join_tables)\n",
    "\n",
    "    table_references[query.id] = {\n",
    "        'tables': list(all_tables),\n",
    "        'valid': all(table.lower() in [t.lower() for t in expected_tables.keys()] for table in all_tables)\n",
    "    }\n",
    "\n",
    "schema_validation_df = pd.DataFrame([\n",
    "    {'query_id': qid, 'tables': ', '.join(ref['tables']), 'valid': ref['valid']}\n",
    "    for qid, ref in table_references.items()\n",
    "])\n",
    "\n",
    "print(\"\\nSchema Validation Results:\")\n",
    "print(f\"Valid table references: {schema_validation_df['valid'].sum()}/{len(schema_validation_df)}\")\n",
    "\n",
    "if not schema_validation_df[schema_validation_df['valid'] == False].empty:\n",
    "    print(\"\\nQueries with invalid table references:\")\n",
    "    display(schema_validation_df[schema_validation_df['valid'] == False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Execution Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute queries (with real DB or mock data)\n",
    "execution_results = []\n",
    "\n",
    "if connection:\n",
    "    print(\"Executing queries with real database...\")\n",
    "    for query in queries:\n",
    "        try:\n",
    "            result = tester.executor.execute(query, connection)\n",
    "            execution_results.append({\n",
    "                'query_id': query.id,\n",
    "                'success': result.success,\n",
    "                'execution_time': result.execution_time,\n",
    "                'row_count': result.row_count,\n",
    "                'error': result.error_message,\n",
    "                'schema': result.result_schema\n",
    "            })\n",
    "        except Exception as e:\n",
    "            execution_results.append({\n",
    "                'query_id': query.id,\n",
    "                'success': False,\n",
    "                'execution_time': 0.0,\n",
    "                'row_count': 0,\n",
    "                'error': str(e),\n",
    "                'schema': None\n",
    "            })\n",
    "else:\n",
    "    print(\"No database connection available. Generating mock data...\")\n",
    "\n",
    "    # Generate mock schema\n",
    "    mock_schema = {}\n",
    "    for table_name, columns in expected_tables.items():\n",
    "        mock_schema[table_name] = {\n",
    "            'columns': [{'name': col, 'type': 'text' if 'id' in col.lower() or 'name' in col.lower() else 'numeric'} for col in columns]\n",
    "        }\n",
    "\n",
    "    # Create mock database\n",
    "    from sqlalchemy import create_engine\n",
    "    mock_engine = tester.mock_generator.create_mock_database(mock_schema, \"sqlite:///:memory:\")\n",
    "    mock_connection = mock_engine.connect()\n",
    "\n",
    "    print(\"Executing queries with mock data...\")\n",
    "    for query in queries:\n",
    "        try:\n",
    "            result = tester.executor.execute(query, mock_connection)\n",
    "            execution_results.append({\n",
    "                'query_id': query.id,\n",
    "                'success': result.success,\n",
    "                'execution_time': result.execution_time,\n",
    "                'row_count': result.row_count,\n",
    "                'error': result.error_message,\n",
    "                'schema': result.result_schema,\n",
    "                'note': 'Mock data used'\n",
    "            })\n",
    "        except Exception as e:\n",
    "            execution_results.append({\n",
    "                'query_id': query.id,\n",
    "                'success': False,\n",
    "                'execution_time': 0.0,\n",
    "                'row_count': 0,\n",
    "                'error': str(e),\n",
    "                'schema': None,\n",
    "                'note': 'Mock data used'\n",
    "            })\n",
    "\n",
    "exec_df = pd.DataFrame(execution_results)\n",
    "print(f\"\\nExecution Results:\")\n",
    "print(f\"Successful: {exec_df['success'].sum()}/{len(exec_df)}\")\n",
    "print(f\"Failed: {(~exec_df['success']).sum()}/{len(exec_df)}\")\n",
    "\n",
    "if exec_df['success'].sum() > 0:\n",
    "    print(f\"Average execution time: {exec_df[exec_df['success']]['execution_time'].mean():.3f}s\")\n",
    "    print(f\"Total rows returned: {exec_df[exec_df['success']]['row_count'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show failed queries\n",
    "if not exec_df[exec_df['success'] == False].empty:\n",
    "    print(\"\\nFailed Queries:\")\n",
    "    failed_df = exec_df[exec_df['success'] == False][['query_id', 'error']]\n",
    "    display(failed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile performance for all queries\n",
    "performance_results = []\n",
    "\n",
    "test_conn = connection if connection else mock_connection if 'mock_connection' in locals() else None\n",
    "\n",
    "if test_conn:\n",
    "    for query in queries:\n",
    "        try:\n",
    "            perf_result = tester.profiler.profile(query, test_conn)\n",
    "            performance_results.append({\n",
    "                'query_id': query.id,\n",
    "                'execution_time': perf_result.execution_time,\n",
    "                'row_count': perf_result.row_count,\n",
    "                'is_slow': perf_result.is_slow,\n",
    "                'suggestions': perf_result.optimization_suggestions\n",
    "            })\n",
    "        except Exception as e:\n",
    "            performance_results.append({\n",
    "                'query_id': query.id,\n",
    "                'execution_time': 0.0,\n",
    "                'row_count': 0,\n",
    "                'is_slow': False,\n",
    "                'suggestions': [f\"Error: {str(e)}\"]\n",
    "            })\n",
    "\n",
    "    perf_df = pd.DataFrame(performance_results)\n",
    "\n",
    "    print(\"\\nPerformance Analysis:\")\n",
    "    print(f\"Average execution time: {perf_df['execution_time'].mean():.3f}s\")\n",
    "    print(f\"Slow queries (>5s): {perf_df['is_slow'].sum()}\")\n",
    "    print(f\"\\nExecution Time Statistics:\")\n",
    "    print(perf_df['execution_time'].describe())\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    perf_df.plot(x='query_id', y='execution_time', kind='bar', ax=plt.gca())\n",
    "    plt.axhline(y=5.0, color='r', linestyle='--', label='Slow threshold (5s)')\n",
    "    plt.title('Query Execution Times (DB-2)')\n",
    "    plt.xlabel('Query ID')\n",
    "    plt.ylabel('Execution Time (seconds)')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    perf_df['execution_time'].hist(bins=20, edgecolor='black')\n",
    "    plt.axvline(x=5.0, color='r', linestyle='--', label='Slow threshold')\n",
    "    plt.title('Execution Time Distribution')\n",
    "    plt.xlabel('Execution Time (seconds)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Show slow queries\n",
    "    if perf_df['is_slow'].sum() > 0:\n",
    "        print(\"\\nSlow Queries (>5s):\")\n",
    "        slow_df = perf_df[perf_df['is_slow']][['query_id', 'execution_time', 'suggestions']]\n",
    "        display(slow_df)\n",
    "else:\n",
    "    print(\"No database connection available for performance profiling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Correctness Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate query results\n",
    "validation_results = []\n",
    "\n",
    "if 'execution_results' in locals() and test_conn:\n",
    "    for exec_result in execution_results:\n",
    "        if exec_result['success']:\n",
    "            # Create ExecutionResult object for validation\n",
    "            from test_queries import ExecutionResult\n",
    "            exec_obj = ExecutionResult(\n",
    "                success=exec_result['success'],\n",
    "                execution_time=exec_result['execution_time'],\n",
    "                row_count=exec_result['row_count'],\n",
    "                result_schema=exec_result['schema']\n",
    "            )\n",
    "\n",
    "            val_result = tester.result_validator.validate(exec_obj)\n",
    "            validation_results.append({\n",
    "                'query_id': exec_result['query_id'],\n",
    "                'is_valid': val_result.is_valid,\n",
    "                'schema_match': val_result.schema_match,\n",
    "                'type_issues': val_result.type_issues,\n",
    "                'warnings': val_result.warnings\n",
    "            })\n",
    "\n",
    "    if validation_results:\n",
    "        val_df = pd.DataFrame(validation_results)\n",
    "        print(\"\\nCorrectness Validation Results:\")\n",
    "        print(f\"Valid results: {val_df['is_valid'].sum()}/{len(val_df)}\")\n",
    "        print(f\"Schema matches: {val_df['schema_match'].sum()}/{len(val_df)}\")\n",
    "\n",
    "        if not val_df[val_df['is_valid'] == False].empty:\n",
    "            print(\"\\nQueries with validation issues:\")\n",
    "            display(val_df[val_df['is_valid'] == False])\n",
    "else:\n",
    "    print(\"No execution results available for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Edge Case Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases (if database available)\n",
    "edge_case_results = []\n",
    "\n",
    "if test_conn:\n",
    "    print(\"Testing edge cases...\")\n",
    "\n",
    "    edge_case_results.append({\n",
    "        'test': 'Empty table handling',\n",
    "        'status': 'Not tested (requires empty table setup)',\n",
    "        'note': 'Would test queries with empty result sets'\n",
    "    })\n",
    "\n",
    "    edge_case_results.append({\n",
    "        'test': 'NULL value handling',\n",
    "        'status': 'Not tested (requires NULL data)',\n",
    "        'note': 'Would test queries with NULL values in columns'\n",
    "    })\n",
    "\n",
    "    edge_case_results.append({\n",
    "        'test': 'Recursive CTE termination',\n",
    "        'status': 'Validated in syntax check',\n",
    "        'note': 'Recursive CTEs checked for termination conditions'\n",
    "    })\n",
    "\n",
    "    edge_case_df = pd.DataFrame(edge_case_results)\n",
    "    display(edge_case_df)\n",
    "else:\n",
    "    print(\"Edge case testing requires database connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "summary = {\n",
    "    'database': DB_NAME,\n",
    "    'test_date': datetime.now().isoformat(),\n",
    "    'total_queries': len(queries),\n",
    "    'syntax_validation': {\n",
    "        'total': len(syntax_results),\n",
    "        'valid': syntax_df['is_valid'].sum() if 'syntax_df' in locals() else 0,\n",
    "        'invalid': (~syntax_df['is_valid']).sum() if 'syntax_df' in locals() else 0\n",
    "    },\n",
    "    'compatibility': {\n",
    "        'postgresql': compat_df['postgresql'].sum() if 'compat_df' in locals() else 0,\n",
    "        'databricks': compat_df['databricks'].sum() if 'compat_df' in locals() else 0,\n",
    "        'databricks': compat_df['databricks'].sum() if 'compat_df' in locals() else 0\n",
    "    },\n",
    "    'execution': {\n",
    "        'total': len(execution_results) if 'execution_results' in locals() else 0,\n",
    "        'successful': exec_df['success'].sum() if 'exec_df' in locals() else 0,\n",
    "        'failed': (~exec_df['success']).sum() if 'exec_df' in locals() else 0,\n",
    "        'avg_execution_time': exec_df[exec_df['success']]['execution_time'].mean() if 'exec_df' in locals() and exec_df['success'].sum() > 0 else 0.0\n",
    "    },\n",
    "    'performance': {\n",
    "        'slow_queries': perf_df['is_slow'].sum() if 'perf_df' in locals() else 0,\n",
    "        'avg_execution_time': perf_df['execution_time'].mean() if 'perf_df' in locals() else 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE TEST SUMMARY - DB-2\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDatabase: {summary['database']}\")\n",
    "print(f\"Test Date: {summary['test_date']}\")\n",
    "print(f\"Total Queries: {summary['total_queries']}\")\n",
    "print(f\"\\nSyntax Validation:\")\n",
    "print(f\"  Valid: {summary['syntax_validation']['valid']}/{summary['syntax_validation']['total']}\")\n",
    "print(f\"  Invalid: {summary['syntax_validation']['invalid']}/{summary['syntax_validation']['total']}\")\n",
    "print(f\"\\nCross-Database Compatibility:\")\n",
    "print(f\"  PostgreSQL: {summary['compatibility']['postgresql']}/{summary['total_queries']}\")\n",
    "print(f\"  Databricks: {summary['compatibility']['databricks']}/{summary['total_queries']}\")\n",
    "print(f\"  Databricks: {summary['compatibility']['databricks']}/{summary['total_queries']}\")\n",
    "print(f\"\\nExecution:\")\n",
    "print(f\"  Successful: {summary['execution']['successful']}/{summary['execution']['total']}\")\n",
    "print(f\"  Failed: {summary['execution']['failed']}/{summary['execution']['total']}\")\n",
    "if summary['execution']['avg_execution_time'] > 0:\n",
    "    print(f\"  Avg Execution Time: {summary['execution']['avg_execution_time']:.3f}s\")\n",
    "print(f\"\\nPerformance:\")\n",
    "if summary['performance']['avg_execution_time'] > 0:\n",
    "    print(f\"  Avg Execution Time: {summary['performance']['avg_execution_time']:.3f}s\")\n",
    "print(f\"  Slow Queries (>5s): {summary['performance']['slow_queries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Syntax validation\n",
    "if 'syntax_df' in locals():\n",
    "    syntax_df['is_valid'].value_counts().plot(kind='pie', ax=axes[0, 0], autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Syntax Validation Results (DB-2)')\n",
    "\n",
    "# 2. Compatibility matrix\n",
    "if 'compat_df' in locals():\n",
    "    compat_summary = pd.DataFrame({\n",
    "        'Database': ['PostgreSQL', 'Databricks', 'Databricks'],\n",
    "        'Compatible': [\n",
    "            compat_df['postgresql'].sum(),\n",
    "            compat_df['databricks'].sum(),\n",
    "            compat_df['databricks'].sum()\n",
    "        ]\n",
    "    })\n",
    "    compat_summary.plot(x='Database', y='Compatible', kind='bar', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Cross-Database Compatibility (DB-2)')\n",
    "    axes[0, 1].set_ylabel('Compatible Queries')\n",
    "\n",
    "# 3. Execution success rate\n",
    "if 'exec_df' in locals():\n",
    "    exec_df['success'].value_counts().plot(kind='pie', ax=axes[1, 0], autopct='%1.1f%%')\n",
    "    axes[1, 0].set_title('Execution Success Rate (DB-2)')\n",
    "\n",
    "# 4. Execution time distribution\n",
    "if 'perf_df' in locals() and perf_df['execution_time'].sum() > 0:\n",
    "    perf_df[perf_df['execution_time'] > 0]['execution_time'].hist(bins=20, ax=axes[1, 1], edgecolor='black')\n",
    "    axes[1, 1].axvline(x=5.0, color='r', linestyle='--', label='Slow threshold')\n",
    "    axes[1, 1].set_title('Execution Time Distribution (DB-2)')\n",
    "    axes[1, 1].set_xlabel('Execution Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON\n",
    "output_file = Path.cwd() / 'query_test_results.json'\n",
    "\n",
    "export_data = {\n",
    "    'summary': summary,\n",
    "    'syntax_results': syntax_results if 'syntax_results' in locals() else [],\n",
    "    'compatibility_results': compatibility_results if 'compatibility_results' in locals() else [],\n",
    "    'execution_results': execution_results if 'execution_results' in locals() else [],\n",
    "    'performance_results': performance_results if 'performance_results' in locals() else [],\n",
    "    'validation_results': validation_results if 'validation_results' in locals() else []\n",
    "}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✓ Results exported to: {output_file}\")\n",
    "\n",
    "# Export to CSV\n",
    "if 'syntax_df' in locals():\n",
    "    syntax_df.to_csv(Path.cwd() / 'syntax_results.csv', index=False)\n",
    "if 'compat_df' in locals():\n",
    "    compat_df.to_csv(Path.cwd() / 'compatibility_results.csv', index=False)\n",
    "if 'exec_df' in locals():\n",
    "    exec_df.to_csv(Path.cwd() / 'execution_results.csv', index=False)\n",
    "if 'perf_df' in locals():\n",
    "    perf_df.to_csv(Path.cwd() / 'performance_results.csv', index=False)\n",
    "\n",
    "print(\"✓ CSV files exported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
