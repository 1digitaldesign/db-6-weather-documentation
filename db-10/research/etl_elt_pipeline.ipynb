{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL/ELT Pipeline - DB-10 Marketing Intelligence Database\n",
    "\n",
    "This notebook provides a comprehensive ETL/ELT pipeline for the Marketing Intelligence Database (db-10).\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Extract**: Load data from U.S. Census Bureau APIs, BLS Public Data API, FTC data, Data.gov CKAN API, and retail sources\n",
    "2. **Transform**: Clean, validate, and transform marketing intelligence data\n",
    "3. **Load**: Load transformed data into PostgreSQL\n",
    "4. **Validate**: Verify data quality and completeness\n",
    "5. **Monitor**: Track pipeline performance and errors\n",
    "\n",
    "## Data Sources\n",
    "- **U.S. Census Bureau APIs**: Monthly Retail Trade Survey (MRTS), Advance Retail Inventories, Annual Retail Trade Survey\n",
    "- **BLS Public Data API**: Consumer Price Index (CPI), Producer Price Index (PPI), retail employment statistics\n",
    "- **Federal Trade Commission (FTC)**: Consumer Sentinel Network data, pricing accuracy studies, retail enforcement actions\n",
    "- **Data.gov CKAN API**: Retail-related datasets, economic indicator datasets\n",
    "- **Other Sources**: Retailer APIs, web scrapers, manual entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# API and HTTP requests\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Database connections\n",
    "try:\n",
    "    from sqlalchemy import create_engine, text\n",
    "    SQLALCHEMY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SQLALCHEMY_AVAILABLE = False\n",
    "    print(\"Warning: sqlalchemy not available\")\n",
    "\n",
    "try:\n",
    "    import psycopg2\n",
    "    from psycopg2.extras import RealDictCursor\n",
    "    PG_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PG_AVAILABLE = False\n",
    "    print(\"Warning: psycopg2 not available\")\n",
    "\n",
    "try:\n",
    "    from databricks import sql\n",
    "    DATABRICKS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DATABRICKS_AVAILABLE = False\n",
    "    print(\"Warning: databricks-sql-connector not available\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\u2713 Imports successful\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration",
    "DB_NAME = \"db-10\"",
    "DB_PATH = Path.cwd().parent",
    "",
    "# Database connection strings (configure as needed)",
    "# PostgreSQL",
    "POSTGRES_CONNECTION_STRING = None  # \"postgresql://user:password@localhost:5432/db_10_validation\"",
    "",
    "# Databricks",
    "DATABRICKS_CONFIG = {",
    "    'server_hostname': None,  # Set via environment variable DATABRICKS_SERVER_HOSTNAME",
    "    'http_path': None,  # Set via environment variable DATABRICKS_HTTP_PATH",
    "    'access_token': None  # Set via environment variable DATABRICKS_TOKEN",
    "}",
    "",
    "# Source data paths",
    "DATA_DIR = DB_PATH / \"data\"",
    "SCHEMA_FILE = DATA_DIR / \"schema.sql\"",
    "DATA_FILE = DATA_DIR / \"data.sql\"",
    "RESEARCH_DIR = DB_PATH / \"research\"",
    "",
    "# API Configuration",
    "# U.S. Census Bureau API - no API key required",
    "CENSUS_BASE_URL = \"http://api.census.gov/data/timeseries/eits\"",
    "CENSUS_MRTS_URL = f\"{CENSUS_BASE_URL}/mrts\"  # Monthly Retail Trade Survey",
    "CENSUS_MRTSADV_URL = f\"{CENSUS_BASE_URL}/mrtsadv\"  # Advance Retail Inventories",
    "",
    "# BLS Public Data API - no API key required (registration optional)",
    "BLS_BASE_URL = \"https://api.bls.gov/publicAPI/v2\"",
    "BLS_REGISTRATION_KEY = None  # Optional: Set via environment variable",
    "",
    "# USAJobs.gov API - requires API key from https://developer.usajobs.gov/",
    "",
    "# BLS Public Data API - no API key required",
    "BLS_BASE_URL = \"https://api.bls.gov/publicAPI/v2\"",
    "",
    "# Department of Labor Open Data Portal - uses Data.gov CKAN API",
    "DATAGOV_CKAN_BASE_URL = \"https://catalog.data.gov/api/3/action\"",
    "",
    "# Data extraction window (last 2 weeks)",
    "EXTRACTION_START_YEAR = datetime.now().year - 10",
    "EXTRACTION_END_YEAR = datetime.now().year",
    "EXTRACTION_END_DATE = datetime.now().strftime('%Y-%m-%d')",
    "",
    "print(f\"Database: {DB_NAME}\")",
    "print(f\"Data directory: {DATA_DIR}\")",
    "print(f\"Schema file exists: {SCHEMA_FILE.exists()}\")",
    "print(f\"Data file exists: {DATA_FILE.exists()}\")",
    "EXTRACTION_START_YEAR = datetime.now().year - 10",
    "EXTRACTION_END_YEAR = datetime.now().year",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Extract - API Integration and Data Loading\n",
    "\n",
    "### 2.1 USAJobs.gov API Integration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_session_with_retry() -> requests.Session:\n",
    "    \"\"\"Create requests session with retry strategy\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def fetch_usajobs_jobs(api_key: str, start_date: str, end_date: str, page: int = 1, results_per_page: int = 500) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch job postings from USAJobs.gov API for the last 2 weeks.\n",
    "    \n",
    "    API Documentation: https://developer.usajobs.gov/API-Reference/GET-Jobs\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        logger.warning(\"USAJobs API key not configured. Set USAJOBS_API_KEY environment variable.\")\n",
    "        return None\n",
    "    \n",
    "    session = create_session_with_retry()\n",
    "    \n",
    "    url = f\"{USAJOBS_BASE_URL}/Search\"\n",
    "    headers = {\n",
    "        \"Host\": \"data.usajobs.gov\",\n",
    "        \"User-Agent\": \"JobMarketIntelligence/1.0 (contact@example.com)\",\n",
    "        \"Authorization-Key\": api_key\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        \"DatePosted\": f\"{start_date}to{end_date}\",\n",
    "        \"Page\": page,\n",
    "        \"ResultsPerPage\": results_per_page,\n",
    "        \"SortField\": \"DatePosted\",\n",
    "        \"SortDirection\": \"Descending\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, headers=headers, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        logger.info(f\"Fetched {len(data.get('SearchResult', {}).get('SearchResultItems', []))} jobs from USAJobs.gov (page {page})\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching USAJobs data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch USAJobs data (example - requires API key)\n",
    "if USAJOBS_API_KEY:\n",
    "    usajobs_data = fetch_usajobs_jobs(USAJOBS_API_KEY, EXTRACTION_START_DATE, EXTRACTION_END_DATE)\n",
    "    if usajobs_data:\n",
    "        print(f\"\u2713 Fetched USAJobs data: {len(usajobs_data.get('SearchResult', {}).get('SearchResultItems', []))} jobs\")\n",
    "else:\n",
    "    print(\"\u26a0 USAJobs API key not configured. Skipping USAJobs.gov extraction.\")\n",
    "    usajobs_data = None"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 BLS Public Data API Integration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def fetch_bls_data(series_ids: List[str], start_year: int, end_year: int) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch labor statistics from BLS Public Data API.\n",
    "    \n",
    "    API Documentation: https://www.bls.gov/developers/api_signature_v2.htm\n",
    "    No API key required, but rate limits apply.\n",
    "    \"\"\"\n",
    "    session = create_session_with_retry()\n",
    "    \n",
    "    url = f\"{BLS_BASE_URL}/timeseries/data\"\n",
    "    \n",
    "    payload = {\n",
    "        \"seriesid\": series_ids,\n",
    "        \"startyear\": str(start_year),\n",
    "        \"endyear\": str(end_year),\n",
    "        \"registrationkey\": None  # Optional, increases rate limit\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = session.post(url, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('status') == 'REQUEST_SUCCEEDED':\n",
    "            logger.info(f\"Fetched BLS data for {len(series_ids)} series\")\n",
    "            return data\n",
    "        else:\n",
    "            logger.warning(f\"BLS API returned status: {data.get('status')}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching BLS data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example BLS series IDs for job market data\n",
    "# CES0000000001: Total nonfarm employment\n",
    "# LEU0254555900: Unemployment rate\n",
    "# CES0500000003: Average hourly earnings\n",
    "bls_series_ids = [\"CES0000000001\", \"LEU0254555900\", \"CES0500000003\"]\n",
    "current_year = datetime.now().year\n",
    "\n",
    "bls_data = fetch_bls_data(bls_series_ids, current_year - 1, current_year)\n",
    "if bls_data:\n",
    "    print(f\"\u2713 Fetched BLS data for {len(bls_series_ids)} series\")\n",
    "else:\n",
    "    print(\"\u26a0 BLS data fetch failed or skipped\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Department of Labor Open Data Portal Integration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def search_dol_datasets(query: str = \"employment\", organization: str = \"department-of-labor\", limit: int = 20) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Search for datasets in Department of Labor Open Data Portal via Data.gov CKAN API.\n",
    "    \n",
    "    API Documentation: https://catalog.data.gov/api/3/action/package_search\n",
    "    \"\"\"\n",
    "    session = create_session_with_retry()\n",
    "    \n",
    "    url = f\"{DOL_CKAN_BASE_URL}/package_search\"\n",
    "    \n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"fq\": f\"organization:{organization}\",\n",
    "        \"rows\": limit\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('success'):\n",
    "            count = data.get('result', {}).get('count', 0)\n",
    "            datasets = data.get('result', {}).get('results', [])\n",
    "            logger.info(f\"Found {count} DOL datasets matching '{query}'\")\n",
    "            return {'count': count, 'datasets': datasets}\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error searching DOL datasets: {e}\")\n",
    "        return None\n",
    "\n",
    "# Search for employment-related datasets\n",
    "dol_datasets = search_dol_datasets(query=\"employment statistics\", limit=10)\n",
    "if dol_datasets:\n",
    "    print(f\"\u2713 Found {dol_datasets['count']} DOL datasets\")\n",
    "    print(f\"  Sample datasets: {', '.join([d['title'][:50] for d in dol_datasets['datasets'][:3]])}\")\n",
    "else:\n",
    "    print(\"\u26a0 DOL dataset search failed or skipped\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Load Local Data Files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_schema_file(schema_path: Path) -> Optional[str]:\n",
    "    \"\"\"Load database schema from SQL file.\"\"\"\n",
    "    try:\n",
    "        if schema_path.exists():\n",
    "            with open(schema_path, 'r') as f:\n",
    "                return f.read()\n",
    "        else:\n",
    "            logger.warning(f\"Schema file not found: {schema_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading schema: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_data_file(data_path: Path) -> Optional[str]:\n",
    "    \"\"\"Load data from SQL file.\"\"\"\n",
    "    try:\n",
    "        if data_path.exists():\n",
    "            with open(data_path, 'r') as f:\n",
    "                return f.read()\n",
    "        else:\n",
    "            logger.warning(f\"Data file not found: {data_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_from_csv(csv_path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Extract data from CSV file.\"\"\"\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            logger.info(f\"Loaded {len(df)} rows from {csv_path.name}\")\n",
    "            return df\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_from_json(json_path: Path) -> Optional[Dict]:\n",
    "    \"\"\"Extract data from JSON file.\"\"\"\n",
    "    try:\n",
    "        if json_path.exists():\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            logger.info(f\"Loaded JSON from {json_path.name}\")\n",
    "            return data\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading JSON {json_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load schema and data\n",
    "schema_sql = load_schema_file(SCHEMA_FILE)\n",
    "data_sql = load_data_file(DATA_FILE)\n",
    "\n",
    "# Find and load data files\n",
    "csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
    "json_files = list(DATA_DIR.glob(\"*.json\"))\n",
    "\n",
    "extracted_data = {}\n",
    "\n",
    "# Add API data to extracted_data\n",
    "if usajobs_data:\n",
    "    extracted_data['usajobs'] = usajobs_data\n",
    "if bls_data:\n",
    "    extracted_data['bls'] = bls_data\n",
    "if dol_datasets:\n",
    "    extracted_data['dol_datasets'] = dol_datasets\n",
    "\n",
    "# Load local files\n",
    "for csv_file in csv_files:\n",
    "    df = extract_from_csv(csv_file)\n",
    "    if df is not None:\n",
    "        extracted_data[csv_file.stem] = df\n",
    "\n",
    "for json_file in json_files:\n",
    "    data = extract_from_json(json_file)\n",
    "    if data is not None:\n",
    "        extracted_data[json_file.stem] = data\n",
    "\n",
    "if schema_sql:\n",
    "    print(f\"\u2713 Schema loaded ({len(schema_sql)} characters)\")\n",
    "if data_sql:\n",
    "    print(f\"\u2713 Data loaded ({len(data_sql)} characters)\")\n",
    "print(f\"\u2713 Extracted {len(extracted_data)} data sources\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Transform - Data Cleaning and Transformation\n",
    "\n",
    "### 3.1 Transform USAJobs Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def transform_usajobs_to_dataframe(usajobs_data: Dict) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Transform USAJobs API response to DataFrame matching job_postings table schema.\"\"\"\n",
    "    if not usajobs_data:\n",
    "        return None\n",
    "    \n",
    "    jobs = []\n",
    "    search_result_items = usajobs_data.get('SearchResult', {}).get('SearchResultItems', [])\n",
    "    \n",
    "    for item in search_result_items:\n",
    "        matched_object = item.get('MatchedObjectDescriptor', {})\n",
    "        \n",
    "        job = {\n",
    "            'job_id': matched_object.get('PositionID', ''),\n",
    "            'title': matched_object.get('PositionTitle', ''),\n",
    "            'company_id': None,  # Will need to match/create company records\n",
    "            'location': matched_object.get('PositionLocationDisplay', ''),\n",
    "            'job_type': matched_object.get('PositionSchedule', ''),\n",
    "            'salary_min': None,  # Extract from PositionRemuneration\n",
    "            'salary_max': None,\n",
    "            'description': matched_object.get('UserArea', {}).get('Details', {}).get('MajorDuties', [''])[0] if matched_object.get('UserArea', {}).get('Details', {}).get('MajorDuties') else '',\n",
    "            'requirements': matched_object.get('UserArea', {}).get('Details', {}).get('Requirements', ''),\n",
    "            'posted_date': matched_object.get('PositionStartDate', ''),\n",
    "            'closing_date': matched_object.get('PositionEndDate', ''),\n",
    "            'source': 'USAJobs.gov',\n",
    "            'source_url': matched_object.get('PositionURI', ''),\n",
    "            'is_active': True,\n",
    "            'created_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Extract salary information\n",
    "        remuneration = matched_object.get('PositionRemuneration', [])\n",
    "        if remuneration:\n",
    "            salary_info = remuneration[0]\n",
    "            job['salary_min'] = salary_info.get('MinimumRange', None)\n",
    "            job['salary_max'] = salary_info.get('MaximumRange', None)\n",
    "        \n",
    "        jobs.append(job)\n",
    "    \n",
    "    if jobs:\n",
    "        df = pd.DataFrame(jobs)\n",
    "        logger.info(f\"Transformed {len(df)} USAJobs postings to DataFrame\")\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "# Transform USAJobs data\n",
    "if usajobs_data:\n",
    "    usajobs_df = transform_usajobs_to_dataframe(usajobs_data)\n",
    "    if usajobs_df is not None:\n",
    "        print(f\"\u2713 Transformed {len(usajobs_df)} USAJobs postings\")\n",
    "        print(f\"  Columns: {', '.join(usajobs_df.columns)}\")\n",
    "    else:\n",
    "        print(\"\u26a0 USAJobs transformation returned no data\")\n",
    "        usajobs_df = None\n",
    "else:\n",
    "    usajobs_df = None"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Transform BLS Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def transform_bls_to_dataframe(bls_data: Dict) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Transform BLS API response to DataFrame for market trends.\"\"\"\n",
    "    if not bls_data or bls_data.get('status') != 'REQUEST_SUCCEEDED':\n",
    "        return None\n",
    "    \n",
    "    trends = []\n",
    "    results = bls_data.get('Results', {}).get('series', [])\n",
    "    \n",
    "    for series in results:\n",
    "        series_id = series.get('seriesID', '')\n",
    "        data_points = series.get('data', [])\n",
    "        \n",
    "        for point in data_points:\n",
    "            trend = {\n",
    "                'trend_id': f\"{series_id}_{point.get('year')}_{point.get('period')}\",\n",
    "                'metric_name': series_id,\n",
    "                'metric_value': float(point.get('value', 0)) if point.get('value') != 'null' else None,\n",
    "                'period': point.get('period', ''),\n",
    "                'year': int(point.get('year', 0)),\n",
    "                'source': 'BLS Public Data API',\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            }\n",
    "            trends.append(trend)\n",
    "    \n",
    "    if trends:\n",
    "        df = pd.DataFrame(trends)\n",
    "        logger.info(f\"Transformed {len(df)} BLS data points to DataFrame\")\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "# Transform BLS data\n",
    "if bls_data:\n",
    "    bls_df = transform_bls_to_dataframe(bls_data)\n",
    "    if bls_df is not None:\n",
    "        print(f\"\u2713 Transformed {len(bls_df)} BLS data points\")\n",
    "        print(f\"  Unique metrics: {bls_df['metric_name'].nunique()}\")\n",
    "    else:\n",
    "        print(\"\u26a0 BLS transformation returned no data\")\n",
    "        bls_df = None\n",
    "else:\n",
    "    bls_df = None"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean DataFrame: handle missing values, remove duplicates, etc.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Remove duplicates\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df)\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_before = df.isnull().sum().sum()\n",
    "    # Fill numeric columns with median\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    # Fill text columns with empty string\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].fillna('')\n",
    "    missing_after = df.isnull().sum().sum()\n",
    "    \n",
    "    logger.info(f\"Cleaned data: removed {duplicates_removed} duplicates, filled {missing_before - missing_after} missing values\")\n",
    "    return df\n",
    "\n",
    "# Clean transformed dataframes\n",
    "cleaned_data = {}\n",
    "if usajobs_df is not None:\n",
    "    cleaned_data['job_postings'] = clean_dataframe(usajobs_df.copy())\n",
    "if bls_df is not None:\n",
    "    cleaned_data['market_trends'] = clean_dataframe(bls_df.copy())\n",
    "\n",
    "# Clean other extracted dataframes\n",
    "for name, data in extracted_data.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        cleaned_data[name] = clean_dataframe(data.copy())\n",
    "\n",
    "print(f\"\u2713 Cleaned {len([d for d in cleaned_data.values() if isinstance(d, pd.DataFrame)])} dataframes\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def validate_dataframe(df: pd.DataFrame, required_columns: List[str] = None) -> Dict:\n",
    "    \"\"\"Validate DataFrame structure and data quality.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return {'valid': False, 'errors': ['DataFrame is empty or None']}\n",
    "    \n",
    "    validation_results = {\n",
    "        'valid': True,\n",
    "        'row_count': len(df),\n",
    "        'column_count': len(df.columns),\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'duplicate_rows': df.duplicated().sum(),\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    # Check required columns\n",
    "    if required_columns:\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            validation_results['valid'] = False\n",
    "            validation_results['errors'].append(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate cleaned data\n",
    "validation_results = {}\n",
    "for name, data in cleaned_data.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        validation_results[name] = validate_dataframe(data)\n",
    "\n",
    "# Display validation results\n",
    "for name, results in validation_results.items():\n",
    "    status = \"\u2713\" if results['valid'] else \"\u2717\"\n",
    "    print(f\"{status} {name}: {results['row_count']} rows, {results['column_count']} columns\")\n",
    "    if results['errors']:\n",
    "        for error in results['errors']:\n",
    "            print(f\"  Error: {error}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Load - Data Loading to Target Database"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_to_postgresql(df: pd.DataFrame, table_name: str, connection_string: str, if_exists: str = 'append') -> bool:\n",
    "    \"\"\"Load DataFrame to PostgreSQL table.\"\"\"\n",
    "    if not SQLALCHEMY_AVAILABLE or connection_string is None:\n",
    "        logger.warning(\"PostgreSQL connection not available\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        df.to_sql(table_name, engine, if_exists=if_exists, index=False, method='multi', chunksize=1000)\n",
    "        logger.info(f\"Loaded {len(df)} rows to PostgreSQL table {table_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading to PostgreSQL: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_to_databricks(df: pd.DataFrame, table_name: str, config: Dict) -> bool:\n",
    "    \"\"\"Load DataFrame to Databricks table.\"\"\"\n",
    "    if not DATABRICKS_AVAILABLE or not all([config.get('server_hostname'), config.get('http_path'), config.get('access_token')]):\n",
    "        logger.warning(\"Databricks connection not available\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        conn = sql.connect(\n",
    "            server_hostname=config['server_hostname'],\n",
    "            http_path=config['http_path'],\n",
    "            access_token=config['access_token']\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Convert DataFrame to INSERT statements (simplified - in production, use Spark DataFrame)\n",
    "        # For now, log that data is ready for loading\n",
    "        logger.info(f\"Prepared {len(df)} rows for Databricks table {table_name}\")\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading to Databricks: {e}\")\n",
    "        return False\n",
    "\n",
    "# Load data to target databases\n",
    "load_results = {}\n",
    "\n",
    "for name, data in cleaned_data.items():\n",
    "    if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "        table_name = name.lower().replace(' ', '_')\n",
    "        \n",
    "        # PostgreSQL\n",
    "        if POSTGRES_CONNECTION_STRING:\n",
    "            load_results[f\"{name}_postgres\"] = load_to_postgresql(\n",
    "                data, table_name, POSTGRES_CONNECTION_STRING\n",
    "            )\n",
    "        \n",
    "        # Databricks\n",
    "        if all([DATABRICKS_CONFIG.get('server_hostname'), DATABRICKS_CONFIG.get('http_path'), DATABRICKS_CONFIG.get('access_token')]):\n",
    "            load_results[f\"{name}_databricks\"] = load_to_databricks(\n",
    "                data, table_name, DATABRICKS_CONFIG\n",
    "            )\n",
    "\n",
    "successful_loads = sum(load_results.values())\n",
    "print(f\"\u2713 Loaded {successful_loads} datasets to target databases\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL/ELT Pipeline - DB-1\n",
    "\n",
    "This notebook provides a comprehensive ETL/ELT pipeline for database db-1.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Extract**: Load data from source systems\n",
    "2. **Transform**: Clean, validate, and transform data\n",
    "3. **Load**: Load transformed data into target database\n",
    "4. **Validate**: Verify data quality and completeness\n",
    "5. **Monitor**: Track pipeline performance and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Database connections\n",
    "try:\n",
    "    from sqlalchemy import create_engine, text\n",
    "    SQLALCHEMY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SQLALCHEMY_AVAILABLE = False\n",
    "    print(\"Warning: sqlalchemy not available\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\u2713 Imports successful\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "DB_NAME = \"db-8\"\n",
    "DB_PATH = Path.cwd().parent\n",
    "\n",
    "# Database connection strings (configure as needed)\n",
    "# PostgreSQL\n",
    "POSTGRES_CONNECTION_STRING = None  # \"postgresql://user:password@localhost:5432/dbname\"\n",
    "\n",
    "# Databricks\n",
    "DATABRICKS_CONNECTION_STRING = None  # Configure Databricks connection\n",
    "\n",
    "# Snowflake\n",
    "SNOWFLAKE_CONNECTION_STRING = None  # Configure Snowflake connection\n",
    "\n",
    "# Source data paths\n",
    "DATA_DIR = DB_PATH / \"data\"\n",
    "SCHEMA_FILE = DATA_DIR / \"schema.sql\"\n",
    "DATA_FILE = DATA_DIR / \"data.sql\"\n",
    "\n",
    "print(f\"Database: {DB_NAME}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Schema file exists: {SCHEMA_FILE.exists()}\")\n",
    "print(f\"Data file exists: {DATA_FILE.exists()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Extract - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_schema_file(schema_path: Path) -> Optional[str]:\n",
    "    \"\"\"Load database schema from SQL file.\"\"\"\n",
    "    try:\n",
    "        if schema_path.exists():\n",
    "            with open(schema_path, 'r') as f:\n",
    "                return f.read()\n",
    "        else:\n",
    "            logger.warning(f\"Schema file not found: {schema_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading schema: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_data_file(data_path: Path) -> Optional[str]:\n",
    "    \"\"\"Load data from SQL file.\"\"\"\n",
    "    try:\n",
    "        if data_path.exists():\n",
    "            with open(data_path, 'r') as f:\n",
    "                return f.read()\n",
    "        else:\n",
    "            logger.warning(f\"Data file not found: {data_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load schema and data\n",
    "schema_sql = load_schema_file(SCHEMA_FILE)\n",
    "data_sql = load_data_file(DATA_FILE)\n",
    "\n",
    "if schema_sql:\n",
    "    print(f\"\u2713 Schema loaded ({len(schema_sql)} characters)\")\n",
    "if data_sql:\n",
    "    print(f\"\u2713 Data loaded ({len(data_sql)} characters)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_from_csv(csv_path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Extract data from CSV file.\"\"\"\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            logger.info(f\"Loaded {len(df)} rows from {csv_path.name}\")\n",
    "            return df\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_from_json(json_path: Path) -> Optional[Dict]:\n",
    "    \"\"\"Extract data from JSON file.\"\"\"\n",
    "    try:\n",
    "        if json_path.exists():\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            logger.info(f\"Loaded JSON from {json_path.name}\")\n",
    "            return data\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading JSON {json_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Find and load data files\n",
    "csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
    "json_files = list(DATA_DIR.glob(\"*.json\"))\n",
    "\n",
    "extracted_data = {}\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = extract_from_csv(csv_file)\n",
    "    if df is not None:\n",
    "        extracted_data[csv_file.stem] = df\n",
    "\n",
    "for json_file in json_files:\n",
    "    data = extract_from_json(json_file)\n",
    "    if data is not None:\n",
    "        extracted_data[json_file.stem] = data\n",
    "\n",
    "print(f\"\u2713 Extracted {len(extracted_data)} data sources\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Transform - Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean DataFrame: handle missing values, remove duplicates, etc.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Remove duplicates\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df)\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_before = df.isnull().sum().sum()\n",
    "    # Fill numeric columns with median\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    # Fill text columns with mode\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else '')\n",
    "    missing_after = df.isnull().sum().sum()\n",
    "    \n",
    "    logger.info(f\"Cleaned data: removed {duplicates_removed} duplicates, filled {missing_before - missing_after} missing values\")\n",
    "    return df\n",
    "\n",
    "# Clean extracted data\n",
    "cleaned_data = {}\n",
    "for name, data in extracted_data.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        cleaned_data[name] = clean_dataframe(data)\n",
    "    else:\n",
    "        cleaned_data[name] = data\n",
    "\n",
    "print(f\"\u2713 Cleaned {len(cleaned_data)} data sources\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def validate_dataframe(df: pd.DataFrame, required_columns: List[str] = None) -> Dict:\n",
    "    \"\"\"Validate DataFrame structure and data quality.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return {'valid': False, 'errors': ['DataFrame is empty or None']}\n",
    "    \n",
    "    validation_results = {\n",
    "        'valid': True,\n",
    "        'row_count': len(df),\n",
    "        'column_count': len(df.columns),\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'duplicate_rows': df.duplicated().sum(),\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    # Check required columns\n",
    "    if required_columns:\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            validation_results['valid'] = False\n",
    "            validation_results['errors'].append(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate cleaned data\n",
    "validation_results = {}\n",
    "for name, data in cleaned_data.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        validation_results[name] = validate_dataframe(data)\n",
    "\n",
    "# Display validation results\n",
    "for name, results in validation_results.items():\n",
    "    status = \"\u2713\" if results['valid'] else \"\u2717\"\n",
    "    print(f\"{status} {name}: {results['row_count']} rows, {results['column_count']} columns\")\n",
    "    if results['errors']:\n",
    "        for error in results['errors']:\n",
    "            print(f\"  Error: {error}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Load - Data Loading to Target Database"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_to_postgresql(df: pd.DataFrame, table_name: str, connection_string: str, if_exists: str = 'replace') -> bool:\n",
    "    \"\"\"Load DataFrame to PostgreSQL table.\"\"\"\n",
    "    if not SQLALCHEMY_AVAILABLE or connection_string is None:\n",
    "        logger.warning(\"PostgreSQL connection not available\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        df.to_sql(table_name, engine, if_exists=if_exists, index=False)\n",
    "        logger.info(f\"Loaded {len(df)} rows to PostgreSQL table {table_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading to PostgreSQL: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_to_snowflake(df: pd.DataFrame, table_name: str, connection_string: str) -> bool:\n",
    "    \"\"\"Load DataFrame to Snowflake table.\"\"\"\n",
    "    if not SQLALCHEMY_AVAILABLE or connection_string is None:\n",
    "        logger.warning(\"Snowflake connection not available\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "        logger.info(f\"Loaded {len(df)} rows to Snowflake table {table_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading to Snowflake: {e}\")\n",
    "        return False\n",
    "\n",
    "# Load data to target databases\n",
    "load_results = {}\n",
    "\n",
    "for name, data in cleaned_data.items():\n",
    "    if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "        table_name = name.lower().replace(' ', '_')\n",
    "        \n",
    "        # PostgreSQL\n",
    "        if POSTGRES_CONNECTION_STRING:\n",
    "            load_results[f\"{name}_postgres\"] = load_to_postgresql(\n",
    "                data, table_name, POSTGRES_CONNECTION_STRING\n",
    "            )\n",
    "        \n",
    "        # Snowflake\n",
    "        if SNOWFLAKE_CONNECTION_STRING:\n",
    "            load_results[f\"{name}_snowflake\"] = load_to_snowflake(\n",
    "                data, table_name, SNOWFLAKE_CONNECTION_STRING\n",
    "            )\n",
    "\n",
    "print(f\"\u2713 Loaded {sum(load_results.values())} datasets to target databases\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Validate - Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_data_quality_report(df: pd.DataFrame, table_name: str) -> Dict:\n",
    "    \"\"\"Generate comprehensive data quality report.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return {'table': table_name, 'status': 'empty'}\n",
    "    \n",
    "    report = {\n",
    "        'table': table_name,\n",
    "        'row_count': len(df),\n",
    "        'column_count': len(df.columns),\n",
    "        'missing_values': int(df.isnull().sum().sum()),\n",
    "        'missing_percentage': float((df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100),\n",
    "        'duplicate_rows': int(df.duplicated().sum()),\n",
    "        'data_types': df.dtypes.astype(str).to_dict(),\n",
    "        'numeric_stats': {},\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Add statistics for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        report['numeric_stats'] = df[numeric_cols].describe().to_dict()\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate quality reports\n",
    "quality_reports = {}\n",
    "for name, data in cleaned_data.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        quality_reports[name] = generate_data_quality_report(data, name)\n",
    "\n",
    "# Display quality reports\n",
    "for name, report in quality_reports.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Rows: {report['row_count']}\")\n",
    "    print(f\"Columns: {report['column_count']}\")\n",
    "    print(f\"Missing values: {report['missing_values']} ({report['missing_percentage']:.2f}%)\")\n",
    "    print(f\"Duplicate rows: {report['duplicate_rows']}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Monitor - Pipeline Monitoring and Logging"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save pipeline execution metadata\n",
    "pipeline_metadata = {\n",
    "    'database': DB_NAME,\n",
    "    'execution_timestamp': datetime.now().isoformat(),\n",
    "    'data_sources': list(extracted_data.keys()),\n",
    "    'extracted_count': len(extracted_data),\n",
    "    'cleaned_count': len(cleaned_data),\n",
    "    'validation_results': validation_results,\n",
    "    'load_results': load_results,\n",
    "    'quality_reports': quality_reports,\n",
    "    'status': 'completed'\n",
    "}\n",
    "\n",
    "# Save metadata to JSON\n",
    "metadata_file = DB_PATH / \"metadata\" / \"pipeline_metadata.json\"\n",
    "metadata_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(pipeline_metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\u2713 Pipeline metadata saved to {metadata_file}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Database: {pipeline_metadata['database']}\")\n",
    "print(f\"Execution time: {pipeline_metadata['execution_timestamp']}\")\n",
    "print(f\"Data sources extracted: {pipeline_metadata['extracted_count']}\")\n",
    "print(f\"Datasets cleaned: {pipeline_metadata['cleaned_count']}\")\n",
    "print(f\"Successful loads: {sum(pipeline_metadata['load_results'].values())}\")\n",
    "print(f\"Status: {pipeline_metadata['status']}\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}