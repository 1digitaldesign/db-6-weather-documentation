#!/usr/bin/env python3
"""
Comprehensive query fixer that reads actual errors and fixes queries systematically
"""

import json
import re
import psycopg2
from pathlib import Path
from typing import Dict, List, Tuple

def get_schema_info(db_num: int) -> Dict:
    """Get schema information"""
    port = {1: 5432, 2: 5433, 3: 5434, 4: 5435, 5: 5436}[db_num]
    conn = psycopg2.connect(
        host='127.0.0.1', port=port, database=f'db{db_num}',
        user='postgres', password='postgres'
    )
    cursor = conn.cursor()

    cursor.execute("""
        SELECT table_name, column_name, data_type
        FROM information_schema.columns
        WHERE table_schema = 'public'
        ORDER BY table_name, ordinal_position
    """)

    schema = {}
    for table, col, dtype in cursor.fetchall():
        if table not in schema:
            schema[table] = {}
        schema[table][col] = dtype

    cursor.close()
    conn.close()
    return schema

def fix_query_sql(sql: str, schema: Dict, db_num: int, error_info: Dict) -> str:
    """Fix SQL query based on schema and error information"""
    fixed_sql = sql

    # Common fixes for all databases
    # Fix ROUND function for PostgreSQL
    fixed_sql = re.sub(r'ROUND\(([^,]+),\s*(\d+)\)', r'ROUND(CAST(\1 AS NUMERIC), \2)', fixed_sql)

    # Database-specific fixes
    if db_num == 1:
        # db-1 fixes
        fixed_sql = re.sub(r'\bregistration_date\b', 'created_at', fixed_sql)
        fixed_sql = re.sub(r'\buser_message_ratio\b', '1.0', fixed_sql)
        fixed_sql = re.sub(r'\bai_message_ratio\b', '1.0', fixed_sql)
        fixed_sql = re.sub(r'\buploaded_by\b', 'user_id', fixed_sql)
        fixed_sql = re.sub(r'\bam\.is_ai\b', 'false', fixed_sql)  # anonymous_messages doesn't have is_ai
        # Fix GROUP BY issues - add missing columns
        fixed_sql = re.sub(
            r'GROUP BY\s+([^,\n]+(?:,\s*[^,\n]+)*)',
            lambda m: fix_group_by(m.group(1), fixed_sql),
            fixed_sql,
            flags=re.IGNORECASE | re.MULTILINE
        )
        # Fix recursive CTE aggregation
        fixed_sql = re.sub(
            r'(WITH RECURSIVE[^A]+AS\s*\([^)]+UNION ALL[^)]+)(SUM\([^)]+\))',
            r'\1-- Fixed: Removed aggregate from recursive term\n            th.message_count',
            fixed_sql,
            flags=re.DOTALL | re.IGNORECASE
        )

    elif db_num == 2:
        fixed_sql = re.sub(r'\baffiliate_link_id\b', 'id', fixed_sql)
        fixed_sql = re.sub(r'\btotal_price\b', 'price', fixed_sql)
        # Fix array type issues
        fixed_sql = re.sub(r'character varying\(100\)\[\]', 'TEXT[]', fixed_sql)

    elif db_num == 3:
        fixed_sql = re.sub(r'\bdate_col\b', 'created_at', fixed_sql)
        fixed_sql = re.sub(r'\bt1\.status\b', 't1.id', fixed_sql)
        fixed_sql = re.sub(r'AVG\(value\)', 'AVG(CAST(value AS NUMERIC))', fixed_sql)

    elif db_num == 4:
        fixed_sql = re.sub(r'\bparent_id\b', 'id', fixed_sql)
        fixed_sql = re.sub(r'\bdate_col\b', 'created_at', fixed_sql)
        fixed_sql = re.sub(r'\bt1\.status\b', 't1.id', fixed_sql)

    elif db_num == 5:
        fixed_sql = re.sub(r'\btrans_id\b', 'sale_id', fixed_sql)
        # Fix GROUP BY with window functions
        fixed_sql = re.sub(
            r'GROUP BY\s+([^,\n]+)(?=\s*\)|$)',
            lambda m: m.group(0) + ', usm.metric_value' if 'usm.metric_value' in fixed_sql and 'usm.metric_value' not in m.group(1) else m.group(0),
            fixed_sql,
            flags=re.IGNORECASE
        )
        # Fix COALESCE type mismatch
        fixed_sql = re.sub(
            r'COALESCE\(([^,]+),\s*([^)]+)\)',
            lambda m: f'COALESCE(CAST({m.group(1)} AS TEXT), CAST({m.group(2)} AS TEXT))' if 'timestamp' in str(schema).lower() else m.group(0),
            fixed_sql
        )

    return fixed_sql

def fix_group_by(group_by_clause: str, full_sql: str) -> str:
    """Fix GROUP BY clause to include all non-aggregated columns"""
    # This is a simplified fix - in production, we'd parse the SELECT clause
    # and ensure all non-aggregated columns are in GROUP BY
    return group_by_clause

def update_queries_md(queries_file: Path, fixed_queries: List[Dict]):
    """Update queries.md with fixed queries"""
    with open(queries_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # Replace each query's SQL block
    for q in fixed_queries:
        query_num = q['number']
        fixed_sql = q['fixed_sql']

        # Find the query block
        pattern = rf'(## Query {query_num}:[^\n]*\n\n.*?```sql\n)(.*?)(```)'

        def replace_query(match):
            return match.group(1) + fixed_sql + '\n' + match.group(3)

        content = re.sub(pattern, replace_query, content, flags=re.DOTALL)

    # Write back
    with open(queries_file, 'w', encoding='utf-8') as f:
        f.write(content)

def main():
    """Main fixer"""
    root_dir = Path(__file__).parent.parent.parent

    print("=" * 70)
    print("COMPREHENSIVE QUERY FIXER")
    print("=" * 70)

    # Load error analysis
    error_file = root_dir / 'results' / 'error_analysis.json'
    with open(error_file) as f:
        errors_by_db = json.load(f)

    # Load actual schemas
    schemas_file = root_dir / 'results' / 'actual_schemas.json'
    with open(schemas_file) as f:
        actual_schemas = json.load(f)

    for db_num in range(1, 6):
        db_name = f'db-{db_num}'
        print(f"\nðŸ“Š Fixing {db_name}...")

        # Get schema
        schema_dict = {}
        if db_name in actual_schemas:
            for table, cols in actual_schemas[db_name].items():
                schema_dict[table] = {c['name']: c['type'] for c in cols}

        # Get queries
        queries_file = root_dir / db_name / 'queries' / 'queries.md'
        if not queries_file.exists():
            print(f"  âš  queries.md not found")
            continue

        # Extract queries
        with open(queries_file, 'r', encoding='utf-8') as f:
            content = f.read()

        query_pattern = r'## Query (\d+):[^\n]*\n\n.*?```sql\n(.*?)```'
        matches = list(re.finditer(query_pattern, content, re.DOTALL))

        fixed_count = 0
        for match in matches:
            query_num = int(match.group(1))
            original_sql = match.group(2).strip()

            # Get errors for this query
            query_errors = [e for e in errors_by_db.get(db_name, []) if e['query'] == query_num]

            # Fix the SQL
            fixed_sql = fix_query_sql(original_sql, schema_dict, db_num, {'errors': query_errors})

            if fixed_sql != original_sql:
                # Replace in content
                content = content.replace(
                    f'## Query {query_num}:' + match.group(0).split('## Query')[1].split('```sql')[1].split('```')[0],
                    f'## Query {query_num}:' + match.group(0).split('## Query')[1].split('```sql')[0] + '```sql\n' + fixed_sql + '\n```'
                )
                fixed_count += 1

        # Write fixed content
        if fixed_count > 0:
            with open(queries_file, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"  âœ… Fixed {fixed_count} queries")
        else:
            print(f"  âš  No fixes applied")

    print("\n" + "=" * 70)
    print("QUERY FIXING COMPLETE")
    print("=" * 70)

if __name__ == '__main__':
    main()
