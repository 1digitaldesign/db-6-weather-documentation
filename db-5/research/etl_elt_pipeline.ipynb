{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL/ELT Pipeline - DB-1\n",
    "\n",
    "This notebook provides a comprehensive ETL/ELT pipeline for database db-1.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Extract**: Load data from source systems\n",
    "2. **Transform**: Clean, validate, and transform data\n",
    "3. **Load**: Load transformed data into target database\n",
    "4. **Validate**: Verify data quality and completeness\n",
    "5. **Monitor**: Track pipeline performance and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Database connections\n",
    "try:\n",
    "    from sqlalchemy import create_engine, text\n",
    "    SQLALCHEMY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SQLALCHEMY_AVAILABLE = False\n",
    "    print(\"Warning: sqlalchemy not available\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\u2713 Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DB_NAME = \"db-5\"\n",
    "DB_PATH = Path.cwd().parent\n",
    "\n",
    "# Database connection strings (configure as needed)\n",
    "# PostgreSQL\n",
    "POSTGRES_CONNECTION_STRING = None  # \"postgresql://user:password@localhost:5432/dbname\"\n",
    "\n",
    "# Databricks\n",
    "DATABRICKS_CONNECTION_STRING = None  # Configure Databricks connection\n",
    "\n",
    "# Databricks\n",
    "SNOWFLAKE_CONNECTION_STRING = None  # Configure Databricks connection\n",
    "\n",
    "# Source data paths\n",
    "DATA_DIR = DB_PATH / \"data\"\n",
    "SCHEMA_FILE = DATA_DIR / \"schema.sql\"\n",
    "DATA_FILE = DATA_DIR / \"data.sql\"\n",
    "\n",
    "print(f\"Database: {DB_NAME}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Schema file exists: {SCHEMA_FILE.exists()}\")\n",
    "print(f\"Data file exists: {DATA_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Extract - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_schema_file(schema_path: Path) -> Optional[str]:\n",
    "    \"\"\"Load database schema from SQL file.\"\"\"\n",
    "    try:\n",
    "        if schema_path.exists():\n",
    "            with open(schema_path, 'r') as f:\n",
    "                return f.read()\n",
    "        else:\n",
    "            logger.warning(f\"Schema file not found: {schema_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading schema: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_data_file(data_path: Path) -> Optional[str]:\n",
    "    \"\"\"Load data from SQL file.\"\"\"\n",
    "    try:\n",
    "        if data_path.exists():\n",
    "            with open(data_path, 'r') as f:\n",
    "                return f.read()\n",
    "        else:\n",
    "            logger.warning(f\"Data file not found: {data_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load schema and data\n",
    "schema_sql = load_schema_file(SCHEMA_FILE)\n",
    "data_sql = load_data_file(DATA_FILE)\n",
    "\n",
    "if schema_sql:\n",
    "    print(f\"\u2713 Schema loaded ({len(schema_sql)} characters)\")\n",
    "if data_sql:\n",
    "    print(f\"\u2713 Data loaded ({len(data_sql)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(csv_path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Extract data from CSV file.\"\"\"\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            logger.info(f\"Loaded {len(df)} rows from {csv_path.name}\")\n",
    "            return df\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_from_json(json_path: Path) -> Optional[Dict]:\n",
    "    \"\"\"Extract data from JSON file.\"\"\"\n",
    "    try:\n",
    "        if json_path.exists():\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            logger.info(f\"Loaded JSON from {json_path.name}\")\n",
    "            return data\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading JSON {json_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Find and load data files\n",
    "csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
    "json_files = list(DATA_DIR.glob(\"*.json\"))\n",
    "\n",
    "extracted_data = {}\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = extract_from_csv(csv_file)\n",
    "    if df is not None:\n",
    "        extracted_data[csv_file.stem] = df\n",
    "\n",
    "for json_file in json_files:\n",
    "    data = extract_from_json(json_file)\n",
    "    if data is not None:\n",
    "        extracted_data[json_file.stem] = data\n",
    "\n",
    "print(f\"\u2713 Extracted {len(extracted_data)} data sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Transform - Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean DataFrame: handle missing values, remove duplicates, etc.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Remove duplicates\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df)\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_before = df.isnull().sum().sum()\n",
    "    # Fill numeric columns with median\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    # Fill text columns with mode\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else '')\n",
    "    missing_after = df.isnull().sum().sum()\n",
    "    \n",
    "    logger.info(f\"Cleaned data: removed {duplicates_removed} duplicates, filled {missing_before - missing_after} missing values\")\n",
    "    return df\n",
    "\n",
    "# Clean extracted data\n",
    "cleaned_data = {}\n",
    "for name, data in extracted_data.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        cleaned_data[name] = clean_dataframe(data)\n",
    "    else:\n",
    "        cleaned_data[name] = data\n",
    "\n",
    "print(f\"\u2713 Cleaned {len(cleaned_data)} data sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataframe(df: pd.DataFrame, required_columns: List[str] = None) -> Dict:\n",
    "    \"\"\"Validate DataFrame structure and data quality.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return {'valid': False, 'errors': ['DataFrame is empty or None']}\n",
    "    \n",
    "    validation_results = {\n",
    "        'valid': True,\n",
    "        'row_count': len(df),\n",
    "        'column_count': len(df.columns),\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'duplicate_rows': df.duplicated().sum(),\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    # Check required columns\n",
    "    if required_columns:\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            validation_results['valid'] = False\n",
    "            validation_results['errors'].append(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate cleaned data\n",
    "validation_results = {}\n",
    "for name, data in cleaned_data.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        validation_results[name] = validate_dataframe(data)\n",
    "\n",
    "# Display validation results\n",
    "for name, results in validation_results.items():\n",
    "    status = \"\u2713\" if results['valid'] else \"\u2717\"\n",
    "    print(f\"{status} {name}: {results['row_count']} rows, {results['column_count']} columns\")\n",
    "    if results['errors']:\n",
    "        for error in results['errors']:\n",
    "            print(f\"  Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Load - Data Loading to Target Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_postgresql(df: pd.DataFrame, table_name: str, connection_string: str, if_exists: str = 'replace') -> bool:\n",
    "    \"\"\"Load DataFrame to PostgreSQL table.\"\"\"\n",
    "    if not SQLALCHEMY_AVAILABLE or connection_string is None:\n",
    "        logger.warning(\"PostgreSQL connection not available\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        df.to_sql(table_name, engine, if_exists=if_exists, index=False)\n",
    "        logger.info(f\"Loaded {len(df)} rows to PostgreSQL table {table_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading to PostgreSQL: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_to_databricks(df: pd.DataFrame, table_name: str, connection_string: str) -> bool:\n",
    "    \"\"\"Load DataFrame to Databricks table.\"\"\"\n",
    "    if not SQLALCHEMY_AVAILABLE or connection_string is None:\n",
    "        logger.warning(\"Databricks connection not available\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "        logger.info(f\"Loaded {len(df)} rows to Databricks table {table_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading to Databricks: {e}\")\n",
    "        return False\n",
    "\n",
    "# Load data to target databases\n",
    "load_results = {}\n",
    "\n",
    "for name, data in cleaned_data.items():\n",
    "    if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "        table_name = name.lower().replace(' ', '_')\n",
    "        \n",
    "        # PostgreSQL\n",
    "        if POSTGRES_CONNECTION_STRING:\n",
    "            load_results[f\"{name}_postgres\"] = load_to_postgresql(\n",
    "                data, table_name, POSTGRES_CONNECTION_STRING\n",
    "            )\n",
    "        \n",
    "        # Databricks\n",
    "        if SNOWFLAKE_CONNECTION_STRING:\n",
    "            load_results[f\"{name}_databricks\"] = load_to_databricks(\n",
    "                data, table_name, SNOWFLAKE_CONNECTION_STRING\n",
    "            )\n",
    "\n",
    "print(f\"\u2713 Loaded {sum(load_results.values())} datasets to target databases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Validate - Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_quality_report(df: pd.DataFrame, table_name: str) -> Dict:\n",
    "    \"\"\"Generate comprehensive data quality report.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return {'table': table_name, 'status': 'empty'}\n",
    "    \n",
    "    report = {\n",
    "        'table': table_name,\n",
    "        'row_count': len(df),\n",
    "        'column_count': len(df.columns),\n",
    "        'missing_values': int(df.isnull().sum().sum()),\n",
    "        'missing_percentage': float((df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100),\n",
    "        'duplicate_rows': int(df.duplicated().sum()),\n",
    "        'data_types': df.dtypes.astype(str).to_dict(),\n",
    "        'numeric_stats': {},\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Add statistics for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        report['numeric_stats'] = df[numeric_cols].describe().to_dict()\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate quality reports\n",
    "quality_reports = {}\n",
    "for name, data in cleaned_data.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        quality_reports[name] = generate_data_quality_report(data, name)\n",
    "\n",
    "# Display quality reports\n",
    "for name, report in quality_reports.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Rows: {report['row_count']}\")\n",
    "    print(f\"Columns: {report['column_count']}\")\n",
    "    print(f\"Missing values: {report['missing_values']} ({report['missing_percentage']:.2f}%)\")\n",
    "    print(f\"Duplicate rows: {report['duplicate_rows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Monitor - Pipeline Monitoring and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pipeline execution metadata\n",
    "pipeline_metadata = {\n",
    "    'database': DB_NAME,\n",
    "    'execution_timestamp': datetime.now().isoformat(),\n",
    "    'data_sources': list(extracted_data.keys()),\n",
    "    'extracted_count': len(extracted_data),\n",
    "    'cleaned_count': len(cleaned_data),\n",
    "    'validation_results': validation_results,\n",
    "    'load_results': load_results,\n",
    "    'quality_reports': quality_reports,\n",
    "    'status': 'completed'\n",
    "}\n",
    "\n",
    "# Save metadata to JSON\n",
    "metadata_file = DB_PATH / \"metadata\" / \"pipeline_metadata.json\"\n",
    "metadata_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(pipeline_metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\u2713 Pipeline metadata saved to {metadata_file}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Database: {pipeline_metadata['database']}\")\n",
    "print(f\"Execution time: {pipeline_metadata['execution_timestamp']}\")\n",
    "print(f\"Data sources extracted: {pipeline_metadata['extracted_count']}\")\n",
    "print(f\"Datasets cleaned: {pipeline_metadata['cleaned_count']}\")\n",
    "print(f\"Successful loads: {sum(pipeline_metadata['load_results'].values())}\")\n",
    "print(f\"Status: {pipeline_metadata['status']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}