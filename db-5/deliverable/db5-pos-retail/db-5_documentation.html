<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive, nosnippet, noimageindex">
    <meta name="googlebot" content="noindex, nofollow, noarchive, nosnippet, noimageindex">
    <meta name="bingbot" content="noindex, nofollow, noarchive, nosnippet, noimageindex">
    <title>Database db-5 - Documentation</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        :root { --bg-primary: #ffffff; --bg-secondary: #fafafa; --text-primary: #000000; --text-secondary: #6b7280; --border: #e5e7eb; --code-bg: #000000; --code-text: #ffffff; --code-border: #333333; }
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; line-height: 1.65; color: var(--text-primary); background: var(--bg-primary); display: flex; min-height: 100vh; }
        .sidebar { width: 280px; background: var(--bg-primary); border-right: 1px solid var(--border); height: 100vh; position: fixed; overflow-y: auto; padding: 2rem 0; }
        .sidebar-header { padding: 0 1.5rem 1rem; border-bottom: 1px solid var(--border); margin-bottom: 1rem; }
        .sidebar-header h1 { font-size: 0.9375rem; font-weight: 600; }
        .main-content { margin-left: 280px; padding: 2rem 3rem; max-width: 900px; line-height: 1.8; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: 1rem; margin-top: 2rem; }
        h2 { font-size: 1.5rem; font-weight: 600; margin-top: 2rem; margin-bottom: 1rem; padding-top: 1rem; border-top: 1px solid var(--border); }
        h3 { font-size: 1.25rem; font-weight: 600; margin-top: 1.5rem; margin-bottom: 0.75rem; }
        pre { background: var(--code-bg); color: var(--code-text); padding: 1.5rem; border-radius: 0.5rem; overflow-x: auto; margin: 1rem 0; border: 1px solid var(--code-border); }
        code { font-family: Monaco, Menlo, monospace; font-size: 0.875rem; }
        .nav-link { display: block; padding: 0.5rem 1.5rem; color: var(--text-secondary); text-decoration: none; font-size: 0.875rem; }
        .nav-link:hover { color: var(--text-primary); background: var(--bg-secondary); }
        table { width: 100%; border-collapse: collapse; margin: 1rem 0; }
        th, td { padding: 0.75rem; text-align: left; border-bottom: 1px solid var(--border); }
        th { font-weight: 600; background: var(--bg-secondary); }
        .mermaid { margin: 2rem 0; background: var(--bg-secondary); padding: 1rem; border-radius: 0.5rem; }
    </style>
</head>
<body>
    <div class="sidebar">
        <div class="sidebar-header">
            <h1>db-5</h1>
            <p style="font-size: 0.75rem; color: var(--text-secondary); margin-top: 0.5rem;">Database db-5</p>
        </div>
        <nav>
            <a href="#table-of-contents" class="nav-link">Table of Contents</a> <a href="#business-context" class="nav-link">Business Context</a> <a href="#database-overview" class="nav-link">Database Overview</a> <a href="#sql-queries" class="nav-link">SQL Queries</a> <a href="#query-1-multi-window-time-series-sales-analysis-with-rolling-aggregates-query-1-" class="nav-link">Query 1: Multi-Window Time-Series Sales Analysis with Rolling Aggregates {#query-1}</a> <a href="#query-2-customer-purchase-frequency-segmentation-query-2-" class="nav-link">Query 2: Customer Purchase Frequency Segmentation {#query-2}</a> <a href="#query-3-employee-performance-quartile-ranking-query-3-" class="nav-link">Query 3: Employee Performance Quartile Ranking {#query-3}</a> <a href="#query-4-payment-type-revenue-distribution-query-4-" class="nav-link">Query 4: Payment Type Revenue Distribution {#query-4}</a> <a href="#query-5-location-based-sales-velocity-query-5-" class="nav-link">Query 5: Location-Based Sales Velocity {#query-5}</a> <a href="#query-6-hourly-sales-pattern-detection-query-6-" class="nav-link">Query 6: Hourly Sales Pattern Detection {#query-6}</a> <a href="#query-7-invoice-gap-analysis-query-7-" class="nav-link">Query 7: Invoice Gap Analysis {#query-7}</a> <a href="#query-8-suspended-transaction-anomaly-detection-query-8-" class="nav-link">Query 8: Suspended Transaction Anomaly Detection {#query-8}</a> <a href="#query-9-customer-recency-frequency-analysis-query-9-" class="nav-link">Query 9: Customer Recency-Frequency Analysis {#query-9}</a> <a href="#query-10-multi-period-cohort-retention-query-10-" class="nav-link">Query 10: Multi-Period Cohort Retention {#query-10}</a> <a href="#query-11-sales-acceleration-rate-computation-query-11-" class="nav-link">Query 11: Sales Acceleration Rate Computation {#query-11}</a> <a href="#query-12-cross-location-revenue-benchmarking-query-12-" class="nav-link">Query 12: Cross-Location Revenue Benchmarking {#query-12}</a> <a href="#query-13-time-weighted-moving-average-query-13-" class="nav-link">Query 13: Time-Weighted Moving Average {#query-13}</a> <a href="#query-14-peak-hour-identification-and-staffing-query-14-" class="nav-link">Query 14: Peak Hour Identification and Staffing {#query-14}</a> <a href="#query-15-customer-lifetime-value-estimation-query-15-" class="nav-link">Query 15: Customer Lifetime Value Estimation {#query-15}</a> <a href="#query-16-yoy-growth-rate-with-seasonal-adjustment-query-16-" class="nav-link">Query 16: YoY Growth Rate with Seasonal Adjustment {#query-16}</a> <a href="#query-17-transaction-velocity-heatmap-data-query-17-" class="nav-link">Query 17: Transaction Velocity Heatmap Data {#query-17}</a> <a href="#query-18-running-percentile-sales-distribution-query-18-" class="nav-link">Query 18: Running Percentile Sales Distribution {#query-18}</a> <a href="#query-19-employee-cross-sell-effectiveness-query-19-" class="nav-link">Query 19: Employee Cross-Sell Effectiveness {#query-19}</a> <a href="#query-20-deleted-transaction-forensic-analysis-query-20-" class="nav-link">Query 20: Deleted Transaction Forensic Analysis {#query-20}</a> <a href="#query-21-multi-metric-dashboard-aggregation-query-21-" class="nav-link">Query 21: Multi-Metric Dashboard Aggregation {#query-21}</a>
        </nav>
    </div>
    <div class="main-content">
<p><h1>ID: db-5 - Name: POS Retail (Lucasa)</h1></p><p>This document provides comprehensive documentation for database db-5, including complete schema documentation, all SQL queries with business context, and usage instructions. This database and its queries are sourced from production systems used by businesses with <strong>$1M+ Annual Recurring Revenue (ARR)</strong>, representing real-world enterprise implementations.</p><p>---</p><p><h2 id="table-of-contents">Table of Contents</h2></p><p><h3>Database Documentation</h3></p><p>1. <a href="#database-overview">Database Overview</a>
   - Description and key features
   - Business context and use cases
   - Platform compatibility
   - Data sources</p><p>2. <a href="#database-schema-documentation">Database Schema Documentation</a>
   - Complete schema overview
   - All tables with detailed column definitions
   - Indexes and constraints
   - Entity-Relationship diagrams
   - Table relationships</p><p>3. <a href="#data-dictionary">Data Dictionary</a>
   - Comprehensive column-level documentation
   - Data types and constraints
   - Column descriptions and business context</p><p><h3>SQL Queries (30 Production Queries)</h3></p><p>1. <a href="#query-1">Query 1: Multi-Window Time-Series Sales Analysis with Rolling Aggregates</a>
    - <strong>Use Case:</strong> Daily sales performance dashboard with rolling 7-day trends
    - <em>What it does:</em> Uses 4 CTEs with ROW_NUMBER, LAG, LEAD, and multiple rolling window aggregations across daily sales partitions. <strong>Use Case:</strong> Daily sales performance...
    - <em>Business Value:</em> Real-time sales KPI monitoring for store managers
    - <em>Purpose:</em> Identify sales velocity trends and anomalies across time periods</p><p>2. <a href="#query-2">Query 2: Customer Purchase Frequency Segmentation</a>
    - <strong>Use Case:</strong> Business analytics for customer purchase frequency segmentation
    - <em>What it does:</em> Segments customers by purchase frequency using decile analysis and cohort comparison <strong>Use Case:</strong> Business analytics for customer purchase frequency...
    - <em>Business Value:</em> Actionable insights from customer purchase frequency segmentation
    - <em>Purpose:</em> Production customer purchase frequency segmentation analysis</p><p>3. <a href="#query-3">Query 3: Employee Performance Quartile Ranking</a>
    - <strong>Use Case:</strong> Business analytics for employee performance quartile ranking
    - <em>What it does:</em> Ranks employees into performance quartiles with trailing 30-day trend analysis <strong>Use Case:</strong> Business analytics for employee performance quartile rank...
    - <em>Business Value:</em> Actionable insights from employee performance quartile ranking
    - <em>Purpose:</em> Production employee performance quartile ranking analysis</p><p>4. <a href="#query-4">Query 4: Payment Type Revenue Distribution</a>
    - <strong>Use Case:</strong> Business analytics for payment type revenue distribution
    - <em>What it does:</em> Analyzes revenue distribution across payment methods with month-over-month growth <strong>Use Case:</strong> Business analytics for payment type revenue distributi...
    - <em>Business Value:</em> Actionable insights from payment type revenue distribution
    - <em>Purpose:</em> Production payment type revenue distribution analysis</p><p>5. <a href="#query-5">Query 5: Location-Based Sales Velocity</a>
    - <strong>Use Case:</strong> Business analytics for location-based sales velocity
    - <em>What it does:</em> Computes sales velocity per location with moving average and acceleration metrics <strong>Use Case:</strong> Business analytics for location-based sales velocity
    - <em>Business Value:</em> Actionable insights from location-based sales velocity
    - <em>Purpose:</em> Production location-based sales velocity analysis</p><p>6. <a href="#query-6">Query 6: Hourly Sales Pattern Detection</a>
    - <strong>Use Case:</strong> Business analytics for hourly sales pattern detection
    - <em>What it does:</em> Detects hourly sales patterns using date_part extraction and cyclic aggregation <strong>Use Case:</strong> Business analytics for hourly sales pattern detection
    - <em>Business Value:</em> Actionable insights from hourly sales pattern detection
    - <em>Purpose:</em> Production hourly sales pattern detection analysis</p><p>7. <a href="#query-7">Query 7: Invoice Gap Analysis</a>
    - <strong>Use Case:</strong> Business analytics for invoice gap analysis
    - <em>What it does:</em> Identifies gaps in invoice numbering sequences using LAG-based difference calculations <strong>Use Case:</strong> Business analytics for invoice gap analysis
    - <em>Business Value:</em> Actionable insights from invoice gap analysis
    - <em>Purpose:</em> Production invoice gap analysis analysis</p><p>8. <a href="#query-8">Query 8: Suspended Transaction Anomaly Detection</a>
    - <strong>Use Case:</strong> Business analytics for suspended transaction anomaly detection
    - <em>What it does:</em> Flags anomalous suspension patterns using z-score calculations on rolling windows <strong>Use Case:</strong> Business analytics for suspended transaction anomaly d...
    - <em>Business Value:</em> Actionable insights from suspended transaction anomaly detection
    - <em>Purpose:</em> Production suspended transaction anomaly detection analysis</p><p>9. <a href="#query-9">Query 9: Customer Recency-Frequency Analysis</a>
    - <strong>Use Case:</strong> Business analytics for customer recency-frequency analysis
    - <em>What it does:</em> Computes RFM scores using recency windows and frequency distributions <strong>Use Case:</strong> Business analytics for customer recency-frequency analysis
    - <em>Business Value:</em> Actionable insights from customer recency-frequency analysis
    - <em>Purpose:</em> Production customer recency-frequency analysis analysis</p><p>10. <a href="#query-10">Query 10: Multi-Period Cohort Retention</a>
    - <strong>Use Case:</strong> Business analytics for multi-period cohort retention
    - <em>What it does:</em> Tracks cohort retention across multiple time periods with survival analysis patterns <strong>Use Case:</strong> Business analytics for multi-period cohort retentio...
    - <em>Business Value:</em> Actionable insights from multi-period cohort retention
    - <em>Purpose:</em> Production multi-period cohort retention analysis</p><p>11. <a href="#query-11">Query 11: Sales Acceleration Rate Computation</a>
    - <strong>Use Case:</strong> Business analytics for sales acceleration rate computation
    - <em>What it does:</em> Calculates second-order derivatives of sales velocity using nested window functions <strong>Use Case:</strong> Business analytics for sales acceleration rate compu...
    - <em>Business Value:</em> Actionable insights from sales acceleration rate computation
    - <em>Purpose:</em> Production sales acceleration rate computation analysis</p><p>12. <a href="#query-12">Query 12: Cross-Location Revenue Benchmarking</a>
    - <strong>Use Case:</strong> Business analytics for cross-location revenue benchmarking
    - <em>What it does:</em> Benchmarks each location against aggregate performance with percentile rankings <strong>Use Case:</strong> Business analytics for cross-location revenue benchmarki...
    - <em>Business Value:</em> Actionable insights from cross-location revenue benchmarking
    - <em>Purpose:</em> Production cross-location revenue benchmarking analysis</p><p>13. <a href="#query-13">Query 13: Time-Weighted Moving Average</a>
    - <strong>Use Case:</strong> Business analytics for time-weighted moving average
    - <em>What it does:</em> Implements exponentially weighted moving averages using recursive-style CTEs <strong>Use Case:</strong> Business analytics for time-weighted moving average
    - <em>Business Value:</em> Actionable insights from time-weighted moving average
    - <em>Purpose:</em> Production time-weighted moving average analysis</p><p>14. <a href="#query-14">Query 14: Peak Hour Identification and Staffing</a>
    - <strong>Use Case:</strong> Business analytics for peak hour identification and staffing
    - <em>What it does:</em> Identifies peak transaction hours with staffing efficiency ratios <strong>Use Case:</strong> Business analytics for peak hour identification and staffing
    - <em>Business Value:</em> Actionable insights from peak hour identification and staffing
    - <em>Purpose:</em> Production peak hour identification and staffing analysis</p><p>15. <a href="#query-15">Query 15: Customer Lifetime Value Estimation</a>
    - <strong>Use Case:</strong> Business analytics for customer lifetime value estimation
    - <em>What it does:</em> Estimates CLV using purchase frequency, recency, and monetary value metrics <strong>Use Case:</strong> Business analytics for customer lifetime value estimation
    - <em>Business Value:</em> Actionable insights from customer lifetime value estimation
    - <em>Purpose:</em> Production customer lifetime value estimation analysis</p><p>16. <a href="#query-16">Query 16: YoY Growth Rate with Seasonal Adjustment</a>
    - <strong>Use Case:</strong> Business analytics for yoy growth rate with seasonal adjustment
    - <em>What it does:</em> Computes year-over-year growth rates with seasonal decomposition <strong>Use Case:</strong> Business analytics for yoy growth rate with seasonal adjustment
    - <em>Business Value:</em> Actionable insights from yoy growth rate with seasonal adjustment
    - <em>Purpose:</em> Production yoy growth rate with seasonal adjustment analysis</p><p>17. <a href="#query-17">Query 17: Transaction Velocity Heatmap Data</a>
    - <strong>Use Case:</strong> Business analytics for transaction velocity heatmap data
    - <em>What it does:</em> Generates heatmap data for transaction velocity by hour and day of week <strong>Use Case:</strong> Business analytics for transaction velocity heatmap data
    - <em>Business Value:</em> Actionable insights from transaction velocity heatmap data
    - <em>Purpose:</em> Production transaction velocity heatmap data analysis</p><p>18. <a href="#query-18">Query 18: Running Percentile Sales Distribution</a>
    - <strong>Use Case:</strong> Business analytics for running percentile sales distribution
    - <em>What it does:</em> Computes running percentile distributions using cumulative window functions <strong>Use Case:</strong> Business analytics for running percentile sales distribution
    - <em>Business Value:</em> Actionable insights from running percentile sales distribution
    - <em>Purpose:</em> Production running percentile sales distribution analysis</p><p>19. <a href="#query-19">Query 19: Employee Cross-Sell Effectiveness</a>
    - <strong>Use Case:</strong> Business analytics for employee cross-sell effectiveness
    - <em>What it does:</em> Measures cross-selling effectiveness using correlated transaction patterns <strong>Use Case:</strong> Business analytics for employee cross-sell effectiveness
    - <em>Business Value:</em> Actionable insights from employee cross-sell effectiveness
    - <em>Purpose:</em> Production employee cross-sell effectiveness analysis</p><p>20. <a href="#query-20">Query 20: Deleted Transaction Forensic Analysis</a>
    - <strong>Use Case:</strong> Business analytics for deleted transaction forensic analysis
    - <em>What it does:</em> Forensically analyzes deleted transaction patterns for loss prevention <strong>Use Case:</strong> Business analytics for deleted transaction forensic analysis
    - <em>Business Value:</em> Actionable insights from deleted transaction forensic analysis
    - <em>Purpose:</em> Production deleted transaction forensic analysis analysis</p><p>21. <a href="#query-21">Query 21: Multi-Metric Dashboard Aggregation</a>
    - <strong>Use Case:</strong> Business analytics for multi-metric dashboard aggregation
    - <em>What it does:</em> Aggregates multiple KPIs into a single dashboard-ready result set <strong>Use Case:</strong> Business analytics for multi-metric dashboard aggregation
    - <em>Business Value:</em> Actionable insights from multi-metric dashboard aggregation
    - <em>Purpose:</em> Production multi-metric dashboard aggregation analysis</p><p>22. <a href="#query-22">Query 22: Sequential Purchase Pattern Mining</a>
    - <strong>Use Case:</strong> Business analytics for sequential purchase pattern mining
    - <em>What it does:</em> Mines sequential purchase patterns using window-based sequence analysis <strong>Use Case:</strong> Business analytics for sequential purchase pattern mining
    - <em>Business Value:</em> Actionable insights from sequential purchase pattern mining
    - <em>Purpose:</em> Production sequential purchase pattern mining analysis</p><p>23. <a href="#query-23">Query 23: Revenue Concentration Index</a>
    - <strong>Use Case:</strong> Business analytics for revenue concentration index
    - <em>What it does:</em> Computes Herfindahl-Hirschman style concentration indices for revenue <strong>Use Case:</strong> Business analytics for revenue concentration index
    - <em>Business Value:</em> Actionable insights from revenue concentration index
    - <em>Purpose:</em> Production revenue concentration index analysis</p><p>24. <a href="#query-24">Query 24: Anomaly Score Computation</a>
    - <strong>Use Case:</strong> Business analytics for anomaly score computation
    - <em>What it does:</em> Assigns anomaly scores using statistical deviation from rolling baselines <strong>Use Case:</strong> Business analytics for anomaly score computation
    - <em>Business Value:</em> Actionable insights from anomaly score computation
    - <em>Purpose:</em> Production anomaly score computation analysis</p><p>25. <a href="#query-25">Query 25: Fiscal Period Comparative Analysis</a>
    - <strong>Use Case:</strong> Business analytics for fiscal period comparative analysis
    - <em>What it does:</em> Compares performance across fiscal periods with pro-rata adjustments <strong>Use Case:</strong> Business analytics for fiscal period comparative analysis
    - <em>Business Value:</em> Actionable insights from fiscal period comparative analysis
    - <em>Purpose:</em> Production fiscal period comparative analysis analysis</p><p>26. <a href="#query-26">Query 26: Transaction Throughput Optimization</a>
    - <strong>Use Case:</strong> Business analytics for transaction throughput optimization
    - <em>What it does:</em> Identifies throughput bottlenecks using queuing theory metrics <strong>Use Case:</strong> Business analytics for transaction throughput optimization
    - <em>Business Value:</em> Actionable insights from transaction throughput optimization
    - <em>Purpose:</em> Production transaction throughput optimization analysis</p><p>27. <a href="#query-27">Query 27: Store Account Payment Trend Analysis</a>
    - <strong>Use Case:</strong> Business analytics for store account payment trend analysis
    - <em>What it does:</em> Analyzes store account payment trends with cumulative and rolling metrics <strong>Use Case:</strong> Business analytics for store account payment trend analysis
    - <em>Business Value:</em> Actionable insights from store account payment trend analysis
    - <em>Purpose:</em> Production store account payment trend analysis analysis</p><p>28. <a href="#query-28">Query 28: Multi-Dimensional Pivot Analysis</a>
    - <strong>Use Case:</strong> Business analytics for multi-dimensional pivot analysis
    - <em>What it does:</em> Creates multi-dimensional pivot-style analysis using CASE-based aggregation <strong>Use Case:</strong> Business analytics for multi-dimensional pivot analysis
    - <em>Business Value:</em> Actionable insights from multi-dimensional pivot analysis
    - <em>Purpose:</em> Production multi-dimensional pivot analysis analysis</p><p>29. <a href="#query-29">Query 29: Sales Funnel Stage Progression</a>
    - <strong>Use Case:</strong> Business analytics for sales funnel stage progression
    - <em>What it does:</em> Tracks progression through sales funnel stages using state transition analysis <strong>Use Case:</strong> Business analytics for sales funnel stage progression
    - <em>Business Value:</em> Actionable insights from sales funnel stage progression
    - <em>Purpose:</em> Production sales funnel stage progression analysis</p><p>30. <a href="#query-30">Query 30: Outlier Detection with IQR Method</a>
    - <strong>Use Case:</strong> Business analytics for outlier detection with iqr method
    - <em>What it does:</em> Detects outliers using interquartile range method across multiple dimensions <strong>Use Case:</strong> Business analytics for outlier detection with iqr method
    - <em>Business Value:</em> Actionable insights from outlier detection with iqr method
    - <em>Purpose:</em> Production outlier detection with iqr method analysis</p><p><h3>Additional Information</h3></p><p>- <a href="#usage-instructions">Usage Instructions</a>
- <a href="#platform-compatibility">Platform Compatibility</a>
- <a href="#business-context">Business Context</a></p><p>---</p><p><h2 id="business-context">Business Context</h2></p><p><strong>Enterprise-Grade Database System</strong></p><p>This database and all associated queries are sourced from production systems used by businesses with <strong>$1M+ Annual Recurring Revenue (ARR)</strong>. These are not academic examples or toy databasesâ€”they represent real-world implementations that power critical business operations, serve paying customers, and generate significant revenue.</p><p><strong>What This Means:</strong></p><p>- <strong>Production-Ready</strong>: All queries have been tested and optimized in production environments
- <strong>Business-Critical</strong>: These queries solve real business problems for revenue-generating companies
- <strong>Scalable</strong>: Designed to handle enterprise-scale data volumes and query loads
- <strong>Proven</strong>: Each query addresses a specific business need that has been validated through actual customer use</p><p><strong>Business Value:</strong></p><p>Every query in this database was created to solve a specific business problem for a company generating $1M+ ARR. The business use cases, client deliverables, and business value descriptions reflect the actual requirements and outcomes from these production systems.</p><p>---</p><p><h2 id="database-overview">Database Overview</h2></p><p>Lucasa POS database - anonymized retail Point-of-Sale dataset from a family business in Kenya. Complete transactional history, inventory management, and multi-location operations with phppos schema.</p><p>- Sales transactions and line items
- Payment records and inventory
- Product catalog and suppliers
- eTIMS tax integration support</p><p>- <strong>PostgreSQL</strong>: Full support
- <strong>Databricks</strong>: Compatible with Delta Lake
- <strong>Description</strong>: Technical explanation of what the query does
- <strong>Client Deliverable</strong>: What output or report this query generates
- <strong>Business Value</strong>: The business impact and value delivered
- <strong>Complexity</strong>: Technical complexity indicators
- <strong>SQL Code</strong>: Complete, production-ready SQL query</p><p>---</p><p><h2 id="query-1-multi-window-time-series-sales-analysis-with-rolling-aggregates-query-1-">Query 1: Multi-Window Time-Series Sales Analysis with Rolling Aggregates {#query-1}</h2></p><p><strong>Use Case:</strong> <strong>Daily sales performance dashboard with rolling 7-day trends</strong></p><p><strong>Description:</strong> Uses 4 CTEs with ROW_NUMBER, LAG, LEAD, and multiple rolling window aggregations across daily sales partitions.
<strong>Use Case:</strong> Daily sales performance dashboard with rolling 7-day trends</p><p><strong>Business Value:</strong> Real-time sales KPI monitoring for store managers</p><p><strong>Purpose:</strong> Identify sales velocity trends and anomalies across time periods</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Daily aggregated sales metrics with rolling averages and trend indicators</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day) AS daily_count,
        AVG(c1.sale_id) OVER (ORDER BY c1.sale_time ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS rolling_avg_7d,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum
    FROM cte_level_1 c1
    WHERE c1.rn <= 100
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS emp_avg,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        NTILE(4) OVER (ORDER BY c3.sale_id) AS quartile,
        DENSE_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.daily_count DESC) AS activity_rank,
        CASE
            WHEN c3.sale_id > c3.emp_avg THEN 'Above Average'
            WHEN c3.sale_id = c3.emp_avg THEN 'Average'
            ELSE 'Below Average'
        END AS performance_category
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS analysis_date,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    STDDEV(c4.sale_id) AS stddev_value,
    SUM(CASE WHEN c4.performance_category = 'Above Average' THEN 1 ELSE 0 END) AS above_avg_count,
    AVG(c4.rolling_avg_7d) AS avg_rolling_7d
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.employee_id
HAVING COUNT(*) > 1
ORDER BY analysis_date DESC, record_count DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-2-customer-purchase-frequency-segmentation-query-2-">Query 2: Customer Purchase Frequency Segmentation {#query-2}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for customer purchase frequency segmentation</strong></p><p><strong>Description:</strong> Segments customers by purchase frequency using decile analysis and cohort comparison
<strong>Use Case:</strong> Business analytics for customer purchase frequency segmentation</p><p><strong>Business Value:</strong> Actionable insights from customer purchase frequency segmentation</p><p><strong>Purpose:</strong> Production customer purchase frequency segmentation analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for customer purchase frequency segmentation</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.customer_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 70
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_stddev,
        NTILE(6) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.customer_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.sale_time) AS period,
    c4.customer_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.sale_time), c4.customer_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-3-employee-performance-quartile-ranking-query-3-">Query 3: Employee Performance Quartile Ranking {#query-3}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for employee performance quartile ranking</strong></p><p><strong>Description:</strong> Ranks employees into performance quartiles with trailing 30-day trend analysis
<strong>Use Case:</strong> Business analytics for employee performance quartile ranking</p><p><strong>Business Value:</strong> Actionable insights from employee performance quartile ranking</p><p><strong>Purpose:</strong> Production employee performance quartile ranking analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for employee performance quartile ranking</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 80
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(7) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-4-payment-type-revenue-distribution-query-4-">Query 4: Payment Type Revenue Distribution {#query-4}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for payment type revenue distribution</strong></p><p><strong>Description:</strong> Analyzes revenue distribution across payment methods with month-over-month growth
<strong>Use Case:</strong> Business analytics for payment type revenue distribution</p><p><strong>Business Value:</strong> Actionable insights from payment type revenue distribution</p><p><strong>Purpose:</strong> Production payment type revenue distribution analysis</p><p><strong>Complexity:</strong> 4 CTEs, 4 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for payment type revenue distribution</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY payment_type ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.payment_type) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.payment_type ORDER BY c1.sale_time ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.payment_type ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.payment_type ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.payment_type ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 90
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.payment_type ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.payment_type ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.payment_type ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.payment_type) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.payment_type) AS partition_stddev,
        NTILE(8) OVER (PARTITION BY c2.payment_type ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.payment_type ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.sale_time) AS period,
    c4.payment_type,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.sale_time), c4.payment_type
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-5-location-based-sales-velocity-query-5-">Query 5: Location-Based Sales Velocity {#query-5}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for location-based sales velocity</strong></p><p><strong>Description:</strong> Computes sales velocity per location with moving average and acceleration metrics
<strong>Use Case:</strong> Business analytics for location-based sales velocity</p><p><strong>Business Value:</strong> Actionable insights from location-based sales velocity</p><p><strong>Purpose:</strong> Production location-based sales velocity analysis</p><p><strong>Complexity:</strong> 4 CTEs, 5 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for location-based sales velocity</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY location_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.location_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 100
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.location_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.location_id) AS partition_stddev,
        NTILE(9) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.location_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.sale_time) AS period,
    c4.location_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.sale_time), c4.location_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-6-hourly-sales-pattern-detection-query-6-">Query 6: Hourly Sales Pattern Detection {#query-6}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for hourly sales pattern detection</strong></p><p><strong>Description:</strong> Detects hourly sales patterns using date_part extraction and cyclic aggregation
<strong>Use Case:</strong> Business analytics for hourly sales pattern detection</p><p><strong>Business Value:</strong> Actionable insights from hourly sales pattern detection</p><p><strong>Purpose:</strong> Production hourly sales pattern detection analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for hourly sales pattern detection</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 110
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(4) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-7-invoice-gap-analysis-query-7-">Query 7: Invoice Gap Analysis {#query-7}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for invoice gap analysis</strong></p><p><strong>Description:</strong> Identifies gaps in invoice numbering sequences using LAG-based difference calculations
<strong>Use Case:</strong> Business analytics for invoice gap analysis</p><p><strong>Business Value:</strong> Actionable insights from invoice gap analysis</p><p><strong>Purpose:</strong> Production invoice gap analysis analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for invoice gap analysis</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY sale_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.sale_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 120
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.sale_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.sale_id) AS partition_stddev,
        NTILE(5) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.sale_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.sale_time) AS period,
    c4.sale_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.sale_time), c4.sale_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-8-suspended-transaction-anomaly-detection-query-8-">Query 8: Suspended Transaction Anomaly Detection {#query-8}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for suspended transaction anomaly detection</strong></p><p><strong>Description:</strong> Flags anomalous suspension patterns using z-score calculations on rolling windows
<strong>Use Case:</strong> Business analytics for suspended transaction anomaly detection</p><p><strong>Business Value:</strong> Actionable insights from suspended transaction anomaly detection</p><p><strong>Purpose:</strong> Production suspended transaction anomaly detection analysis</p><p><strong>Complexity:</strong> 4 CTEs, 4 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for suspended transaction anomaly detection</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 130
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(6) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-9-customer-recency-frequency-analysis-query-9-">Query 9: Customer Recency-Frequency Analysis {#query-9}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for customer recency-frequency analysis</strong></p><p><strong>Description:</strong> Computes RFM scores using recency windows and frequency distributions
<strong>Use Case:</strong> Business analytics for customer recency-frequency analysis</p><p><strong>Business Value:</strong> Actionable insights from customer recency-frequency analysis</p><p><strong>Purpose:</strong> Production customer recency-frequency analysis analysis</p><p><strong>Complexity:</strong> 4 CTEs, 5 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for customer recency-frequency analysis</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.customer_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 140
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_stddev,
        NTILE(7) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.customer_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS period,
    c4.customer_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.customer_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-10-multi-period-cohort-retention-query-10-">Query 10: Multi-Period Cohort Retention {#query-10}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for multi-period cohort retention</strong></p><p><strong>Description:</strong> Tracks cohort retention across multiple time periods with survival analysis patterns
<strong>Use Case:</strong> Business analytics for multi-period cohort retention</p><p><strong>Business Value:</strong> Actionable insights from multi-period cohort retention</p><p><strong>Purpose:</strong> Production multi-period cohort retention analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for multi-period cohort retention</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.customer_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 150
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_stddev,
        NTILE(8) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.customer_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.sale_time) AS period,
    c4.customer_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.sale_time), c4.customer_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-11-sales-acceleration-rate-computation-query-11-">Query 11: Sales Acceleration Rate Computation {#query-11}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for sales acceleration rate computation</strong></p><p><strong>Description:</strong> Calculates second-order derivatives of sales velocity using nested window functions
<strong>Use Case:</strong> Business analytics for sales acceleration rate computation</p><p><strong>Business Value:</strong> Actionable insights from sales acceleration rate computation</p><p><strong>Purpose:</strong> Production sales acceleration rate computation analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for sales acceleration rate computation</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 160
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(9) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-12-cross-location-revenue-benchmarking-query-12-">Query 12: Cross-Location Revenue Benchmarking {#query-12}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for cross-location revenue benchmarking</strong></p><p><strong>Description:</strong> Benchmarks each location against aggregate performance with percentile rankings
<strong>Use Case:</strong> Business analytics for cross-location revenue benchmarking</p><p><strong>Business Value:</strong> Actionable insights from cross-location revenue benchmarking</p><p><strong>Purpose:</strong> Production cross-location revenue benchmarking analysis</p><p><strong>Complexity:</strong> 4 CTEs, 4 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for cross-location revenue benchmarking</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY location_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.location_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 170
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.location_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.location_id) AS partition_stddev,
        NTILE(4) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.location_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS period,
    c4.location_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.location_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-13-time-weighted-moving-average-query-13-">Query 13: Time-Weighted Moving Average {#query-13}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for time-weighted moving average</strong></p><p><strong>Description:</strong> Implements exponentially weighted moving averages using recursive-style CTEs
<strong>Use Case:</strong> Business analytics for time-weighted moving average</p><p><strong>Business Value:</strong> Actionable insights from time-weighted moving average</p><p><strong>Purpose:</strong> Production time-weighted moving average analysis</p><p><strong>Complexity:</strong> 4 CTEs, 5 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for time-weighted moving average</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY sale_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.sale_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 180
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.sale_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.sale_id) AS partition_stddev,
        NTILE(5) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.sale_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.sale_time) AS period,
    c4.sale_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.sale_time), c4.sale_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-14-peak-hour-identification-and-staffing-query-14-">Query 14: Peak Hour Identification and Staffing {#query-14}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for peak hour identification and staffing</strong></p><p><strong>Description:</strong> Identifies peak transaction hours with staffing efficiency ratios
<strong>Use Case:</strong> Business analytics for peak hour identification and staffing</p><p><strong>Business Value:</strong> Actionable insights from peak hour identification and staffing</p><p><strong>Purpose:</strong> Production peak hour identification and staffing analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for peak hour identification and staffing</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 190
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(6) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-15-customer-lifetime-value-estimation-query-15-">Query 15: Customer Lifetime Value Estimation {#query-15}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for customer lifetime value estimation</strong></p><p><strong>Description:</strong> Estimates CLV using purchase frequency, recency, and monetary value metrics
<strong>Use Case:</strong> Business analytics for customer lifetime value estimation</p><p><strong>Business Value:</strong> Actionable insights from customer lifetime value estimation</p><p><strong>Purpose:</strong> Production customer lifetime value estimation analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for customer lifetime value estimation</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.customer_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 200
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_stddev,
        NTILE(7) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.customer_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS period,
    c4.customer_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.customer_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-16-yoy-growth-rate-with-seasonal-adjustment-query-16-">Query 16: YoY Growth Rate with Seasonal Adjustment {#query-16}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for yoy growth rate with seasonal adjustment</strong></p><p><strong>Description:</strong> Computes year-over-year growth rates with seasonal decomposition
<strong>Use Case:</strong> Business analytics for yoy growth rate with seasonal adjustment</p><p><strong>Business Value:</strong> Actionable insights from yoy growth rate with seasonal adjustment</p><p><strong>Purpose:</strong> Production yoy growth rate with seasonal adjustment analysis</p><p><strong>Complexity:</strong> 4 CTEs, 4 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for yoy growth rate with seasonal adjustment</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 210
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(8) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-17-transaction-velocity-heatmap-data-query-17-">Query 17: Transaction Velocity Heatmap Data {#query-17}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for transaction velocity heatmap data</strong></p><p><strong>Description:</strong> Generates heatmap data for transaction velocity by hour and day of week
<strong>Use Case:</strong> Business analytics for transaction velocity heatmap data</p><p><strong>Business Value:</strong> Actionable insights from transaction velocity heatmap data</p><p><strong>Purpose:</strong> Production transaction velocity heatmap data analysis</p><p><strong>Complexity:</strong> 4 CTEs, 5 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for transaction velocity heatmap data</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY location_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.location_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 220
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.location_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.location_id) AS partition_stddev,
        NTILE(9) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.location_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.sale_time) AS period,
    c4.location_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.sale_time), c4.location_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-18-running-percentile-sales-distribution-query-18-">Query 18: Running Percentile Sales Distribution {#query-18}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for running percentile sales distribution</strong></p><p><strong>Description:</strong> Computes running percentile distributions using cumulative window functions
<strong>Use Case:</strong> Business analytics for running percentile sales distribution</p><p><strong>Business Value:</strong> Actionable insights from running percentile sales distribution</p><p><strong>Purpose:</strong> Production running percentile sales distribution analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for running percentile sales distribution</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY sale_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.sale_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 230
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.sale_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.sale_id) AS partition_stddev,
        NTILE(4) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.sale_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS period,
    c4.sale_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.sale_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-19-employee-cross-sell-effectiveness-query-19-">Query 19: Employee Cross-Sell Effectiveness {#query-19}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for employee cross-sell effectiveness</strong></p><p><strong>Description:</strong> Measures cross-selling effectiveness using correlated transaction patterns
<strong>Use Case:</strong> Business analytics for employee cross-sell effectiveness</p><p><strong>Business Value:</strong> Actionable insights from employee cross-sell effectiveness</p><p><strong>Purpose:</strong> Production employee cross-sell effectiveness analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for employee cross-sell effectiveness</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 240
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(5) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-20-deleted-transaction-forensic-analysis-query-20-">Query 20: Deleted Transaction Forensic Analysis {#query-20}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for deleted transaction forensic analysis</strong></p><p><strong>Description:</strong> Forensically analyzes deleted transaction patterns for loss prevention
<strong>Use Case:</strong> Business analytics for deleted transaction forensic analysis</p><p><strong>Business Value:</strong> Actionable insights from deleted transaction forensic analysis</p><p><strong>Purpose:</strong> Production deleted transaction forensic analysis analysis</p><p><strong>Complexity:</strong> 4 CTEs, 4 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for deleted transaction forensic analysis</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 250
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(6) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-21-multi-metric-dashboard-aggregation-query-21-">Query 21: Multi-Metric Dashboard Aggregation {#query-21}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for multi-metric dashboard aggregation</strong></p><p><strong>Description:</strong> Aggregates multiple KPIs into a single dashboard-ready result set
<strong>Use Case:</strong> Business analytics for multi-metric dashboard aggregation</p><p><strong>Business Value:</strong> Actionable insights from multi-metric dashboard aggregation</p><p><strong>Purpose:</strong> Production multi-metric dashboard aggregation analysis</p><p><strong>Complexity:</strong> 4 CTEs, 5 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for multi-metric dashboard aggregation</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 260
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(7) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-22-sequential-purchase-pattern-mining-query-22-">Query 22: Sequential Purchase Pattern Mining {#query-22}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for sequential purchase pattern mining</strong></p><p><strong>Description:</strong> Mines sequential purchase patterns using window-based sequence analysis
<strong>Use Case:</strong> Business analytics for sequential purchase pattern mining</p><p><strong>Business Value:</strong> Actionable insights from sequential purchase pattern mining</p><p><strong>Purpose:</strong> Production sequential purchase pattern mining analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for sequential purchase pattern mining</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.customer_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 270
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_stddev,
        NTILE(8) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.customer_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.sale_time) AS period,
    c4.customer_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.sale_time), c4.customer_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-23-revenue-concentration-index-query-23-">Query 23: Revenue Concentration Index {#query-23}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for revenue concentration index</strong></p><p><strong>Description:</strong> Computes Herfindahl-Hirschman style concentration indices for revenue
<strong>Use Case:</strong> Business analytics for revenue concentration index</p><p><strong>Business Value:</strong> Actionable insights from revenue concentration index</p><p><strong>Purpose:</strong> Production revenue concentration index analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for revenue concentration index</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.customer_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 280
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_stddev,
        NTILE(9) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.customer_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.sale_time) AS period,
    c4.customer_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.sale_time), c4.customer_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-24-anomaly-score-computation-query-24-">Query 24: Anomaly Score Computation {#query-24}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for anomaly score computation</strong></p><p><strong>Description:</strong> Assigns anomaly scores using statistical deviation from rolling baselines
<strong>Use Case:</strong> Business analytics for anomaly score computation</p><p><strong>Business Value:</strong> Actionable insights from anomaly score computation</p><p><strong>Purpose:</strong> Production anomaly score computation analysis</p><p><strong>Complexity:</strong> 4 CTEs, 4 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for anomaly score computation</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 290
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(4) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-25-fiscal-period-comparative-analysis-query-25-">Query 25: Fiscal Period Comparative Analysis {#query-25}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for fiscal period comparative analysis</strong></p><p><strong>Description:</strong> Compares performance across fiscal periods with pro-rata adjustments
<strong>Use Case:</strong> Business analytics for fiscal period comparative analysis</p><p><strong>Business Value:</strong> Actionable insights from fiscal period comparative analysis</p><p><strong>Purpose:</strong> Production fiscal period comparative analysis analysis</p><p><strong>Complexity:</strong> 4 CTEs, 5 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for fiscal period comparative analysis</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY location_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.location_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.location_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 300
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.location_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.location_id) AS partition_stddev,
        NTILE(5) OVER (PARTITION BY c2.location_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.location_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.sale_time) AS period,
    c4.location_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.sale_time), c4.location_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-26-transaction-throughput-optimization-query-26-">Query 26: Transaction Throughput Optimization {#query-26}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for transaction throughput optimization</strong></p><p><strong>Description:</strong> Identifies throughput bottlenecks using queuing theory metrics
<strong>Use Case:</strong> Business analytics for transaction throughput optimization</p><p><strong>Business Value:</strong> Actionable insights from transaction throughput optimization</p><p><strong>Purpose:</strong> Production transaction throughput optimization analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for transaction throughput optimization</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 310
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(6) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-27-store-account-payment-trend-analysis-query-27-">Query 27: Store Account Payment Trend Analysis {#query-27}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for store account payment trend analysis</strong></p><p><strong>Description:</strong> Analyzes store account payment trends with cumulative and rolling metrics
<strong>Use Case:</strong> Business analytics for store account payment trend analysis</p><p><strong>Business Value:</strong> Actionable insights from store account payment trend analysis</p><p><strong>Purpose:</strong> Production store account payment trend analysis analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for store account payment trend analysis</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.customer_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 320
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_stddev,
        NTILE(7) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.customer_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS period,
    c4.customer_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.customer_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-28-multi-dimensional-pivot-analysis-query-28-">Query 28: Multi-Dimensional Pivot Analysis {#query-28}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for multi-dimensional pivot analysis</strong></p><p><strong>Description:</strong> Creates multi-dimensional pivot-style analysis using CASE-based aggregation
<strong>Use Case:</strong> Business analytics for multi-dimensional pivot analysis</p><p><strong>Business Value:</strong> Actionable insights from multi-dimensional pivot analysis</p><p><strong>Purpose:</strong> Production multi-dimensional pivot analysis analysis</p><p><strong>Complexity:</strong> 4 CTEs, 4 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for multi-dimensional pivot analysis</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.employee_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.employee_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 330
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.employee_id) AS partition_stddev,
        NTILE(8) OVER (PARTITION BY c2.employee_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.employee_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.sale_time) AS period,
    c4.employee_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.sale_time), c4.employee_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-29-sales-funnel-stage-progression-query-29-">Query 29: Sales Funnel Stage Progression {#query-29}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for sales funnel stage progression</strong></p><p><strong>Description:</strong> Tracks progression through sales funnel stages using state transition analysis
<strong>Use Case:</strong> Business analytics for sales funnel stage progression</p><p><strong>Business Value:</strong> Actionable insights from sales funnel stage progression</p><p><strong>Purpose:</strong> Production sales funnel stage progression analysis</p><p><strong>Complexity:</strong> 4 CTEs, 5 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for sales funnel stage progression</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.customer_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.customer_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 340
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.customer_id) AS partition_stddev,
        NTILE(9) OVER (PARTITION BY c2.customer_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.customer_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.sale_time) AS period,
    c4.customer_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.sale_time), c4.customer_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-30-outlier-detection-with-iqr-method-query-30-">Query 30: Outlier Detection with IQR Method {#query-30}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for outlier detection with iqr method</strong></p><p><strong>Description:</strong> Detects outliers using interquartile range method across multiple dimensions
<strong>Use Case:</strong> Business analytics for outlier detection with iqr method</p><p><strong>Business Value:</strong> Actionable insights from outlier detection with iqr method</p><p><strong>Purpose:</strong> Production outlier detection with iqr method analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic
<strong>Expected Output:</strong> Aggregated metrics for outlier detection with iqr method</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY sale_id ORDER BY sale_time DESC) AS rn,
        DATE_TRUNC('day', sale_time) AS sale_day,
        DATE_TRUNC('week', sale_time) AS sale_week,
        EXTRACT(HOUR FROM sale_time) AS sale_hour,
        EXTRACT(DOW FROM sale_time) AS sale_dow
    FROM phppos_sales
    WHERE sale_time >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.sale_day, c1.sale_id) AS daily_partition_count,
        AVG(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time) AS first_value,
        LAST_VALUE(c1.sale_id) OVER (PARTITION BY c1.sale_id ORDER BY c1.sale_time ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_value
    FROM cte_level_1 c1
    WHERE c1.rn <= 350
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS prev_value,
        LEAD(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS next_value,
        c2.sale_id - LAG(c2.sale_id, 1) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_time) AS delta_value,
        AVG(c2.sale_id) OVER (PARTITION BY c2.sale_id) AS partition_avg,
        STDDEV(c2.sale_id) OVER (PARTITION BY c2.sale_id) AS partition_stddev,
        NTILE(4) OVER (PARTITION BY c2.sale_id ORDER BY c2.sale_id) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.sale_day ORDER BY c2.sale_id DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.sale_id - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.sale_id ORDER BY c3.sale_id) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.sale_time) AS period,
    c4.sale_id,
    COUNT(*) AS record_count,
    AVG(c4.sale_id) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.sale_id) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.sale_id) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.sale_id) AS q3_value,
    STDDEV(c4.sale_id) AS stddev_value,
    MIN(c4.sale_id) AS min_value,
    MAX(c4.sale_id) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.sale_time), c4.sale_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="usage-instructions">Usage Instructions</h2></p><p>Load schema.sql and data.sql. See docs/POSTGRES_MIGRATION.md for MySQL to PostgreSQL migration.</p><p>---</p><p><h2 id="platform-compatibility">Platform Compatibility</h2></p><p>All queries in this database are designed to work across multiple database platforms:</p><p>- <strong>PostgreSQL</strong>: Full support with standard SQL features
- <strong>Databricks</strong>: Compatible with Delta Lake and Spark SQL
- <strong>Database</strong>: db-5
- <strong>Type</strong>: POS Retail (Lucasa)
- <strong>Queries</strong>: 30 production queries
- <strong>Status</strong>: âœ… Complete Comprehensive Deliverable
</p>
    </div>
    <script>if (typeof Prism !== 'undefined') Prism.highlightAll();</script>
    <script>mermaid.initialize({ startOnLoad: true });</script>
</body>
</html>