{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL/ELT Pipeline - db-16\n",
    "\n",
    "Flood Risk Assessment Database ETL pipeline for FEMA, NOAA, USGS, and NASA data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('etl_pipeline')\n",
    "\n",
    "# Configuration\n",
    "DB_NUM = 16\n",
    "DB_NAME = 'db16'\n",
    "BASE_DIR = Path('.').parent\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "print(f'ETL Pipeline for db-{DB_NUM}')\n",
    "print(f'Base directory: {BASE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Phase\n",
    "\n",
    "### FEMA National Flood Hazard Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMA Flood Map API\n",
    "# Documentation: https://www.fema.gov/flood-maps/tools-resources/flood-map-products/national-flood-hazard-layer\n",
    "FEMA_API_BASE = 'https://hazards.fema.gov/gis/nfhl/rest/services'\n",
    "\n",
    "def extract_fema_flood_zones(state_fips, max_records=1000):\n",
    "    \"\"\"Extract FEMA flood zone data for a state.\"\"\"\n",
    "    url = f'{FEMA_API_BASE}/public/NFHL/MapServer/28/query'\n",
    "    params = {\n",
    "        'where': f\"DFIRM_ID LIKE '{state_fips}%'\",\n",
    "        'outFields': '*',\n",
    "        'f': 'json',\n",
    "        'resultRecordCount': max_records\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            features = data.get('features', [])\n",
    "            logger.info(f'FEMA: Extracted {len(features)} features for {state_fips}')\n",
    "            return features\n",
    "    except Exception as e:\n",
    "        logger.error(f'FEMA extraction error: {e}')\n",
    "    return []\n",
    "\n",
    "print('FEMA extraction function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOAA Sea Level Rise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA CO-OPS API\n",
    "# Documentation: https://api.tidesandcurrents.noaa.gov/api/prod/\n",
    "NOAA_API_BASE = 'https://api.tidesandcurrents.noaa.gov/dpapi/prod/webapi'\n",
    "\n",
    "def extract_noaa_sea_level_rise(station_id):\n",
    "    \"\"\"Extract NOAA sea level rise projections for a station.\"\"\"\n",
    "    url = f'{NOAA_API_BASE}/product/sltrends.json'\n",
    "    params = {'station': station_id}\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            logger.info(f'NOAA: Extracted SLR data for station {station_id}')\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        logger.error(f'NOAA extraction error: {e}')\n",
    "    return None\n",
    "\n",
    "print('NOAA extraction function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGS Streamflow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USGS Water Services API\n",
    "# Documentation: https://waterservices.usgs.gov/\n",
    "USGS_API_BASE = 'https://waterservices.usgs.gov/nwis'\n",
    "\n",
    "def extract_usgs_streamflow(state_code, period='P30D'):\n",
    "    \"\"\"Extract USGS streamflow data for a state.\"\"\"\n",
    "    url = f'{USGS_API_BASE}/iv/'\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'stateCd': state_code,\n",
    "        'parameterCd': '00060,00065',  # Discharge and gage height\n",
    "        'period': period,\n",
    "        'siteType': 'ST',  # Stream sites\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=60)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            ts = data.get('value', {}).get('timeSeries', [])\n",
    "            logger.info(f'USGS: Extracted {len(ts)} time series for {state_code}')\n",
    "            return ts\n",
    "    except Exception as e:\n",
    "        logger.error(f'USGS extraction error: {e}')\n",
    "    return []\n",
    "\n",
    "print('USGS extraction function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Flood Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASA Global Flood Monitoring System\n",
    "# Documentation: https://flood.umd.edu/\n",
    "NASA_GFMS_BASE = 'https://flood.umd.edu'\n",
    "\n",
    "def extract_nasa_flood_data(date_str):\n",
    "    \"\"\"Extract NASA GFMS flood data for a date.\"\"\"\n",
    "    logger.info(f'NASA: Extracting flood data for {date_str}')\n",
    "    # NASA GFMS provides binary data files\n",
    "    # Actual extraction would download .bin or .tif files\n",
    "    return None\n",
    "\n",
    "print('NASA extraction function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform functions for each data source\n",
    "\n",
    "def transform_fema_data(features):\n",
    "    \"\"\"Transform FEMA features into DataFrame.\"\"\"\n",
    "    records = []\n",
    "    for f in features:\n",
    "        attrs = f.get('attributes', {})\n",
    "        records.append({\n",
    "            'zone_code': attrs.get('FLD_ZONE', ''),\n",
    "            'zone_description': attrs.get('ZONE_SUBTY', ''),\n",
    "            'base_flood_elevation': attrs.get('STATIC_BFE'),\n",
    "            'state_code': attrs.get('DFIRM_ID', '')[:2],\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "print('Transform functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions (requires psycopg2 and database connection)\n",
    "\n",
    "def load_to_postgresql(df, table_name, conn_params):\n",
    "    \"\"\"Load DataFrame into PostgreSQL table.\"\"\"\n",
    "    try:\n",
    "        import psycopg2\n",
    "        from psycopg2.extras import execute_values\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        cur = conn.cursor()\n",
    "        cols = ', '.join(df.columns)\n",
    "        vals = [tuple(row) for _, row in df.iterrows()]\n",
    "        sql = f'INSERT INTO {table_name} ({cols}) VALUES %s ON CONFLICT DO NOTHING'\n",
    "        execute_values(cur, sql, vals)\n",
    "        conn.commit()\n",
    "        logger.info(f'Loaded {len(vals)} rows into {table_name}')\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f'Load error: {e}')\n",
    "\n",
    "print('Load functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full ETL pipeline\n",
    "print('ETL Pipeline ready')\n",
    "print('To run extraction, call the extract_* functions with appropriate parameters')\n",
    "print('To transform, call transform_* functions')\n",
    "print('To load, call load_to_postgresql with connection parameters')"
   ]
  }
 ]
}