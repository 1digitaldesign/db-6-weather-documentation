{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL/ELT Pipeline - DB-11 Parking Intelligence Database\n",
    "\n",
    "This notebook provides a comprehensive ETL/ELT pipeline for the Parking Intelligence Database (db-11).\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Extract**: Load data from Data.gov, Census Bureau, BTS TranStats, FHWA, and city open data portals\n",
    "2. **Transform**: Clean, validate, and transform parking intelligence data\n",
    "3. **Load**: Load transformed data into PostgreSQL\n",
    "4. **Validate**: Verify data quality and completeness\n",
    "5. **Monitor**: Track pipeline performance and errors\n",
    "\n",
    "## Data Sources\n",
    "- **Data.gov CKAN API**: Parking facility datasets from various cities\n",
    "- **Census Bureau API**: Demographics and population data for metropolitan areas\n",
    "- **BTS TranStats**: Airport passenger volumes and statistics\n",
    "- **FHWA**: Traffic volume data and highway statistics\n",
    "- **City Open Data Portals**: Real-time parking utilization and pricing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# API and HTTP requests\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Geospatial libraries\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point, Polygon\n",
    "    GEOSPATIAL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEOSPATIAL_AVAILABLE = False\n",
    "    print(\"Warning: geopandas/shapely not available\")\n",
    "\n",
    "# Database connections\n",
    "try:\n",
    "    from sqlalchemy import create_engine, text\n",
    "    SQLALCHEMY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SQLALCHEMY_AVAILABLE = False\n",
    "    print(\"Warning: sqlalchemy not available\")\n",
    "\n",
    "try:\n",
    "    import psycopg2\n",
    "    from psycopg2.extras import RealDictCursor\n",
    "    PG_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PG_AVAILABLE = False\n",
    "    print(\"Warning: psycopg2 not available\")\n",
    "\n",
    "try:\n",
    "    from databricks import sql\n",
    "    DATABRICKS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DATABRICKS_AVAILABLE = False\n",
    "    print(\"Warning: databricks-sql-connector not available\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DB_NAME = \"db-11\"\n",
    "DB_PATH = Path.cwd().parent\n",
    "\n",
    "# Database connection strings (configure as needed)\n",
    "# PostgreSQL\n",
    "POSTGRES_CONNECTION_STRING = None  # \"postgresql://user:password@localhost:5432/db_11_validation\"\n",
    "\n",
    "# Databricks\n",
    "DATABRICKS_CONFIG = {\n",
    "    'server_hostname': None,  # Set via environment variable DATABRICKS_SERVER_HOSTNAME\n",
    "    'http_path': None,  # Set via environment variable DATABRICKS_HTTP_PATH\n",
    "    'access_token': None  # Set via environment variable DATABRICKS_TOKEN\n",
    "}\n",
    "\n",
    "# Source data paths\n",
    "DATA_DIR = DB_PATH / \"data\"\n",
    "SCHEMA_FILE = DATA_DIR / \"schema.sql\"\n",
    "DATA_FILE = DATA_DIR / \"data.sql\"\n",
    "RESEARCH_DIR = DB_PATH / \"research\"\n",
    "\n",
    "# API Configuration\n",
    "# Data.gov CKAN API - optional API key for higher rate limits\n",
    "DATA_GOV_API_KEY = None  # Set via environment variable DATA_GOV_API_KEY\n",
    "DATA_GOV_CKAN_BASE_URL = \"https://catalog.data.gov/api/3/action\"\n",
    "\n",
    "# Census Bureau API - optional API key recommended\n",
    "CENSUS_API_KEY = None  # Set via environment variable CENSUS_API_KEY\n",
    "CENSUS_BASE_URL = \"https://api.census.gov/data\"\n",
    "\n",
    "# Target data size: 1 GB\n",
    "TARGET_DATA_SIZE_GB_MIN = 1.0\n",
    "TARGET_DATA_SIZE_GB_MAX = 30.0\n",
    "\n",
    "# Geographic coverage: 400+ cities\n",
    "TARGET_CITIES_COUNT = 400\n",
    "\n",
    "print(f\"Database: {DB_NAME}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Schema file exists: {SCHEMA_FILE.exists()}\")\n",
    "print(f\"Target data size: {TARGET_DATA_SIZE_GB} GB\")\n",
    "print(f\"Target cities: {TARGET_CITIES_COUNT}+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Extract Phase\n",
    "\n",
    "### 2.1 Data.gov CKAN API Integration - Parking Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session_with_retry() -> requests.Session:\n",
    "    \"\"\"Create requests session with retry strategy\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def search_data_gov_datasets(query: str = \"parking\", limit: int = 20, api_key: Optional[str] = None) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Search for parking datasets in Data.gov via CKAN API.\n",
    "\n",
    "    API Documentation: https://catalog.data.gov/api/3/action/package_search\n",
    "    \"\"\"\n",
    "    session = create_session_with_retry()\n",
    "\n",
    "    url = f\"{DATA_GOV_CKAN_BASE_URL}/package_search\"\n",
    "\n",
    "    headers = {}\n",
    "    if api_key:\n",
    "        headers[\"X-API-Key\"] = api_key\n",
    "\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"rows\": limit,\n",
    "        \"start\": 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get('success'):\n",
    "            results = data.get('result', {}).get('results', [])\n",
    "            logger.info(f\"Found {len(results)} parking datasets on Data.gov\")\n",
    "            return data\n",
    "        else:\n",
    "            logger.warning(f\"Data.gov API returned success=False\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching Data.gov data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Search for parking datasets\n",
    "parking_datasets = search_data_gov_datasets(query=\"parking\", limit=50, api_key=DATA_GOV_API_KEY)\n",
    "if parking_datasets:\n",
    "    results = parking_datasets.get('result', {}).get('results', [])\n",
    "    print(f\"✓ Found {len(results)} parking datasets on Data.gov\")\n",
    "    if results:\n",
    "        print(f\"Sample dataset: {results[0].get('title', 'N/A')}\")\n",
    "else:\n",
    "    print(\"⚠ Data.gov dataset search failed or returned no results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Census Bureau API Integration - Metropolitan Areas and Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_census_msa_data(year: int = 2023, api_key: Optional[str] = None) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch metropolitan statistical area (MSA) population and demographic data.\n",
    "\n",
    "    API Documentation: https://www.census.gov/data/developers/data-sets.html\n",
    "    \"\"\"\n",
    "    session = create_session_with_retry()\n",
    "\n",
    "    # ACS 5-year estimates for MSA data\n",
    "    url = f\"{CENSUS_BASE_URL}/{year}/acs/acs5\"\n",
    "\n",
    "    params = {\n",
    "        \"get\": \"B01001_001E,B19013_001E\",  # Total population, Median household income\n",
    "        \"for\": \"metropolitan statistical area/micropolitan statistical area:*\",\n",
    "        \"key\": api_key if api_key else None\n",
    "    }\n",
    "\n",
    "    # Remove None values\n",
    "    params = {k: v for k, v in params.items() if v is not None}\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        logger.info(f\"Fetched Census MSA data for year {year}\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching Census data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch Census MSA data\n",
    "current_year = datetime.now().year\n",
    "census_msa_data = fetch_census_msa_data(year=current_year - 1, api_key=CENSUS_API_KEY)\n",
    "if census_msa_data:\n",
    "    print(f\"✓ Fetched Census MSA data\")\n",
    "    if len(census_msa_data) > 1:\n",
    "        print(f\"Sample: {census_msa_data[1]}\")\n",
    "else:\n",
    "    print(\"⚠ Census MSA data fetch failed or skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 BTS TranStats - Airport Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bts_airport_data() -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch airport passenger data from BTS TranStats.\n",
    "    Note: BTS TranStats uses web interface, may require web scraping or CSV download.\n",
    "    \"\"\"\n",
    "    # BTS TranStats URL for airport data\n",
    "    bts_url = \"https://www.transtats.bts.gov/airports.asp\"\n",
    "\n",
    "    try:\n",
    "        # Note: This is a placeholder - actual implementation would require\n",
    "        # web scraping or CSV download from BTS TranStats\n",
    "        logger.info(\"BTS TranStats data extraction - requires web scraping or CSV download\")\n",
    "        print(\"⚠ BTS TranStats extraction requires web scraping or manual CSV download\")\n",
    "        print(f\"URL: {bts_url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching BTS data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Attempt BTS data extraction\n",
    "bts_airport_data = fetch_bts_airport_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 City Open Data Portals - Parking Facility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_city_parking_data(city: str, dataset_id: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch parking facility data from city open data portals.\n",
    "    Supports CKAN, Socrata, and ArcGIS APIs.\n",
    "    \"\"\"\n",
    "    city_portals = {\n",
    "        \"seattle\": {\n",
    "            \"base_url\": \"https://data.seattle.gov/api/views\",\n",
    "            \"api_type\": \"Socrata\",\n",
    "            \"dataset_id\": \"3k2p-39jp\"  # Public Garages and Parking Lots\n",
    "        },\n",
    "        \"san_francisco\": {\n",
    "            \"base_url\": \"https://data.sfgov.org/api/views\",\n",
    "            \"api_type\": \"Socrata\",\n",
    "            \"dataset_id\": \"wj8u-xu2y\"  # Parking Meters\n",
    "        },\n",
    "        \"austin\": {\n",
    "            \"base_url\": \"https://data.austintexas.gov/api/views\",\n",
    "            \"api_type\": \"Socrata\",\n",
    "            \"dataset_id\": \"7d8e-dm7r\"  # Off-Street Parking\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if city.lower() not in city_portals:\n",
    "        logger.warning(f\"City {city} not configured\")\n",
    "        return None\n",
    "\n",
    "    portal_config = city_portals[city.lower()]\n",
    "    session = create_session_with_retry()\n",
    "\n",
    "    # Socrata API endpoint\n",
    "    url = f\"{portal_config['base_url']}/{portal_config['dataset_id']}/rows.json\"\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        logger.info(f\"Fetched {len(df)} parking facilities from {city}\")\n",
    "        return df\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching {city} parking data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: Fetch Seattle parking data\n",
    "seattle_parking = fetch_city_parking_data(\"seattle\", \"3k2p-39jp\")\n",
    "if seattle_parking is not None:\n",
    "    print(f\"✓ Fetched {len(seattle_parking)} parking facilities from Seattle\")\n",
    "else:\n",
    "    print(\"⚠ Seattle parking data fetch failed or skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Transform Phase\n",
    "\n",
    "### 3.1 Data Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parking_facility_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and normalize parking facility data.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Standardize column names (example - adjust based on actual data)\n",
    "    # This is a placeholder - actual cleaning depends on source data format\n",
    "\n",
    "    # Handle missing values\n",
    "    # df = df.fillna(...)\n",
    "\n",
    "    # Validate coordinates\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        # Filter valid coordinates\n",
    "        df = df[\n",
    "            (df['latitude'] >= -90) & (df['latitude'] <= 90) &\n",
    "            (df['longitude'] >= -180) & (df['longitude'] <= 180)\n",
    "        ]\n",
    "\n",
    "    logger.info(f\"Cleaned parking facility data: {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "# Example: Clean Seattle parking data\n",
    "if seattle_parking is not None:\n",
    "    cleaned_seattle = clean_parking_facility_data(seattle_parking)\n",
    "    print(f\"✓ Cleaned {len(cleaned_seattle)} parking facilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Load Phase\n",
    "\n",
    "### 4.1 Load to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_postgresql(df: pd.DataFrame, table_name: str, connection_string: str) -> bool:\n",
    "    \"\"\"\n",
    "    Load DataFrame to PostgreSQL table.\n",
    "    \"\"\"\n",
    "    if not SQLALCHEMY_AVAILABLE or not connection_string:\n",
    "        logger.warning(\"PostgreSQL connection not configured\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "        logger.info(f\"Loaded {len(df)} records to PostgreSQL table {table_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading to PostgreSQL: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example: Load to PostgreSQL (requires connection string)\n",
    "if POSTGRES_CONNECTION_STRING and seattle_parking is not None:\n",
    "    load_to_postgresql(cleaned_seattle, \"parking_facilities\", POSTGRES_CONNECTION_STRING)\n",
    "else:\n",
    "    print(\"⚠ PostgreSQL loading skipped (connection not configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Validate Phase\n",
    "\n",
    "### 5.1 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_quality(df: pd.DataFrame, table_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate data quality metrics.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return {\"status\": \"empty\", \"records\": 0}\n",
    "\n",
    "    metrics = {\n",
    "        \"table_name\": table_name,\n",
    "        \"total_records\": len(df),\n",
    "        \"null_percentage\": (df.isnull().sum() / len(df) * 100).to_dict(),\n",
    "        \"duplicate_count\": df.duplicated().sum(),\n",
    "        \"data_types\": df.dtypes.to_dict()\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Data quality metrics for {table_name}: {metrics}\")\n",
    "    return metrics\n",
    "\n",
    "# Example: Validate Seattle parking data\n",
    "if seattle_parking is not None:\n",
    "    quality_metrics = validate_data_quality(cleaned_seattle, \"parking_facilities\")\n",
    "    print(f\"✓ Data quality validation complete: {quality_metrics['total_records']} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Monitor Phase\n",
    "\n",
    "### 6.1 Pipeline Execution Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track pipeline execution\n",
    "pipeline_metadata = {\n",
    "    \"execution_timestamp\": datetime.now().isoformat(),\n",
    "    \"database\": DB_NAME,\n",
    "    \"sources_extracted\": {\n",
    "        \"data_gov\": parking_datasets is not None,\n",
    "        \"census\": census_msa_data is not None,\n",
    "        \"bts\": bts_airport_data is not None,\n",
    "        \"city_portals\": seattle_parking is not None\n",
    "    },\n",
    "    \"records_extracted\": {\n",
    "        \"parking_facilities\": len(seattle_parking) if seattle_parking is not None else 0\n",
    "    },\n",
    "    \"data_quality_scores\": {}\n",
    "}\n",
    "\n",
    "# Save pipeline metadata\n",
    "metadata_file = RESEARCH_DIR / \"pipeline_execution_log.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(pipeline_metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Pipeline execution metadata saved to {metadata_file}\")\n",
    "print(f\"Pipeline Summary:\")\n",
    "print(json.dumps(pipeline_metadata, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
