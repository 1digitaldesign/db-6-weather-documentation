{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL/ELT Pipeline - DB-9 Shipping Intelligence Database\n",
        "\n",
        "This notebook provides a comprehensive ETL/ELT pipeline for the Shipping Intelligence Database (db-9).\n",
        "\n",
        "## Pipeline Overview\n",
        "1. **Extract**: Load data from USPS Developer Portal, UPS Developer Portal, U.S. Census Bureau International Trade Data, and Data.gov\n",
        "2. **Transform**: Clean, validate, and transform shipping data (rates, zones, tracking, customs)\n",
        "3. **Load**: Load transformed data into PostgreSQL and Databricks\n",
        "4. **Validate**: Verify data quality and completeness\n",
        "5. **Monitor**: Track pipeline performance and errors\n",
        "\n",
        "## Data Sources\n",
        "- **USPS Developer Portal**: Shipping rates, labels, address validation, tracking, adjustments\n",
        "- **UPS Developer Portal**: Shipping rates, tracking, zone charts\n",
        "- **U.S. Census Bureau International Trade Data**: Customs data, HTSUSA codes, country of origin\n",
        "- **Data.gov CKAN API**: ZIP code boundaries, postal service datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, List, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# API and HTTP requests\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# Database connections\n",
        "try:\n",
        "    from sqlalchemy import create_engine, text\n",
        "    SQLALCHEMY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SQLALCHEMY_AVAILABLE = False\n",
        "    print(\"Warning: sqlalchemy not available\")\n",
        "\n",
        "try:\n",
        "    import psycopg2\n",
        "    from psycopg2.extras import RealDictCursor\n",
        "    PG_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PG_AVAILABLE = False\n",
        "    print(\"Warning: psycopg2 not available\")\n",
        "\n",
        "try:\n",
        "    from databricks import sql\n",
        "    DATABRICKS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DATABRICKS_AVAILABLE = False\n",
        "    print(\"Warning: databricks-sql-connector not available\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"✓ Imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DB_NAME = \"db-9\"\n",
        "DB_PATH = Path.cwd().parent\n",
        "\n",
        "# Database connection strings (configure as needed)\n",
        "# PostgreSQL\n",
        "POSTGRES_CONNECTION_STRING = None  # \"postgresql://user:password@localhost:5432/shipping_intelligence_db\"\n",
        "\n",
        "# Databricks\n",
        "DATABRICKS_CONFIG = {\n",
        "    'server_hostname': None,  # Set via environment variable DATABRICKS_SERVER_HOSTNAME\n",
        "    'http_path': None,  # Set via environment variable DATABRICKS_HTTP_PATH\n",
        "    'access_token': None  # Set via environment variable DATABRICKS_TOKEN\n",
        "}\n",
        "\n",
        "# Source data paths\n",
        "DATA_DIR = DB_PATH / \"data\"\n",
        "SCHEMA_FILE = DATA_DIR / \"schema.sql\"\n",
        "DATA_FILE = DATA_DIR / \"data.sql\"\n",
        "RESEARCH_DIR = DB_PATH / \"research\"\n",
        "\n",
        "# API Configuration\n",
        "# USPS Developer Portal - requires OAuth 2.0 authentication\n",
        "USPS_API_KEY = None  # Set via environment variable USPS_API_KEY\n",
        "USPS_BASE_URL = \"https://developers.usps.com/api\"\n",
        "\n",
        "# UPS Developer Portal - requires OAuth 2.0 authentication\n",
        "UPS_API_KEY = None  # Set via environment variable UPS_API_KEY\n",
        "UPS_BASE_URL = \"https://developer.ups.com/api\"\n",
        "\n",
        "# U.S. Census Bureau International Trade Data - public access\n",
        "CENSUS_BASE_URL = \"https://www.census.gov/foreign-trade/data\"\n",
        "\n",
        "# Data.gov CKAN API - public access (API key optional for higher rate limits)\n",
        "DATA_GOV_API_KEY = None  # Set via environment variable DATA_GOV_API_KEY (optional)\n",
        "DATA_GOV_CKAN_BASE_URL = \"https://catalog.data.gov/api/3/action\"\n",
        "\n",
        "print(f\"Database: {DB_NAME}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Schema file exists: {SCHEMA_FILE.exists()}\")\n",
        "print(f\"Data file exists: {DATA_FILE.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Extract - API Integration and Data Loading\n",
        "\n",
        "### 2.1 USPS Developer Portal API Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_session_with_retry() -> requests.Session:\n",
        "    \"\"\"Create requests session with retry strategy\"\"\"\n",
        "    session = requests.Session()\n",
        "    retry_strategy = Retry(\n",
        "        total=3,\n",
        "        backoff_factor=1,\n",
        "        status_forcelist=[429, 500, 502, 503, 504]\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    session.mount(\"http://\", adapter)\n",
        "    session.mount(\"https://\", adapter)\n",
        "    return session\n",
        "\n",
        "def fetch_usps_domestic_prices(api_key: str, package_data: Dict) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Fetch domestic shipping prices from USPS Developer Portal.\n",
        "\n",
        "    API Documentation: https://developers.usps.com/api-documentation\n",
        "    Requires OAuth 2.0 authentication.\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        logger.warning(\"USPS API key not configured. Set USPS_API_KEY environment variable.\")\n",
        "        return None\n",
        "\n",
        "    session = create_session_with_retry()\n",
        "\n",
        "    url = f\"{USPS_BASE_URL}/prices/v1/domestic\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = session.post(url, headers=headers, json=package_data, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        logger.info(f\"Fetched USPS domestic prices\")\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error fetching USPS prices: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_usps_address_validation(api_key: str, address_data: Dict) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Validate and standardize addresses using USPS Addresses API.\n",
        "\n",
        "    API Documentation: https://developers.usps.com/api-documentation/addresses\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        logger.warning(\"USPS API key not configured.\")\n",
        "        return None\n",
        "\n",
        "    session = create_session_with_retry()\n",
        "\n",
        "    url = f\"{USPS_BASE_URL}/addresses/v1\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = session.post(url, headers=headers, json=address_data, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        logger.info(f\"Validated address via USPS API\")\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error validating address: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_usps_tracking(api_key: str, tracking_number: str) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Fetch tracking information from USPS Tracking API.\n",
        "\n",
        "    API Documentation: https://developers.usps.com/api-documentation/tracking\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        logger.warning(\"USPS API key not configured.\")\n",
        "        return None\n",
        "\n",
        "    session = create_session_with_retry()\n",
        "\n",
        "    url = f\"{USPS_BASE_URL}/tracking/v1\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    params = {\"trackingNumber\": tracking_number}\n",
        "\n",
        "    try:\n",
        "        response = session.get(url, headers=headers, params=params, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        logger.info(f\"Fetched tracking data for {tracking_number}\")\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error fetching tracking data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example USPS API calls (requires API key)\n",
        "if USPS_API_KEY:\n",
        "    # Example package data for rate request\n",
        "    example_package = {\n",
        "        \"from\": {\"zip\": \"10001\"},\n",
        "        \"to\": {\"zip\": \"90210\"},\n",
        "        \"weight\": {\"pounds\": 1, \"ounces\": 0},\n",
        "        \"service\": \"PRIORITY\"\n",
        "    }\n",
        "    usps_prices = fetch_usps_domestic_prices(USPS_API_KEY, example_package)\n",
        "    if usps_prices:\n",
        "        print(\"✓ Fetched USPS domestic prices\")\n",
        "    else:\n",
        "        print(\"⚠ USPS price fetch failed\")\n",
        "else:\n",
        "    print(\"⚠ USPS API key not configured. Skipping USPS extraction.\")\n",
        "    usps_prices = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 UPS Developer Portal API Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_ups_rates(api_key: str, rate_request: Dict) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Fetch shipping rates from UPS Rating API.\n",
        "\n",
        "    API Documentation: https://developer.ups.com/api/reference\n",
        "    Requires OAuth 2.0 authentication.\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        logger.warning(\"UPS API key not configured. Set UPS_API_KEY environment variable.\")\n",
        "        return None\n",
        "\n",
        "    session = create_session_with_retry()\n",
        "\n",
        "    url = f\"{UPS_BASE_URL}/rating/v1/Rate\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = session.post(url, headers=headers, json=rate_request, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        logger.info(f\"Fetched UPS rates\")\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error fetching UPS rates: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_ups_tracking(api_key: str, tracking_number: str) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Fetch tracking information from UPS Tracking API.\n",
        "\n",
        "    API Documentation: https://developer.ups.com/api/reference\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        logger.warning(\"UPS API key not configured.\")\n",
        "        return None\n",
        "\n",
        "    session = create_session_with_retry()\n",
        "\n",
        "    url = f\"{UPS_BASE_URL}/tracking/v1/Track\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\"TrackingNumber\": tracking_number}\n",
        "\n",
        "    try:\n",
        "        response = session.post(url, headers=headers, json=payload, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        logger.info(f\"Fetched tracking data for {tracking_number}\")\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error fetching UPS tracking data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example UPS API calls (requires API key)\n",
        "if UPS_API_KEY:\n",
        "    # Example rate request\n",
        "    example_rate_request = {\n",
        "        \"RateRequest\": {\n",
        "            \"Request\": {\"RequestOption\": \"Rate\"},\n",
        "            \"Shipment\": {\n",
        "                \"Shipper\": {\"Address\": {\"PostalCode\": \"10001\"}},\n",
        "                \"ShipTo\": {\"Address\": {\"PostalCode\": \"90210\"}},\n",
        "                \"ShipFrom\": {\"Address\": {\"PostalCode\": \"10001\"}},\n",
        "                \"Package\": {\"Weight\": \"1\"}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    ups_rates = fetch_ups_rates(UPS_API_KEY, example_rate_request)\n",
        "    if ups_rates:\n",
        "        print(\"✓ Fetched UPS rates\")\n",
        "    else:\n",
        "        print(\"⚠ UPS rate fetch failed\")\n",
        "else:\n",
        "    print(\"⚠ UPS API key not configured. Skipping UPS extraction.\")\n",
        "    ups_rates = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 U.S. Census Bureau International Trade Data Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_census_customs_data(year: int, month: int = None) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Fetch international trade/customs data from U.S. Census Bureau.\n",
        "\n",
        "    Data sources:\n",
        "    - SPI Databank: https://www.census.gov/foreign-trade/data/SPIM.html\n",
        "    - USA Trade Online: Query tool for trade data\n",
        "\n",
        "    Note: This is a placeholder for bulk file download or API integration.\n",
        "    In production, would download CSV/Excel files or use Census API.\n",
        "    \"\"\"\n",
        "    session = create_session_with_retry()\n",
        "\n",
        "    # Placeholder for Census Bureau data extraction\n",
        "    # In production, would download SPI Databank files or query USA Trade Online\n",
        "    logger.info(f\"Fetching Census Bureau customs data for year {year}\")\n",
        "\n",
        "    # Example: Would download CSV/Excel files from Census Bureau\n",
        "    # For now, return None as placeholder\n",
        "    return None\n",
        "\n",
        "# Fetch Census Bureau customs data (placeholder)\n",
        "census_customs_data = fetch_census_customs_data(datetime.now().year)\n",
        "if census_customs_data:\n",
        "    print(\"✓ Fetched Census Bureau customs data\")\n",
        "else:\n",
        "    print(\"⚠ Census Bureau data extraction not implemented (requires file download)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Data.gov CKAN API Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_datagov_datasets(query: str = \"usps shipping\", limit: int = 20) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Search for shipping-related datasets in Data.gov via CKAN API.\n",
        "\n",
        "    API Documentation: https://catalog.data.gov/api/3/action/package_search\n",
        "    \"\"\"\n",
        "    session = create_session_with_retry()\n",
        "\n",
        "    url = f\"{DATA_GOV_CKAN_BASE_URL}/package_search\"\n",
        "\n",
        "    headers = {}\n",
        "    if DATA_GOV_API_KEY:\n",
        "        headers[\"X-API-Key\"] = DATA_GOV_API_KEY\n",
        "\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"rows\": limit\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = session.get(url, headers=headers, params=params, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if data.get('success'):\n",
        "            count = data.get('result', {}).get('count', 0)\n",
        "            datasets = data.get('result', {}).get('results', [])\n",
        "            logger.info(f\"Found {count} Data.gov datasets matching '{query}'\")\n",
        "            return {'count': count, 'datasets': datasets}\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error searching Data.gov datasets: {e}\")\n",
        "        return None\n",
        "\n",
        "# Search for shipping-related datasets\n",
        "datagov_datasets = search_datagov_datasets(query=\"usps shipping postal\", limit=10)\n",
        "if datagov_datasets:\n",
        "    print(f\"✓ Found {datagov_datasets['count']} Data.gov datasets\")\n",
        "    print(f\"  Sample datasets: {', '.join([d['title'][:50] for d in datagov_datasets['datasets'][:3]])}\")\n",
        "else:\n",
        "    print(\"⚠ Data.gov dataset search failed or skipped\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Load Local Data Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_schema_file(schema_path: Path) -> Optional[str]:\n",
        "    \"\"\"Load database schema from SQL file.\"\"\"\n",
        "    try:\n",
        "        if schema_path.exists():\n",
        "            with open(schema_path, 'r') as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            logger.warning(f\"Schema file not found: {schema_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading schema: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_data_file(data_path: Path) -> Optional[str]:\n",
        "    \"\"\"Load data from SQL file.\"\"\"\n",
        "    try:\n",
        "        if data_path.exists():\n",
        "            with open(data_path, 'r') as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            logger.warning(f\"Data file not found: {data_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_from_csv(csv_path: Path) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Extract data from CSV file.\"\"\"\n",
        "    try:\n",
        "        if csv_path.exists():\n",
        "            df = pd.read_csv(csv_path)\n",
        "            logger.info(f\"Loaded {len(df)} rows from {csv_path.name}\")\n",
        "            return df\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading CSV {csv_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_from_json(json_path: Path) -> Optional[Dict]:\n",
        "    \"\"\"Extract data from JSON file.\"\"\"\n",
        "    try:\n",
        "        if json_path.exists():\n",
        "            with open(json_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            logger.info(f\"Loaded JSON from {json_path.name}\")\n",
        "            return data\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading JSON {json_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load schema and data\n",
        "schema_sql = load_schema_file(SCHEMA_FILE)\n",
        "data_sql = load_data_file(DATA_FILE)\n",
        "\n",
        "# Find and load data files\n",
        "csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
        "json_files = list(DATA_DIR.glob(\"*.json\"))\n",
        "\n",
        "extracted_data = {}\n",
        "\n",
        "# Add API data to extracted_data\n",
        "if usps_prices:\n",
        "    extracted_data['usps_prices'] = usps_prices\n",
        "if ups_rates:\n",
        "    extracted_data['ups_rates'] = ups_rates\n",
        "if census_customs_data:\n",
        "    extracted_data['census_customs'] = census_customs_data\n",
        "if datagov_datasets:\n",
        "    extracted_data['datagov_datasets'] = datagov_datasets\n",
        "\n",
        "# Load local files\n",
        "for csv_file in csv_files:\n",
        "    df = extract_from_csv(csv_file)\n",
        "    if df is not None:\n",
        "        extracted_data[csv_file.stem] = df\n",
        "\n",
        "for json_file in json_files:\n",
        "    data = extract_from_json(json_file)\n",
        "    if data is not None:\n",
        "        extracted_data[json_file.stem] = data\n",
        "\n",
        "if schema_sql:\n",
        "    print(f\"✓ Schema loaded ({len(schema_sql)} characters)\")\n",
        "if data_sql:\n",
        "    print(f\"✓ Data loaded ({len(data_sql)} characters)\")\n",
        "print(f\"✓ Extracted {len(extracted_data)} data sources\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Transform - Data Cleaning and Transformation\n",
        "\n",
        "### 3.1 Transform USPS Rate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_usps_rates_to_dataframe(usps_data: Dict) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Transform USPS API rate response to DataFrame matching shipping_rates table schema.\"\"\"\n",
        "    if not usps_data:\n",
        "        return None\n",
        "\n",
        "    rates = []\n",
        "\n",
        "    # Parse USPS API response structure (adjust based on actual API response)\n",
        "    # This is a placeholder transformation - adjust based on actual USPS API response format\n",
        "    if isinstance(usps_data, dict):\n",
        "        # Example transformation - adjust based on actual API structure\n",
        "        rate_entry = {\n",
        "            'rate_id': f\"usps_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
        "            'carrier_id': 'carrier_usps',\n",
        "            'service_id': usps_data.get('service', 'PRIORITY'),\n",
        "            'zone_id': None,  # Would extract from API response\n",
        "            'weight_lbs': usps_data.get('weight', {}).get('pounds', 0),\n",
        "            'weight_oz': usps_data.get('weight', {}).get('ounces', 0),\n",
        "            'rate_amount': float(usps_data.get('price', 0)),\n",
        "            'effective_date': datetime.now().isoformat(),\n",
        "            'expiration_date': (datetime.now() + timedelta(days=365)).isoformat(),\n",
        "            'created_at': datetime.now().isoformat()\n",
        "        }\n",
        "        rates.append(rate_entry)\n",
        "\n",
        "    if rates:\n",
        "        df = pd.DataFrame(rates)\n",
        "        logger.info(f\"Transformed {len(df)} USPS rates to DataFrame\")\n",
        "        return df\n",
        "    return None\n",
        "\n",
        "# Transform USPS data\n",
        "if usps_prices:\n",
        "    usps_rates_df = transform_usps_rates_to_dataframe(usps_prices)\n",
        "    if usps_rates_df is not None:\n",
        "        print(f\"✓ Transformed {len(usps_rates_df)} USPS rates\")\n",
        "        print(f\"  Columns: {', '.join(usps_rates_df.columns)}\")\n",
        "    else:\n",
        "        print(\"⚠ USPS transformation returned no data\")\n",
        "        usps_rates_df = None\n",
        "else:\n",
        "    usps_rates_df = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Transform UPS Rate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_ups_rates_to_dataframe(ups_data: Dict) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Transform UPS API rate response to DataFrame matching shipping_rates table schema.\"\"\"\n",
        "    if not ups_data:\n",
        "        return None\n",
        "\n",
        "    rates = []\n",
        "\n",
        "    # Parse UPS API response structure (adjust based on actual API response)\n",
        "    # This is a placeholder transformation - adjust based on actual UPS API response format\n",
        "    if isinstance(ups_data, dict):\n",
        "        # Example transformation - adjust based on actual API structure\n",
        "        rate_entry = {\n",
        "            'rate_id': f\"ups_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
        "            'carrier_id': 'carrier_ups',\n",
        "            'service_id': ups_data.get('service', 'GROUND'),\n",
        "            'zone_id': None,  # Would extract from API response\n",
        "            'weight_lbs': ups_data.get('weight', 0),\n",
        "            'weight_oz': 0,\n",
        "            'rate_amount': float(ups_data.get('totalCharges', 0)),\n",
        "            'effective_date': datetime.now().isoformat(),\n",
        "            'expiration_date': (datetime.now() + timedelta(days=365)).isoformat(),\n",
        "            'created_at': datetime.now().isoformat()\n",
        "        }\n",
        "        rates.append(rate_entry)\n",
        "\n",
        "    if rates:\n",
        "        df = pd.DataFrame(rates)\n",
        "        logger.info(f\"Transformed {len(df)} UPS rates to DataFrame\")\n",
        "        return df\n",
        "    return None\n",
        "\n",
        "# Transform UPS data\n",
        "if ups_rates:\n",
        "    ups_rates_df = transform_ups_rates_to_dataframe(ups_rates)\n",
        "    if ups_rates_df is not None:\n",
        "        print(f\"✓ Transformed {len(ups_rates_df)} UPS rates\")\n",
        "        print(f\"  Columns: {', '.join(ups_rates_df.columns)}\")\n",
        "    else:\n",
        "        print(\"⚠ UPS transformation returned no data\")\n",
        "        ups_rates_df = None\n",
        "else:\n",
        "    ups_rates_df = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Data Cleaning Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean DataFrame: handle missing values, remove duplicates, etc.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    # Remove duplicates\n",
        "    initial_rows = len(df)\n",
        "    df = df.drop_duplicates()\n",
        "    duplicates_removed = initial_rows - len(df)\n",
        "\n",
        "    # Handle missing values\n",
        "    missing_before = df.isnull().sum().sum()\n",
        "    # Fill numeric columns with median\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "    # Fill text columns with empty string\n",
        "    text_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in text_cols:\n",
        "        df[col] = df[col].fillna('')\n",
        "    missing_after = df.isnull().sum().sum()\n",
        "\n",
        "    logger.info(f\"Cleaned data: removed {duplicates_removed} duplicates, filled {missing_before - missing_after} missing values\")\n",
        "    return df\n",
        "\n",
        "# Clean transformed dataframes\n",
        "cleaned_data = {}\n",
        "if usps_rates_df is not None:\n",
        "    cleaned_data['shipping_rates_usps'] = clean_dataframe(usps_rates_df.copy())\n",
        "if ups_rates_df is not None:\n",
        "    cleaned_data['shipping_rates_ups'] = clean_dataframe(ups_rates_df.copy())\n",
        "\n",
        "# Clean other extracted dataframes\n",
        "for name, data in extracted_data.items():\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        cleaned_data[name] = clean_dataframe(data.copy())\n",
        "\n",
        "print(f\"✓ Cleaned {len([d for d in cleaned_data.values() if isinstance(d, pd.DataFrame)])} dataframes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Data Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_dataframe(df: pd.DataFrame, required_columns: List[str] = None) -> Dict:\n",
        "    \"\"\"Validate DataFrame structure and data quality.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return {'valid': False, 'errors': ['DataFrame is empty or None']}\n",
        "\n",
        "    validation_results = {\n",
        "        'valid': True,\n",
        "        'row_count': len(df),\n",
        "        'column_count': len(df.columns),\n",
        "        'missing_values': df.isnull().sum().to_dict(),\n",
        "        'duplicate_rows': df.duplicated().sum(),\n",
        "        'errors': []\n",
        "    }\n",
        "\n",
        "    # Check required columns\n",
        "    if required_columns:\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            validation_results['valid'] = False\n",
        "            validation_results['errors'].append(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "    return validation_results\n",
        "\n",
        "# Validate cleaned data\n",
        "validation_results = {}\n",
        "for name, data in cleaned_data.items():\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        validation_results[name] = validate_dataframe(data)\n",
        "\n",
        "# Display validation results\n",
        "for name, results in validation_results.items():\n",
        "    status = \"✓\" if results['valid'] else \"✗\"\n",
        "    print(f\"{status} {name}: {results['row_count']} rows, {results['column_count']} columns\")\n",
        "    if results['errors']:\n",
        "        for error in results['errors']:\n",
        "            print(f\"  Error: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Load - Data Loading to Target Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_to_postgresql(df: pd.DataFrame, table_name: str, connection_string: str, if_exists: str = 'append') -> bool:\n",
        "    \"\"\"Load DataFrame to PostgreSQL table.\"\"\"\n",
        "    if not SQLALCHEMY_AVAILABLE or connection_string is None:\n",
        "        logger.warning(\"PostgreSQL connection not available\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        engine = create_engine(connection_string)\n",
        "        df.to_sql(table_name, engine, if_exists=if_exists, index=False, method='multi', chunksize=1000)\n",
        "        logger.info(f\"Loaded {len(df)} rows to PostgreSQL table {table_name}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading to PostgreSQL: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_to_databricks(df: pd.DataFrame, table_name: str, config: Dict) -> bool:\n",
        "    \"\"\"Load DataFrame to Databricks table.\"\"\"\n",
        "    if not DATABRICKS_AVAILABLE or not all([config.get('server_hostname'), config.get('http_path'), config.get('access_token')]):\n",
        "        logger.warning(\"Databricks connection not available\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        conn = sql.connect(\n",
        "            server_hostname=config['server_hostname'],\n",
        "            http_path=config['http_path'],\n",
        "            access_token=config['access_token']\n",
        "        )\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Convert DataFrame to INSERT statements (simplified - in production, use Spark DataFrame)\n",
        "        # For now, log that data is ready for loading\n",
        "        logger.info(f\"Prepared {len(df)} rows for Databricks table {table_name}\")\n",
        "\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading to Databricks: {e}\")\n",
        "        return False\n",
        "\n",
        "# Load data to target databases\n",
        "load_results = {}\n",
        "\n",
        "for name, data in cleaned_data.items():\n",
        "    if isinstance(data, pd.DataFrame) and not data.empty:\n",
        "        table_name = name.lower().replace(' ', '_')\n",
        "\n",
        "        # PostgreSQL\n",
        "        if POSTGRES_CONNECTION_STRING:\n",
        "            load_results[f\"{name}_postgres\"] = load_to_postgresql(\n",
        "                data, table_name, POSTGRES_CONNECTION_STRING\n",
        "            )\n",
        "\n",
        "        # Databricks\n",
        "        if all([DATABRICKS_CONFIG.get('server_hostname'), DATABRICKS_CONFIG.get('http_path'), DATABRICKS_CONFIG.get('access_token')]):\n",
        "            load_results[f\"{name}_databricks\"] = load_to_databricks(\n",
        "                data, table_name, DATABRICKS_CONFIG\n",
        "            )\n",
        "\n",
        "successful_loads = sum(load_results.values())\n",
        "print(f\"✓ Loaded {successful_loads} datasets to target databases\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Validate - Data Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data_quality_report(df: pd.DataFrame, table_name: str) -> Dict:\n",
        "    \"\"\"Generate comprehensive data quality report.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return {'table': table_name, 'status': 'empty'}\n",
        "\n",
        "    report = {\n",
        "        'table': table_name,\n",
        "        'row_count': len(df),\n",
        "        'column_count': len(df.columns),\n",
        "        'missing_values': int(df.isnull().sum().sum()),\n",
        "        'missing_percentage': float((df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100),\n",
        "        'duplicate_rows': int(df.duplicated().sum()),\n",
        "        'data_types': df.dtypes.astype(str).to_dict(),\n",
        "        'numeric_stats': {},\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Add statistics for numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        report['numeric_stats'] = df[numeric_cols].describe().to_dict()\n",
        "\n",
        "    return report\n",
        "\n",
        "# Generate quality reports\n",
        "quality_reports = {}\n",
        "for name, data in cleaned_data.items():\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        quality_reports[name] = generate_data_quality_report(data, name)\n",
        "\n",
        "# Display quality reports\n",
        "for name, report in quality_reports.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"Rows: {report['row_count']}\")\n",
        "    print(f\"Columns: {report['column_count']}\")\n",
        "    print(f\"Missing values: {report['missing_values']} ({report['missing_percentage']:.2f}%)\")\n",
        "    print(f\"Duplicate rows: {report['duplicate_rows']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Monitor - Pipeline Monitoring and Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save pipeline execution metadata\n",
        "pipeline_metadata = {\n",
        "    'database': DB_NAME,\n",
        "    'execution_timestamp': datetime.now().isoformat(),\n",
        "    'data_sources': list(extracted_data.keys()),\n",
        "    'extracted_count': len(extracted_data),\n",
        "    'cleaned_count': len(cleaned_data),\n",
        "    'validation_results': validation_results,\n",
        "    'load_results': load_results,\n",
        "    'quality_reports': quality_reports,\n",
        "    'status': 'completed'\n",
        "}\n",
        "\n",
        "# Save metadata to JSON\n",
        "metadata_file = DB_PATH / \"metadata\" / \"pipeline_metadata.json\"\n",
        "metadata_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(metadata_file, 'w') as f:\n",
        "    json.dump(pipeline_metadata, f, indent=2, default=str)\n",
        "\n",
        "print(f\"✓ Pipeline metadata saved to {metadata_file}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PIPELINE EXECUTION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Database: {pipeline_metadata['database']}\")\n",
        "print(f\"Execution time: {pipeline_metadata['execution_timestamp']}\")\n",
        "print(f\"Data sources extracted: {pipeline_metadata['extracted_count']}\")\n",
        "print(f\"Datasets cleaned: {pipeline_metadata['cleaned_count']}\")\n",
        "print(f\"Successful loads: {sum(pipeline_metadata['load_results'].values())}\")\n",
        "print(f\"Status: {pipeline_metadata['status']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2.5: Large Dataset Extraction (2-30 GB)\n",
        "\n",
        "### Extract Large Datasets from Internet Sources\n",
        "\n",
        "For bulk data extraction (2-30 GB), use the dedicated extraction script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Large Dataset Extraction\n",
        "# This section demonstrates how to extract 2-30 GB of shipping intelligence data\n",
        "# from Census Bureau, Data.gov, and other sources\n",
        "\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to extraction script\n",
        "extraction_script = DB_PATH / \"scripts\" / \"extract_large_datasets.py\"\n",
        "transformation_script = DB_PATH / \"scripts\" / \"transform_large_datasets.py\"\n",
        "\n",
        "# Check if scripts exist\n",
        "if extraction_script.exists():\n",
        "    print(f\"✓ Extraction script found: {extraction_script}\")\n",
        "    print(\"\\nTo extract large datasets (2-30 GB), run:\")\n",
        "    print(f\"  python3 {extraction_script}\")\n",
        "    print(\"\\nThis will download:\")\n",
        "    print(\"  - Census Bureau SPI Databank (5-15 GB)\")\n",
        "    print(\"  - ZIP Code Boundaries (500 MB - 2 GB)\")\n",
        "    print(\"  - Data.gov Shipping Datasets (1-5 GB)\")\n",
        "    print(\"  - Postal Service Datasets (500 MB - 2 GB)\")\n",
        "    print(\"\\nTotal: 2-30 GB of shipping intelligence data\")\n",
        "\n",
        "    # Optionally run extraction (commented out - takes hours)\n",
        "    # print(\"\\n⚠️  Running extraction (this may take hours)...\")\n",
        "    # result = subprocess.run([sys.executable, str(extraction_script)],\n",
        "    #                        capture_output=True, text=True)\n",
        "    # print(result.stdout)\n",
        "    # if result.returncode != 0:\n",
        "    #     print(f\"Error: {result.stderr}\")\n",
        "else:\n",
        "    print(f\"⚠️  Extraction script not found: {extraction_script}\")\n",
        "\n",
        "if transformation_script.exists():\n",
        "    print(f\"\\n✓ Transformation script found: {transformation_script}\")\n",
        "    print(\"\\nTo transform downloaded datasets, run:\")\n",
        "    print(f\"  python3 {transformation_script}\")\n",
        "    print(\"\\nThis will:\")\n",
        "    print(\"  - Transform Census SPI data → international_customs table\")\n",
        "    print(\"  - Transform ZIP boundaries → shipping_zones table\")\n",
        "    print(\"  - Transform postal data → address_validation_results table\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  Transformation script not found: {transformation_script}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL/ELT Pipeline - DB-1\n",
        "\n",
        "This notebook provides a comprehensive ETL/ELT pipeline for database db-1.\n",
        "\n",
        "## Pipeline Overview\n",
        "1. **Extract**: Load data from source systems\n",
        "2. **Transform**: Clean, validate, and transform data\n",
        "3. **Load**: Load transformed data into target database\n",
        "4. **Validate**: Verify data quality and completeness\n",
        "5. **Monitor**: Track pipeline performance and errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, List, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Database connections\n",
        "try:\n",
        "    from sqlalchemy import create_engine, text\n",
        "    SQLALCHEMY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SQLALCHEMY_AVAILABLE = False\n",
        "    print(\"Warning: sqlalchemy not available\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"✓ Imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DB_NAME = \"db-9\"\n",
        "DB_PATH = Path.cwd().parent\n",
        "\n",
        "# Database connection strings (configure as needed)\n",
        "# PostgreSQL\n",
        "POSTGRES_CONNECTION_STRING = None  # \"postgresql://user:password@localhost:5432/dbname\"\n",
        "\n",
        "# Databricks\n",
        "DATABRICKS_CONNECTION_STRING = None  # Configure Databricks connection\n",
        "\n",
        "# Snowflake\n",
        "SNOWFLAKE_CONNECTION_STRING = None  # Configure Snowflake connection\n",
        "\n",
        "# Source data paths\n",
        "DATA_DIR = DB_PATH / \"data\"\n",
        "SCHEMA_FILE = DATA_DIR / \"schema.sql\"\n",
        "DATA_FILE = DATA_DIR / \"data.sql\"\n",
        "\n",
        "print(f\"Database: {DB_NAME}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Schema file exists: {SCHEMA_FILE.exists()}\")\n",
        "print(f\"Data file exists: {DATA_FILE.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Extract - Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_schema_file(schema_path: Path) -> Optional[str]:\n",
        "    \"\"\"Load database schema from SQL file.\"\"\"\n",
        "    try:\n",
        "        if schema_path.exists():\n",
        "            with open(schema_path, 'r') as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            logger.warning(f\"Schema file not found: {schema_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading schema: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_data_file(data_path: Path) -> Optional[str]:\n",
        "    \"\"\"Load data from SQL file.\"\"\"\n",
        "    try:\n",
        "        if data_path.exists():\n",
        "            with open(data_path, 'r') as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            logger.warning(f\"Data file not found: {data_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load schema and data\n",
        "schema_sql = load_schema_file(SCHEMA_FILE)\n",
        "data_sql = load_data_file(DATA_FILE)\n",
        "\n",
        "if schema_sql:\n",
        "    print(f\"✓ Schema loaded ({len(schema_sql)} characters)\")\n",
        "if data_sql:\n",
        "    print(f\"✓ Data loaded ({len(data_sql)} characters)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_from_csv(csv_path: Path) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Extract data from CSV file.\"\"\"\n",
        "    try:\n",
        "        if csv_path.exists():\n",
        "            df = pd.read_csv(csv_path)\n",
        "            logger.info(f\"Loaded {len(df)} rows from {csv_path.name}\")\n",
        "            return df\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading CSV {csv_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_from_json(json_path: Path) -> Optional[Dict]:\n",
        "    \"\"\"Extract data from JSON file.\"\"\"\n",
        "    try:\n",
        "        if json_path.exists():\n",
        "            with open(json_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            logger.info(f\"Loaded JSON from {json_path.name}\")\n",
        "            return data\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading JSON {json_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Find and load data files\n",
        "csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
        "json_files = list(DATA_DIR.glob(\"*.json\"))\n",
        "\n",
        "extracted_data = {}\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    df = extract_from_csv(csv_file)\n",
        "    if df is not None:\n",
        "        extracted_data[csv_file.stem] = df\n",
        "\n",
        "for json_file in json_files:\n",
        "    data = extract_from_json(json_file)\n",
        "    if data is not None:\n",
        "        extracted_data[json_file.stem] = data\n",
        "\n",
        "print(f\"✓ Extracted {len(extracted_data)} data sources\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Transform - Data Cleaning and Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean DataFrame: handle missing values, remove duplicates, etc.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    # Remove duplicates\n",
        "    initial_rows = len(df)\n",
        "    df = df.drop_duplicates()\n",
        "    duplicates_removed = initial_rows - len(df)\n",
        "\n",
        "    # Handle missing values\n",
        "    missing_before = df.isnull().sum().sum()\n",
        "    # Fill numeric columns with median\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "    # Fill text columns with mode\n",
        "    text_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in text_cols:\n",
        "        df[col] = df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else '')\n",
        "    missing_after = df.isnull().sum().sum()\n",
        "\n",
        "    logger.info(f\"Cleaned data: removed {duplicates_removed} duplicates, filled {missing_before - missing_after} missing values\")\n",
        "    return df\n",
        "\n",
        "# Clean extracted data\n",
        "cleaned_data = {}\n",
        "for name, data in extracted_data.items():\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        cleaned_data[name] = clean_dataframe(data)\n",
        "    else:\n",
        "        cleaned_data[name] = data\n",
        "\n",
        "print(f\"✓ Cleaned {len(cleaned_data)} data sources\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_dataframe(df: pd.DataFrame, required_columns: List[str] = None) -> Dict:\n",
        "    \"\"\"Validate DataFrame structure and data quality.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return {'valid': False, 'errors': ['DataFrame is empty or None']}\n",
        "\n",
        "    validation_results = {\n",
        "        'valid': True,\n",
        "        'row_count': len(df),\n",
        "        'column_count': len(df.columns),\n",
        "        'missing_values': df.isnull().sum().to_dict(),\n",
        "        'duplicate_rows': df.duplicated().sum(),\n",
        "        'errors': []\n",
        "    }\n",
        "\n",
        "    # Check required columns\n",
        "    if required_columns:\n",
        "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            validation_results['valid'] = False\n",
        "            validation_results['errors'].append(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "    return validation_results\n",
        "\n",
        "# Validate cleaned data\n",
        "validation_results = {}\n",
        "for name, data in cleaned_data.items():\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        validation_results[name] = validate_dataframe(data)\n",
        "\n",
        "# Display validation results\n",
        "for name, results in validation_results.items():\n",
        "    status = \"✓\" if results['valid'] else \"✗\"\n",
        "    print(f\"{status} {name}: {results['row_count']} rows, {results['column_count']} columns\")\n",
        "    if results['errors']:\n",
        "        for error in results['errors']:\n",
        "            print(f\"  Error: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Load - Data Loading to Target Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_to_postgresql(df: pd.DataFrame, table_name: str, connection_string: str, if_exists: str = 'replace') -> bool:\n",
        "    \"\"\"Load DataFrame to PostgreSQL table.\"\"\"\n",
        "    if not SQLALCHEMY_AVAILABLE or connection_string is None:\n",
        "        logger.warning(\"PostgreSQL connection not available\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        engine = create_engine(connection_string)\n",
        "        df.to_sql(table_name, engine, if_exists=if_exists, index=False)\n",
        "        logger.info(f\"Loaded {len(df)} rows to PostgreSQL table {table_name}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading to PostgreSQL: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_to_snowflake(df: pd.DataFrame, table_name: str, connection_string: str) -> bool:\n",
        "    \"\"\"Load DataFrame to Snowflake table.\"\"\"\n",
        "    if not SQLALCHEMY_AVAILABLE or connection_string is None:\n",
        "        logger.warning(\"Snowflake connection not available\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        engine = create_engine(connection_string)\n",
        "        df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
        "        logger.info(f\"Loaded {len(df)} rows to Snowflake table {table_name}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading to Snowflake: {e}\")\n",
        "        return False\n",
        "\n",
        "# Load data to target databases\n",
        "load_results = {}\n",
        "\n",
        "for name, data in cleaned_data.items():\n",
        "    if isinstance(data, pd.DataFrame) and not data.empty:\n",
        "        table_name = name.lower().replace(' ', '_')\n",
        "\n",
        "        # PostgreSQL\n",
        "        if POSTGRES_CONNECTION_STRING:\n",
        "            load_results[f\"{name}_postgres\"] = load_to_postgresql(\n",
        "                data, table_name, POSTGRES_CONNECTION_STRING\n",
        "            )\n",
        "\n",
        "        # Snowflake\n",
        "        if SNOWFLAKE_CONNECTION_STRING:\n",
        "            load_results[f\"{name}_snowflake\"] = load_to_snowflake(\n",
        "                data, table_name, SNOWFLAKE_CONNECTION_STRING\n",
        "            )\n",
        "\n",
        "print(f\"✓ Loaded {sum(load_results.values())} datasets to target databases\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Validate - Data Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data_quality_report(df: pd.DataFrame, table_name: str) -> Dict:\n",
        "    \"\"\"Generate comprehensive data quality report.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return {'table': table_name, 'status': 'empty'}\n",
        "\n",
        "    report = {\n",
        "        'table': table_name,\n",
        "        'row_count': len(df),\n",
        "        'column_count': len(df.columns),\n",
        "        'missing_values': int(df.isnull().sum().sum()),\n",
        "        'missing_percentage': float((df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100),\n",
        "        'duplicate_rows': int(df.duplicated().sum()),\n",
        "        'data_types': df.dtypes.astype(str).to_dict(),\n",
        "        'numeric_stats': {},\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Add statistics for numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        report['numeric_stats'] = df[numeric_cols].describe().to_dict()\n",
        "\n",
        "    return report\n",
        "\n",
        "# Generate quality reports\n",
        "quality_reports = {}\n",
        "for name, data in cleaned_data.items():\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        quality_reports[name] = generate_data_quality_report(data, name)\n",
        "\n",
        "# Display quality reports\n",
        "for name, report in quality_reports.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"Rows: {report['row_count']}\")\n",
        "    print(f\"Columns: {report['column_count']}\")\n",
        "    print(f\"Missing values: {report['missing_values']} ({report['missing_percentage']:.2f}%)\")\n",
        "    print(f\"Duplicate rows: {report['duplicate_rows']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Monitor - Pipeline Monitoring and Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save pipeline execution metadata\n",
        "pipeline_metadata = {\n",
        "    'database': DB_NAME,\n",
        "    'execution_timestamp': datetime.now().isoformat(),\n",
        "    'data_sources': list(extracted_data.keys()),\n",
        "    'extracted_count': len(extracted_data),\n",
        "    'cleaned_count': len(cleaned_data),\n",
        "    'validation_results': validation_results,\n",
        "    'load_results': load_results,\n",
        "    'quality_reports': quality_reports,\n",
        "    'status': 'completed'\n",
        "}\n",
        "\n",
        "# Save metadata to JSON\n",
        "metadata_file = DB_PATH / \"metadata\" / \"pipeline_metadata.json\"\n",
        "metadata_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(metadata_file, 'w') as f:\n",
        "    json.dump(pipeline_metadata, f, indent=2, default=str)\n",
        "\n",
        "print(f\"✓ Pipeline metadata saved to {metadata_file}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PIPELINE EXECUTION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Database: {pipeline_metadata['database']}\")\n",
        "print(f\"Execution time: {pipeline_metadata['execution_timestamp']}\")\n",
        "print(f\"Data sources extracted: {pipeline_metadata['extracted_count']}\")\n",
        "print(f\"Datasets cleaned: {pipeline_metadata['cleaned_count']}\")\n",
        "print(f\"Successful loads: {sum(pipeline_metadata['load_results'].values())}\")\n",
        "print(f\"Status: {pipeline_metadata['status']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
