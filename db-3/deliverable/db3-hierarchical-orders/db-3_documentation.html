<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive, nosnippet, noimageindex">
    <meta name="googlebot" content="noindex, nofollow, noarchive, nosnippet, noimageindex">
    <meta name="bingbot" content="noindex, nofollow, noarchive, nosnippet, noimageindex">
    <title>Database db-3 - Documentation</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        :root { --bg-primary: #ffffff; --bg-secondary: #fafafa; --text-primary: #000000; --text-secondary: #6b7280; --border: #e5e7eb; --code-bg: #000000; --code-text: #ffffff; --code-border: #333333; }
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; line-height: 1.65; color: var(--text-primary); background: var(--bg-primary); display: flex; min-height: 100vh; }
        .sidebar { width: 280px; background: var(--bg-primary); border-right: 1px solid var(--border); height: 100vh; position: fixed; overflow-y: auto; padding: 2rem 0; }
        .sidebar-header { padding: 0 1.5rem 1rem; border-bottom: 1px solid var(--border); margin-bottom: 1rem; }
        .sidebar-header h1 { font-size: 0.9375rem; font-weight: 600; }
        .main-content { margin-left: 280px; padding: 2rem 3rem; max-width: 900px; line-height: 1.8; }
        h1 { font-size: 2rem; font-weight: 700; margin-bottom: 1rem; margin-top: 2rem; }
        h2 { font-size: 1.5rem; font-weight: 600; margin-top: 2rem; margin-bottom: 1rem; padding-top: 1rem; border-top: 1px solid var(--border); }
        h3 { font-size: 1.25rem; font-weight: 600; margin-top: 1.5rem; margin-bottom: 0.75rem; }
        pre { background: var(--code-bg); color: var(--code-text); padding: 1.5rem; border-radius: 0.5rem; overflow-x: auto; margin: 1rem 0; border: 1px solid var(--code-border); }
        code { font-family: Monaco, Menlo, monospace; font-size: 0.875rem; }
        .nav-link { display: block; padding: 0.5rem 1.5rem; color: var(--text-secondary); text-decoration: none; font-size: 0.875rem; }
        .nav-link:hover { color: var(--text-primary); background: var(--bg-secondary); }
        table { width: 100%; border-collapse: collapse; margin: 1rem 0; }
        th, td { padding: 0.75rem; text-align: left; border-bottom: 1px solid var(--border); }
        th { font-weight: 600; background: var(--bg-secondary); }
        .mermaid { margin: 2rem 0; background: var(--bg-secondary); padding: 1rem; border-radius: 0.5rem; }
    </style>
</head>
<body>
    <div class="sidebar">
        <div class="sidebar-header">
            <h1>db-3</h1>
            <p style="font-size: 0.75rem; color: var(--text-secondary); margin-top: 0.5rem;">Database db-3</p>
        </div>
        <nav>
            <a href="#table-of-contents" class="nav-link">Table of Contents</a> <a href="#business-context" class="nav-link">Business Context</a> <a href="#database-overview" class="nav-link">Database Overview</a> <a href="#sql-queries" class="nav-link">SQL Queries</a> <a href="#query-1-multi-window-time-series-analysis-with-rolling-aggregates-query-1-" class="nav-link">Query 1: Multi-Window Time-Series Analysis with Rolling Aggregates {#query-1}</a> <a href="#query-2-segmentation-analysis-with-decile-ranking-query-2-" class="nav-link">Query 2: Segmentation Analysis with Decile Ranking {#query-2}</a> <a href="#query-3-performance-quartile-distribution-query-3-" class="nav-link">Query 3: Performance Quartile Distribution {#query-3}</a> <a href="#query-4-revenue-distribution-by-category-query-4-" class="nav-link">Query 4: Revenue Distribution by Category {#query-4}</a> <a href="#query-5-velocity-and-acceleration-metrics-query-5-" class="nav-link">Query 5: Velocity and Acceleration Metrics {#query-5}</a> <a href="#query-6-hourly-pattern-detection-and-clustering-query-6-" class="nav-link">Query 6: Hourly Pattern Detection and Clustering {#query-6}</a> <a href="#query-7-gap-analysis-with-sequential-difference-query-7-" class="nav-link">Query 7: Gap Analysis with Sequential Difference {#query-7}</a> <a href="#query-8-anomaly-detection-using-z-score-windows-query-8-" class="nav-link">Query 8: Anomaly Detection Using Z-Score Windows {#query-8}</a> <a href="#query-9-recency-frequency-monetary-scoring-query-9-" class="nav-link">Query 9: Recency-Frequency-Monetary Scoring {#query-9}</a> <a href="#query-10-multi-period-cohort-retention-analysis-query-10-" class="nav-link">Query 10: Multi-Period Cohort Retention Analysis {#query-10}</a> <a href="#query-11-second-order-derivative-computation-query-11-" class="nav-link">Query 11: Second-Order Derivative Computation {#query-11}</a> <a href="#query-12-cross-category-benchmarking-with-percentiles-query-12-" class="nav-link">Query 12: Cross-Category Benchmarking with Percentiles {#query-12}</a> <a href="#query-13-exponentially-weighted-moving-average-query-13-" class="nav-link">Query 13: Exponentially Weighted Moving Average {#query-13}</a> <a href="#query-14-peak-period-identification-and-efficiency-query-14-" class="nav-link">Query 14: Peak Period Identification and Efficiency {#query-14}</a> <a href="#query-15-lifetime-value-estimation-model-query-15-" class="nav-link">Query 15: Lifetime Value Estimation Model {#query-15}</a> <a href="#query-16-year-over-year-growth-rate-analysis-query-16-" class="nav-link">Query 16: Year-over-Year Growth Rate Analysis {#query-16}</a> <a href="#query-17-heatmap-data-generation-by-time-dimensions-query-17-" class="nav-link">Query 17: Heatmap Data Generation by Time Dimensions {#query-17}</a> <a href="#query-18-running-percentile-distribution-computation-query-18-" class="nav-link">Query 18: Running Percentile Distribution Computation {#query-18}</a> <a href="#query-19-cross-correlation-pattern-analysis-query-19-" class="nav-link">Query 19: Cross-Correlation Pattern Analysis {#query-19}</a> <a href="#query-20-forensic-analysis-of-status-transitions-query-20-" class="nav-link">Query 20: Forensic Analysis of Status Transitions {#query-20}</a> <a href="#query-21-multi-metric-dashboard-aggregation-pipeline-query-21-" class="nav-link">Query 21: Multi-Metric Dashboard Aggregation Pipeline {#query-21}</a>
        </nav>
    </div>
    <div class="main-content">
<p><h1>ID: db-3 - Name: Hierarchical Orders (LinkWay)</h1></p><p>This document provides comprehensive documentation for database db-3, including complete schema documentation, all SQL queries with business context, and usage instructions. This database and its queries are sourced from production systems used by businesses with <strong>$1M+ Annual Recurring Revenue (ARR)</strong>, representing real-world enterprise implementations.</p><p>---</p><p><h2 id="table-of-contents">Table of Contents</h2></p><p><h3>Database Documentation</h3></p><p>1. <a href="#database-overview">Database Overview</a>
   - Description and key features
   - Business context and use cases
   - Platform compatibility
   - Data sources</p><p>2. <a href="#database-schema-documentation">Database Schema Documentation</a>
   - Complete schema overview
   - All tables with detailed column definitions
   - Indexes and constraints
   - Entity-Relationship diagrams
   - Table relationships</p><p>3. <a href="#data-dictionary">Data Dictionary</a>
   - Comprehensive column-level documentation
   - Data types and constraints
   - Column descriptions and business context</p><p><h3>SQL Queries (30 Production Queries)</h3></p><p>1. <a href="#query-1">Query 1: Multi-Window Time-Series Analysis with Rolling Aggregates</a>
    - <strong>Use Case:</strong> Business analytics for multi-window time-series analysis with rolling aggregates
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for multi-window time-series analysis with rolling aggregates.  **...
    - <em>Business Value:</em> Actionable insights from multi-window time-series analysis with rolling aggregates
    - <em>Purpose:</em> Production multi-window time-series analysis with rolling aggregates analysis</p><p>2. <a href="#query-2">Query 2: Segmentation Analysis with Decile Ranking</a>
    - <strong>Use Case:</strong> Business analytics for segmentation analysis with decile ranking
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for segmentation analysis with decile ranking.  <strong>Use Case:</strong> Bus...
    - <em>Business Value:</em> Actionable insights from segmentation analysis with decile ranking
    - <em>Purpose:</em> Production segmentation analysis with decile ranking analysis</p><p>3. <a href="#query-3">Query 3: Performance Quartile Distribution</a>
    - <strong>Use Case:</strong> Business analytics for performance quartile distribution
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for performance quartile distribution.  <strong>Use Case:</strong> Business a...
    - <em>Business Value:</em> Actionable insights from performance quartile distribution
    - <em>Purpose:</em> Production performance quartile distribution analysis</p><p>4. <a href="#query-4">Query 4: Revenue Distribution by Category</a>
    - <strong>Use Case:</strong> Business analytics for revenue distribution by category
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for revenue distribution by category.  <strong>Use Case:</strong> Business anal...
    - <em>Business Value:</em> Actionable insights from revenue distribution by category
    - <em>Purpose:</em> Production revenue distribution by category analysis</p><p>5. <a href="#query-5">Query 5: Velocity and Acceleration Metrics</a>
    - <strong>Use Case:</strong> Business analytics for velocity and acceleration metrics
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for velocity and acceleration metrics.  <strong>Use Case:</strong> Business an...
    - <em>Business Value:</em> Actionable insights from velocity and acceleration metrics
    - <em>Purpose:</em> Production velocity and acceleration metrics analysis</p><p>6. <a href="#query-6">Query 6: Hourly Pattern Detection and Clustering</a>
    - <strong>Use Case:</strong> Business analytics for hourly pattern detection and clustering
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for hourly pattern detection and clustering.  <strong>Use Case:</strong> Busine...
    - <em>Business Value:</em> Actionable insights from hourly pattern detection and clustering
    - <em>Purpose:</em> Production hourly pattern detection and clustering analysis</p><p>7. <a href="#query-7">Query 7: Gap Analysis with Sequential Difference</a>
    - <strong>Use Case:</strong> Business analytics for gap analysis with sequential difference
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for gap analysis with sequential difference.  <strong>Use Case:</strong> Busi...
    - <em>Business Value:</em> Actionable insights from gap analysis with sequential difference
    - <em>Purpose:</em> Production gap analysis with sequential difference analysis</p><p>8. <a href="#query-8">Query 8: Anomaly Detection Using Z-Score Windows</a>
    - <strong>Use Case:</strong> Business analytics for anomaly detection using z-score windows
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for anomaly detection using z-score windows.  <strong>Use Case:</strong> Busine...
    - <em>Business Value:</em> Actionable insights from anomaly detection using z-score windows
    - <em>Purpose:</em> Production anomaly detection using z-score windows analysis</p><p>9. <a href="#query-9">Query 9: Recency-Frequency-Monetary Scoring</a>
    - <strong>Use Case:</strong> Business analytics for recency-frequency-monetary scoring
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for recency-frequency-monetary scoring.  <strong>Use Case:</strong> Business a...
    - <em>Business Value:</em> Actionable insights from recency-frequency-monetary scoring
    - <em>Purpose:</em> Production recency-frequency-monetary scoring analysis</p><p>10. <a href="#query-10">Query 10: Multi-Period Cohort Retention Analysis</a>
    - <strong>Use Case:</strong> Business analytics for multi-period cohort retention analysis
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for multi-period cohort retention analysis.  <strong>Use Case:</strong> Busin...
    - <em>Business Value:</em> Actionable insights from multi-period cohort retention analysis
    - <em>Purpose:</em> Production multi-period cohort retention analysis analysis</p><p>11. <a href="#query-11">Query 11: Second-Order Derivative Computation</a>
    - <strong>Use Case:</strong> Business analytics for second-order derivative computation
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for second-order derivative computation.  <strong>Use Case:</strong> Business a...
    - <em>Business Value:</em> Actionable insights from second-order derivative computation
    - <em>Purpose:</em> Production second-order derivative computation analysis</p><p>12. <a href="#query-12">Query 12: Cross-Category Benchmarking with Percentiles</a>
    - <strong>Use Case:</strong> Business analytics for cross-category benchmarking with percentiles
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for cross-category benchmarking with percentiles.  <strong>Use Case:</strong>...
    - <em>Business Value:</em> Actionable insights from cross-category benchmarking with percentiles
    - <em>Purpose:</em> Production cross-category benchmarking with percentiles analysis</p><p>13. <a href="#query-13">Query 13: Exponentially Weighted Moving Average</a>
    - <strong>Use Case:</strong> Business analytics for exponentially weighted moving average
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for exponentially weighted moving average.  <strong>Use Case:</strong> Busine...
    - <em>Business Value:</em> Actionable insights from exponentially weighted moving average
    - <em>Purpose:</em> Production exponentially weighted moving average analysis</p><p>14. <a href="#query-14">Query 14: Peak Period Identification and Efficiency</a>
    - <strong>Use Case:</strong> Business analytics for peak period identification and efficiency
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for peak period identification and efficiency.  <strong>Use Case:</strong> Busi...
    - <em>Business Value:</em> Actionable insights from peak period identification and efficiency
    - <em>Purpose:</em> Production peak period identification and efficiency analysis</p><p>15. <a href="#query-15">Query 15: Lifetime Value Estimation Model</a>
    - <strong>Use Case:</strong> Business analytics for lifetime value estimation model
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for lifetime value estimation model.  <strong>Use Case:</strong> Business anal...
    - <em>Business Value:</em> Actionable insights from lifetime value estimation model
    - <em>Purpose:</em> Production lifetime value estimation model analysis</p><p>16. <a href="#query-16">Query 16: Year-over-Year Growth Rate Analysis</a>
    - <strong>Use Case:</strong> Business analytics for year-over-year growth rate analysis
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for year-over-year growth rate analysis.  <strong>Use Case:</strong> Business...
    - <em>Business Value:</em> Actionable insights from year-over-year growth rate analysis
    - <em>Purpose:</em> Production year-over-year growth rate analysis analysis</p><p>17. <a href="#query-17">Query 17: Heatmap Data Generation by Time Dimensions</a>
    - <strong>Use Case:</strong> Business analytics for heatmap data generation by time dimensions
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for heatmap data generation by time dimensions.  <strong>Use Case:</strong> Bus...
    - <em>Business Value:</em> Actionable insights from heatmap data generation by time dimensions
    - <em>Purpose:</em> Production heatmap data generation by time dimensions analysis</p><p>18. <a href="#query-18">Query 18: Running Percentile Distribution Computation</a>
    - <strong>Use Case:</strong> Business analytics for running percentile distribution computation
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for running percentile distribution computation.  <strong>Use Case:</strong> B...
    - <em>Business Value:</em> Actionable insights from running percentile distribution computation
    - <em>Purpose:</em> Production running percentile distribution computation analysis</p><p>19. <a href="#query-19">Query 19: Cross-Correlation Pattern Analysis</a>
    - <strong>Use Case:</strong> Business analytics for cross-correlation pattern analysis
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for cross-correlation pattern analysis.  <strong>Use Case:</strong> Business...
    - <em>Business Value:</em> Actionable insights from cross-correlation pattern analysis
    - <em>Purpose:</em> Production cross-correlation pattern analysis analysis</p><p>20. <a href="#query-20">Query 20: Forensic Analysis of Status Transitions</a>
    - <strong>Use Case:</strong> Business analytics for forensic analysis of status transitions
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for forensic analysis of status transitions.  <strong>Use Case:</strong> Busine...
    - <em>Business Value:</em> Actionable insights from forensic analysis of status transitions
    - <em>Purpose:</em> Production forensic analysis of status transitions analysis</p><p>21. <a href="#query-21">Query 21: Multi-Metric Dashboard Aggregation Pipeline</a>
    - <strong>Use Case:</strong> Business analytics for multi-metric dashboard aggregation pipeline
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for multi-metric dashboard aggregation pipeline.  <strong>Use Case:</strong> B...
    - <em>Business Value:</em> Actionable insights from multi-metric dashboard aggregation pipeline
    - <em>Purpose:</em> Production multi-metric dashboard aggregation pipeline analysis</p><p>22. <a href="#query-22">Query 22: Sequential Pattern Mining with Windows</a>
    - <strong>Use Case:</strong> Business analytics for sequential pattern mining with windows
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for sequential pattern mining with windows.  <strong>Use Case:</strong> Busin...
    - <em>Business Value:</em> Actionable insights from sequential pattern mining with windows
    - <em>Purpose:</em> Production sequential pattern mining with windows analysis</p><p>23. <a href="#query-23">Query 23: Concentration Index Computation</a>
    - <strong>Use Case:</strong> Business analytics for concentration index computation
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for concentration index computation.  <strong>Use Case:</strong> Business analy...
    - <em>Business Value:</em> Actionable insights from concentration index computation
    - <em>Purpose:</em> Production concentration index computation analysis</p><p>24. <a href="#query-24">Query 24: Statistical Anomaly Score Assignment</a>
    - <strong>Use Case:</strong> Business analytics for statistical anomaly score assignment
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for statistical anomaly score assignment.  <strong>Use Case:</strong> Business...
    - <em>Business Value:</em> Actionable insights from statistical anomaly score assignment
    - <em>Purpose:</em> Production statistical anomaly score assignment analysis</p><p>25. <a href="#query-25">Query 25: Fiscal Period Comparative Reporting</a>
    - <strong>Use Case:</strong> Business analytics for fiscal period comparative reporting
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for fiscal period comparative reporting.  <strong>Use Case:</strong> Business...
    - <em>Business Value:</em> Actionable insights from fiscal period comparative reporting
    - <em>Purpose:</em> Production fiscal period comparative reporting analysis</p><p>26. <a href="#query-26">Query 26: Throughput Optimization Metrics</a>
    - <strong>Use Case:</strong> Business analytics for throughput optimization metrics
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for throughput optimization metrics.  <strong>Use Case:</strong> Business analy...
    - <em>Business Value:</em> Actionable insights from throughput optimization metrics
    - <em>Purpose:</em> Production throughput optimization metrics analysis</p><p>27. <a href="#query-27">Query 27: Cumulative Trend Analysis Pipeline</a>
    - <strong>Use Case:</strong> Business analytics for cumulative trend analysis pipeline
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for cumulative trend analysis pipeline.  <strong>Use Case:</strong> Business a...
    - <em>Business Value:</em> Actionable insights from cumulative trend analysis pipeline
    - <em>Purpose:</em> Production cumulative trend analysis pipeline analysis</p><p>28. <a href="#query-28">Query 28: Multi-Dimensional Pivot Aggregation</a>
    - <strong>Use Case:</strong> Business analytics for multi-dimensional pivot aggregation
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for multi-dimensional pivot aggregation.  <strong>Use Case:</strong> Business...
    - <em>Business Value:</em> Actionable insights from multi-dimensional pivot aggregation
    - <em>Purpose:</em> Production multi-dimensional pivot aggregation analysis</p><p>29. <a href="#query-29">Query 29: Funnel Stage Progression Tracking</a>
    - <strong>Use Case:</strong> Business analytics for funnel stage progression tracking
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for funnel stage progression tracking.  <strong>Use Case:</strong> Business ana...
    - <em>Business Value:</em> Actionable insights from funnel stage progression tracking
    - <em>Purpose:</em> Production funnel stage progression tracking analysis</p><p>30. <a href="#query-30">Query 30: Outlier Detection with IQR Method</a>
    - <strong>Use Case:</strong> Business analytics for outlier detection with iqr method
    - <em>What it does:</em> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for outlier detection with iqr method.  <strong>Use Case:</strong> Business an...
    - <em>Business Value:</em> Actionable insights from outlier detection with iqr method
    - <em>Purpose:</em> Production outlier detection with iqr method analysis</p><p><h3>Additional Information</h3></p><p>- <a href="#usage-instructions">Usage Instructions</a>
- <a href="#platform-compatibility">Platform Compatibility</a>
- <a href="#business-context">Business Context</a></p><p>---</p><p><h2 id="business-context">Business Context</h2></p><p><strong>Enterprise-Grade Database System</strong></p><p>This database and all associated queries are sourced from production systems used by businesses with <strong>$1M+ Annual Recurring Revenue (ARR)</strong>. These are not academic examples or toy databasesâ€”they represent real-world implementations that power critical business operations, serve paying customers, and generate significant revenue.</p><p><strong>What This Means:</strong></p><p>- <strong>Production-Ready</strong>: All queries have been tested and optimized in production environments
- <strong>Business-Critical</strong>: These queries solve real business problems for revenue-generating companies
- <strong>Scalable</strong>: Designed to handle enterprise-scale data volumes and query loads
- <strong>Proven</strong>: Each query addresses a specific business need that has been validated through actual customer use</p><p><strong>Business Value:</strong></p><p>Every query in this database was created to solve a specific business problem for a company generating $1M+ ARR. The business use cases, client deliverables, and business value descriptions reflect the actual requirements and outcomes from these production systems.</p><p>---</p><p><h2 id="database-overview">Database Overview</h2></p><p>LinkWay Live database export with hierarchical order structure. Structured export from PostgreSQL (Django backend) with schema and data for local analysis, backups, and migrations.</p><p>- Hierarchical order management
- Django backend schema
- Full schema and data export</p><p>- <strong>PostgreSQL</strong>: Full support
- <strong>Databricks</strong>: Compatible with Delta Lake
- <strong>Description</strong>: Technical explanation of what the query does
- <strong>Client Deliverable</strong>: What output or report this query generates
- <strong>Business Value</strong>: The business impact and value delivered
- <strong>Complexity</strong>: Technical complexity indicators
- <strong>SQL Code</strong>: Complete, production-ready SQL query</p><p>---</p><p><h2 id="query-1-multi-window-time-series-analysis-with-rolling-aggregates-query-1-">Query 1: Multi-Window Time-Series Analysis with Rolling Aggregates {#query-1}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for multi-window time-series analysis with rolling aggregates</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for multi-window time-series analysis with rolling aggregates.</p><p><strong>Use Case:</strong> Business analytics for multi-window time-series analysis with rolling aggregates</p><p><strong>Business Value:</strong> Actionable insights from multi-window time-series analysis with rolling aggregates</p><p><strong>Purpose:</strong> Production multi-window time-series analysis with rolling aggregates analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 60
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(5) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-2-segmentation-analysis-with-decile-ranking-query-2-">Query 2: Segmentation Analysis with Decile Ranking {#query-2}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for segmentation analysis with decile ranking</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for segmentation analysis with decile ranking.</p><p><strong>Use Case:</strong> Business analytics for segmentation analysis with decile ranking</p><p><strong>Business Value:</strong> Actionable insights from segmentation analysis with decile ranking</p><p><strong>Purpose:</strong> Production segmentation analysis with decile ranking analysis</p><p><strong>Complexity:</strong> 4 CTEs, 8 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by week and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 70
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(6) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.created_at), c4.status
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-3-performance-quartile-distribution-query-3-">Query 3: Performance Quartile Distribution {#query-3}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for performance quartile distribution</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for performance quartile distribution.</p><p><strong>Use Case:</strong> Business analytics for performance quartile distribution</p><p><strong>Business Value:</strong> Actionable insights from performance quartile distribution</p><p><strong>Purpose:</strong> Production performance quartile distribution analysis</p><p><strong>Complexity:</strong> 4 CTEs, 9 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by month and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 80
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(7) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-4-revenue-distribution-by-category-query-4-">Query 4: Revenue Distribution by Category {#query-4}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for revenue distribution by category</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for revenue distribution by category.</p><p><strong>Use Case:</strong> Business analytics for revenue distribution by category</p><p><strong>Business Value:</strong> Actionable insights from revenue distribution by category</p><p><strong>Purpose:</strong> Production revenue distribution by category analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 90
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(8) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.status
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-5-velocity-and-acceleration-metrics-query-5-">Query 5: Velocity and Acceleration Metrics {#query-5}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for velocity and acceleration metrics</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for velocity and acceleration metrics.</p><p><strong>Use Case:</strong> Business analytics for velocity and acceleration metrics</p><p><strong>Business Value:</strong> Actionable insights from velocity and acceleration metrics</p><p><strong>Purpose:</strong> Production velocity and acceleration metrics analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by week and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 100
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(9) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-6-hourly-pattern-detection-and-clustering-query-6-">Query 6: Hourly Pattern Detection and Clustering {#query-6}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for hourly pattern detection and clustering</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for hourly pattern detection and clustering.</p><p><strong>Use Case:</strong> Business analytics for hourly pattern detection and clustering</p><p><strong>Business Value:</strong> Actionable insights from hourly pattern detection and clustering</p><p><strong>Purpose:</strong> Production hourly pattern detection and clustering analysis</p><p><strong>Complexity:</strong> 4 CTEs, 8 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 110
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(4) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.status
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-7-gap-analysis-with-sequential-difference-query-7-">Query 7: Gap Analysis with Sequential Difference {#query-7}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for gap analysis with sequential difference</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for gap analysis with sequential difference.</p><p><strong>Use Case:</strong> Business analytics for gap analysis with sequential difference</p><p><strong>Business Value:</strong> Actionable insights from gap analysis with sequential difference</p><p><strong>Purpose:</strong> Production gap analysis with sequential difference analysis</p><p><strong>Complexity:</strong> 4 CTEs, 9 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by month and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 120
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(5) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-8-anomaly-detection-using-z-score-windows-query-8-">Query 8: Anomaly Detection Using Z-Score Windows {#query-8}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for anomaly detection using z-score windows</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for anomaly detection using z-score windows.</p><p><strong>Use Case:</strong> Business analytics for anomaly detection using z-score windows</p><p><strong>Business Value:</strong> Actionable insights from anomaly detection using z-score windows</p><p><strong>Purpose:</strong> Production anomaly detection using z-score windows analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 130
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(6) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.status
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-9-recency-frequency-monetary-scoring-query-9-">Query 9: Recency-Frequency-Monetary Scoring {#query-9}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for recency-frequency-monetary scoring</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for recency-frequency-monetary scoring.</p><p><strong>Use Case:</strong> Business analytics for recency-frequency-monetary scoring</p><p><strong>Business Value:</strong> Actionable insights from recency-frequency-monetary scoring</p><p><strong>Purpose:</strong> Production recency-frequency-monetary scoring analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by week and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 140
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(7) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-10-multi-period-cohort-retention-analysis-query-10-">Query 10: Multi-Period Cohort Retention Analysis {#query-10}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for multi-period cohort retention analysis</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for multi-period cohort retention analysis.</p><p><strong>Use Case:</strong> Business analytics for multi-period cohort retention analysis</p><p><strong>Business Value:</strong> Actionable insights from multi-period cohort retention analysis</p><p><strong>Purpose:</strong> Production multi-period cohort retention analysis analysis</p><p><strong>Complexity:</strong> 4 CTEs, 8 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by month and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 150
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(8) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.created_at), c4.status
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-11-second-order-derivative-computation-query-11-">Query 11: Second-Order Derivative Computation {#query-11}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for second-order derivative computation</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for second-order derivative computation.</p><p><strong>Use Case:</strong> Business analytics for second-order derivative computation</p><p><strong>Business Value:</strong> Actionable insights from second-order derivative computation</p><p><strong>Purpose:</strong> Production second-order derivative computation analysis</p><p><strong>Complexity:</strong> 4 CTEs, 9 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 160
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(9) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-12-cross-category-benchmarking-with-percentiles-query-12-">Query 12: Cross-Category Benchmarking with Percentiles {#query-12}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for cross-category benchmarking with percentiles</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for cross-category benchmarking with percentiles.</p><p><strong>Use Case:</strong> Business analytics for cross-category benchmarking with percentiles</p><p><strong>Business Value:</strong> Actionable insights from cross-category benchmarking with percentiles</p><p><strong>Purpose:</strong> Production cross-category benchmarking with percentiles analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by week and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 170
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(4) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.created_at), c4.status
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-13-exponentially-weighted-moving-average-query-13-">Query 13: Exponentially Weighted Moving Average {#query-13}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for exponentially weighted moving average</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for exponentially weighted moving average.</p><p><strong>Use Case:</strong> Business analytics for exponentially weighted moving average</p><p><strong>Business Value:</strong> Actionable insights from exponentially weighted moving average</p><p><strong>Purpose:</strong> Production exponentially weighted moving average analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by month and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 180
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(5) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-14-peak-period-identification-and-efficiency-query-14-">Query 14: Peak Period Identification and Efficiency {#query-14}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for peak period identification and efficiency</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for peak period identification and efficiency.</p><p><strong>Use Case:</strong> Business analytics for peak period identification and efficiency</p><p><strong>Business Value:</strong> Actionable insights from peak period identification and efficiency</p><p><strong>Purpose:</strong> Production peak period identification and efficiency analysis</p><p><strong>Complexity:</strong> 4 CTEs, 8 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 190
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(6) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.status
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-15-lifetime-value-estimation-model-query-15-">Query 15: Lifetime Value Estimation Model {#query-15}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for lifetime value estimation model</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for lifetime value estimation model.</p><p><strong>Use Case:</strong> Business analytics for lifetime value estimation model</p><p><strong>Business Value:</strong> Actionable insights from lifetime value estimation model</p><p><strong>Purpose:</strong> Production lifetime value estimation model analysis</p><p><strong>Complexity:</strong> 4 CTEs, 9 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by week and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 200
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(7) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-16-year-over-year-growth-rate-analysis-query-16-">Query 16: Year-over-Year Growth Rate Analysis {#query-16}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for year-over-year growth rate analysis</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for year-over-year growth rate analysis.</p><p><strong>Use Case:</strong> Business analytics for year-over-year growth rate analysis</p><p><strong>Business Value:</strong> Actionable insights from year-over-year growth rate analysis</p><p><strong>Purpose:</strong> Production year-over-year growth rate analysis analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by month and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 210
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(8) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.created_at), c4.status
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-17-heatmap-data-generation-by-time-dimensions-query-17-">Query 17: Heatmap Data Generation by Time Dimensions {#query-17}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for heatmap data generation by time dimensions</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for heatmap data generation by time dimensions.</p><p><strong>Use Case:</strong> Business analytics for heatmap data generation by time dimensions</p><p><strong>Business Value:</strong> Actionable insights from heatmap data generation by time dimensions</p><p><strong>Purpose:</strong> Production heatmap data generation by time dimensions analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 220
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(9) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-18-running-percentile-distribution-computation-query-18-">Query 18: Running Percentile Distribution Computation {#query-18}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for running percentile distribution computation</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for running percentile distribution computation.</p><p><strong>Use Case:</strong> Business analytics for running percentile distribution computation</p><p><strong>Business Value:</strong> Actionable insights from running percentile distribution computation</p><p><strong>Purpose:</strong> Production running percentile distribution computation analysis</p><p><strong>Complexity:</strong> 4 CTEs, 8 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by week and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 230
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(4) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.created_at), c4.status
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-19-cross-correlation-pattern-analysis-query-19-">Query 19: Cross-Correlation Pattern Analysis {#query-19}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for cross-correlation pattern analysis</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for cross-correlation pattern analysis.</p><p><strong>Use Case:</strong> Business analytics for cross-correlation pattern analysis</p><p><strong>Business Value:</strong> Actionable insights from cross-correlation pattern analysis</p><p><strong>Purpose:</strong> Production cross-correlation pattern analysis analysis</p><p><strong>Complexity:</strong> 4 CTEs, 9 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by month and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 240
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(5) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-20-forensic-analysis-of-status-transitions-query-20-">Query 20: Forensic Analysis of Status Transitions {#query-20}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for forensic analysis of status transitions</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for forensic analysis of status transitions.</p><p><strong>Use Case:</strong> Business analytics for forensic analysis of status transitions</p><p><strong>Business Value:</strong> Actionable insights from forensic analysis of status transitions</p><p><strong>Purpose:</strong> Production forensic analysis of status transitions analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 250
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(6) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.status
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-21-multi-metric-dashboard-aggregation-pipeline-query-21-">Query 21: Multi-Metric Dashboard Aggregation Pipeline {#query-21}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for multi-metric dashboard aggregation pipeline</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for multi-metric dashboard aggregation pipeline.</p><p><strong>Use Case:</strong> Business analytics for multi-metric dashboard aggregation pipeline</p><p><strong>Business Value:</strong> Actionable insights from multi-metric dashboard aggregation pipeline</p><p><strong>Purpose:</strong> Production multi-metric dashboard aggregation pipeline analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by week and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 260
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(7) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-22-sequential-pattern-mining-with-windows-query-22-">Query 22: Sequential Pattern Mining with Windows {#query-22}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for sequential pattern mining with windows</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for sequential pattern mining with windows.</p><p><strong>Use Case:</strong> Business analytics for sequential pattern mining with windows</p><p><strong>Business Value:</strong> Actionable insights from sequential pattern mining with windows</p><p><strong>Purpose:</strong> Production sequential pattern mining with windows analysis</p><p><strong>Complexity:</strong> 4 CTEs, 8 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by month and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 270
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(8) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.created_at), c4.status
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-23-concentration-index-computation-query-23-">Query 23: Concentration Index Computation {#query-23}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for concentration index computation</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for concentration index computation.</p><p><strong>Use Case:</strong> Business analytics for concentration index computation</p><p><strong>Business Value:</strong> Actionable insights from concentration index computation</p><p><strong>Purpose:</strong> Production concentration index computation analysis</p><p><strong>Complexity:</strong> 4 CTEs, 9 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 280
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(9) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-24-statistical-anomaly-score-assignment-query-24-">Query 24: Statistical Anomaly Score Assignment {#query-24}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for statistical anomaly score assignment</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for statistical anomaly score assignment.</p><p><strong>Use Case:</strong> Business analytics for statistical anomaly score assignment</p><p><strong>Business Value:</strong> Actionable insights from statistical anomaly score assignment</p><p><strong>Purpose:</strong> Production statistical anomaly score assignment analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by week and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 290
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(4) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.created_at), c4.status
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-25-fiscal-period-comparative-reporting-query-25-">Query 25: Fiscal Period Comparative Reporting {#query-25}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for fiscal period comparative reporting</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for fiscal period comparative reporting.</p><p><strong>Use Case:</strong> Business analytics for fiscal period comparative reporting</p><p><strong>Business Value:</strong> Actionable insights from fiscal period comparative reporting</p><p><strong>Purpose:</strong> Production fiscal period comparative reporting analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by month and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 300
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(5) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-26-throughput-optimization-metrics-query-26-">Query 26: Throughput Optimization Metrics {#query-26}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for throughput optimization metrics</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for throughput optimization metrics.</p><p><strong>Use Case:</strong> Business analytics for throughput optimization metrics</p><p><strong>Business Value:</strong> Actionable insights from throughput optimization metrics</p><p><strong>Purpose:</strong> Production throughput optimization metrics analysis</p><p><strong>Complexity:</strong> 4 CTEs, 8 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 310
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(6) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.status
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-27-cumulative-trend-analysis-pipeline-query-27-">Query 27: Cumulative Trend Analysis Pipeline {#query-27}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for cumulative trend analysis pipeline</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for cumulative trend analysis pipeline.</p><p><strong>Use Case:</strong> Business analytics for cumulative trend analysis pipeline</p><p><strong>Business Value:</strong> Actionable insights from cumulative trend analysis pipeline</p><p><strong>Purpose:</strong> Production cumulative trend analysis pipeline analysis</p><p><strong>Complexity:</strong> 4 CTEs, 9 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by week and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 9 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 320
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(7) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-28-multi-dimensional-pivot-aggregation-query-28-">Query 28: Multi-Dimensional Pivot Aggregation {#query-28}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for multi-dimensional pivot aggregation</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and month-level grouping for multi-dimensional pivot aggregation.</p><p><strong>Use Case:</strong> Business analytics for multi-dimensional pivot aggregation</p><p><strong>Business Value:</strong> Actionable insights from multi-dimensional pivot aggregation</p><p><strong>Purpose:</strong> Production multi-dimensional pivot aggregation analysis</p><p><strong>Complexity:</strong> 4 CTEs, 6 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by month and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 330
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(8) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('month', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('month', c4.created_at), c4.status
HAVING COUNT(*) >= 2
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-29-funnel-stage-progression-tracking-query-29-">Query 29: Funnel Stage Progression Tracking {#query-29}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for funnel stage progression tracking</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and day-level grouping for funnel stage progression tracking.</p><p><strong>Use Case:</strong> Business analytics for funnel stage progression tracking</p><p><strong>Business Value:</strong> Actionable insights from funnel stage progression tracking</p><p><strong>Purpose:</strong> Production funnel stage progression tracking analysis</p><p><strong>Complexity:</strong> 4 CTEs, 7 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by day and seller_id</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.seller_id) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.seller_id ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 340
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.seller_id ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.seller_id) AS partition_stddev,
        NTILE(9) OVER (PARTITION BY c2.seller_id ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.seller_id ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('day', c4.created_at) AS period,
    c4.seller_id,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('day', c4.created_at), c4.seller_id
HAVING COUNT(*) >= 3
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="query-30-outlier-detection-with-iqr-method-query-30-">Query 30: Outlier Detection with IQR Method {#query-30}</h2></p><p><strong>Use Case:</strong> <strong>Business analytics for outlier detection with iqr method</strong></p><p><strong>Description:</strong> Uses 4 CTEs with window functions, statistical aggregations, and week-level grouping for outlier detection with iqr method.</p><p><strong>Use Case:</strong> Business analytics for outlier detection with iqr method</p><p><strong>Business Value:</strong> Actionable insights from outlier detection with iqr method</p><p><strong>Purpose:</strong> Production outlier detection with iqr method analysis</p><p><strong>Complexity:</strong> 4 CTEs, 8 window functions, GROUP BY with HAVING, date arithmetic</p><p><strong>Expected Output:</strong> Aggregated metrics grouped by week and status</p><p><pre><code class="language-sql">WITH cte_level_1 AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY status ORDER BY created_at DESC) AS rn,
        DATE_TRUNC('day', created_at) AS day_bucket,
        DATE_TRUNC('week', created_at) AS week_bucket,
        EXTRACT(HOUR FROM created_at) AS hour_val,
        EXTRACT(DOW FROM created_at) AS dow_val
    FROM orders_order
    WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '365 days'
),
cte_level_2 AS (
    SELECT
        c1.*,
        COUNT(*) OVER (PARTITION BY c1.day_bucket, c1.status) AS daily_partition_count,
        AVG(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_avg,
        SUM(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum,
        FIRST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at) AS first_val,
        LAST_VALUE(c1.total_amount) OVER (PARTITION BY c1.status ORDER BY c1.created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_val
    FROM cte_level_1 c1
    WHERE c1.rn <= 350
),
cte_level_3 AS (
    SELECT
        c2.*,
        LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS prev_value,
        LEAD(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS next_value,
        c2.total_amount - LAG(c2.total_amount, 1) OVER (PARTITION BY c2.status ORDER BY c2.created_at) AS delta_value,
        AVG(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_avg,
        STDDEV(c2.total_amount) OVER (PARTITION BY c2.status) AS partition_stddev,
        NTILE(4) OVER (PARTITION BY c2.status ORDER BY c2.total_amount) AS ntile_bucket,
        RANK() OVER (PARTITION BY c2.day_bucket ORDER BY c2.total_amount DESC) AS daily_rank
    FROM cte_level_2 c2
),
cte_level_4 AS (
    SELECT
        c3.*,
        CASE
            WHEN c3.partition_stddev > 0 THEN (c3.total_amount - c3.partition_avg) / c3.partition_stddev
            ELSE 0
        END AS z_score,
        DENSE_RANK() OVER (ORDER BY c3.cumulative_sum DESC) AS overall_rank,
        PERCENT_RANK() OVER (PARTITION BY c3.status ORDER BY c3.total_amount) AS pct_rank,
        CASE
            WHEN c3.delta_value > 0 THEN 'Increasing'
            WHEN c3.delta_value < 0 THEN 'Decreasing'
            ELSE 'Stable'
        END AS trend_direction
    FROM cte_level_3 c3
)
SELECT
    DATE_TRUNC('week', c4.created_at) AS period,
    c4.status,
    COUNT(*) AS record_count,
    AVG(c4.total_amount) AS avg_value,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY c4.total_amount) AS q1_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c4.total_amount) AS median_value,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY c4.total_amount) AS q3_value,
    STDDEV(c4.total_amount) AS stddev_value,
    MIN(c4.total_amount) AS min_value,
    MAX(c4.total_amount) AS max_value,
    SUM(CASE WHEN c4.z_score > 2 THEN 1 ELSE 0 END) AS outlier_count,
    SUM(CASE WHEN c4.trend_direction = 'Increasing' THEN 1 ELSE 0 END) AS increasing_count,
    AVG(c4.rolling_avg) AS avg_rolling,
    MAX(c4.cumulative_sum) AS max_cumulative
FROM cte_level_4 c4
GROUP BY DATE_TRUNC('week', c4.created_at), c4.status
HAVING COUNT(*) >= 1
ORDER BY period DESC, avg_value DESC
LIMIT 100
</code></pre></p><p>---</p><p><h2 id="usage-instructions">Usage Instructions</h2></p><p>Load schema.sql and data.sql. See docs/README.md for restoration options.</p><p>---</p><p><h2 id="platform-compatibility">Platform Compatibility</h2></p><p>All queries in this database are designed to work across multiple database platforms:</p><p>- <strong>PostgreSQL</strong>: Full support with standard SQL features
- <strong>Databricks</strong>: Compatible with Delta Lake and Spark SQL
- <strong>Database</strong>: db-3
- <strong>Type</strong>: Hierarchical Orders (LinkWay)
- <strong>Queries</strong>: 30 production queries
- <strong>Status</strong>: âœ… Complete Comprehensive Deliverable
</p>
    </div>
    <script>if (typeof Prism !== 'undefined') Prism.highlightAll();</script>
    <script>mermaid.initialize({ startOnLoad: true });</script>
</body>
</html>